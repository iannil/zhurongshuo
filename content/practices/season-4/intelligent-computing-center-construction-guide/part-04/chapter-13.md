---
title: "第13章 服务机器学习的GPU计算平台落地案例"
date: 2025-12-07T00:00:00+08:00
description: ""
draft: false
hidden: false
tags: ["书稿"]
keywords: ["智算中心建设指南：大模型算力的基础架构", "第13章 服务机器学习的GPU计算平台落地案例"]
slug: "chapter-13"
---

在本书的前三个部分，我们已经系统性地、由浅入深地完成了构建一个大规模GPU计算平台所需的全栈知识体系的学习。我们如同一个学徒，从认识最基础的“砖瓦”（GPU芯片），到学会建造“房屋”（GPU服务器），再到规划整个“城市”的交通（网络）、仓储（存储）、市政（应用平台）与管理（运维运营）。我们已经掌握了所有的理论知识、设计原则和关键技术。

现在，是时候将所有这些知识融会贯通，投入到一场“实战演习”中了。理论的价值最终体现在实践之中。本章——《服务机器学习的GPU计算平台落地案例》——将作为本书的收官之作，我们将扮演一个AI基础设施总架构师的角色，从零开始，为一个具体、真实、且极具挑战性的机器学习应用场景，设计和实现一个完整的、端到端的GPU计算平台。

我们将以当前AI领域最复杂、投入最大的应用之一——自动驾驶模型训练——作为我们的需求来源。自动驾驶对算力、数据、网络、存储的需求是全方位、立体化、且极其苛刻的，这使得它成为了检验我们基础设施设计能力的绝佳“试金石”。

我们将遵循一个完整的系统设计流程，从需求分析开始，到总体架构设计，再到计算、存储、网络等各个子系统的详细设计与实现。

1. 需求分析：我们将首先深入自动驾驶模型训练的工作流，理解其独特的数据特征、模型规模、训练范式以及对基础设施的极致要求。
2. 总体设计：基于需求分析，我们将确立平台的核心设计理念——基于云原生的高性能计算（Cloud-Native HPC），旨在将HPC的极致性能与云的弹性、敏捷性完美结合。
3. 分系统设计与实现：随后，我们将把前面章节学习到的所有知识都应用到这个案例中，逐一进行计算、存储、网络三大核心子系统的需求分析、技术选型和架构设计。
   1. 计算：如何选择GPU和服务器？如何设计集群规模和混合并行策略？
   2. 存储：如何构建分层存储体系来管理PB级的海量数据？如何解决高并发的数据读取瓶颈？
   3. 网络：如何设计多平面网络来承载训练、存储和管理等不同流量？
4. 平台与运维串联：在设计三大子系统的同时，我们会将第十一、十二章学习到的应用平台和运维运营理念贯穿其中，确保我们构建的不仅仅是一个“裸”的HPC集群，而是一个易于使用、易于管理的现代化AI PaaS平台。

通过这个从需求到落地的完整案例演练，我们将把之前所有分散的知识点串联成一个有机的整体，真正体验一次作为总架构师进行端到端系统设计的全过程。这将是对我们整个学习旅程的最终检验，也是将理论知识转化为实践能力的最后一跃。完成本章后，你将不仅仅是“知道”如何构建一个GPU平台，更是“懂得”如何根据真实世界的复杂需求，去思考、去权衡、去创造一个真正优秀的解决方案。

## 13.1 需求来源：自动驾驶模型训练

自动驾驶，特别是L4/L5级别的高度自动驾驶，被誉为“人工智能皇冠上的明珠”。其背后，是一个由数据驱动的、规模空前庞大的机器学习闭环系统。要理解如何为它设计基础设施，我们必须首先深入其工作流的核心。

### 13.1.1 自动驾驶的“数据闭环”工作流

自动驾驶系统的迭代，依赖于一个持续循环、不断优化的“数据闭环”：

#### 数据采集（Data Acquisition）

庞大的自动驾驶测试车队，每天24小时在全球各地路采。

每辆车都配备了多种传感器：高分辨率摄像头（前视、后视、侧视、环视）、高精度激光雷达（LiDAR）、毫米波雷达（RADAR）、IMU（惯性测量单元）、GPS等。

这些传感器每秒钟都会产生巨量的数据。一辆测试车每小时产生的数据量可达1-2 TB。一个拥有数百辆车的车队，每天产生的数据量可以轻松达到PB级别。

#### 数据回传与存储（Data Ingestion & Storage）

采集到的数据通过移动网络或在车辆回场后，通过高速有线网络，回传到数据中心。

这些原始的、非结构化的数据（我们称之为“Raw Data”），需要被可靠地、低成本地存储起来。其总体量将达到数十乃至数百PB，是典型的“数据湖”场景。

#### 数据处理与标注（Data Processing & Labeling）

原始数据并不能直接用于训练。算法工程师需要从海量的“生肉”中，挖掘出有价值的“困难场景（Corner Cases）”，例如一次罕见的行人横穿、一次恶劣天气下的光照变化、一次复杂的无保护左转等。这个过程被称为数据挖掘（Data Mining）。

被挖掘出的有价值数据片段，需要被送去进行人工或自动化的数据标注。例如，在图像中框出所有的车辆、行人、交通标志，并为它们打上标签；在LiDAR点云中，对每个点进行语义分割。标注后的数据（我们称之为“Ground Truth”），是模型学习的“标准答案”。

#### 模型训练（Model Training）

这是对算力需求最集中的环节。算法工程师使用标注好的数据集，来训练各种类型的深度学习模型：

感知（Perception）模型：如基于图像的2D/3D目标检测（YOLO, CenterPoint）、基于LiDAR点云的语义分割和目标检测、多传感器融合（BEV-Fusion）等。

预测（Prediction）模型：预测道路上其他交通参与者（车辆、行人）在未来几秒内的轨迹。

规划与控制（Planning & Control）模型：基于对环境的理解和对未来的预测，决策出本车最优的行驶路径和速度。

这些模型，特别是端到端的大模型和BEV（鸟瞰图视角）模型，其参数量巨大，训练过程需要在大规模的GPU集群上进行，采用复杂的混合并行策略。

#### 仿真与评估（Simulation & Evaluation）

训练好的新模型，并不能直接上路测试。它首先需要在一个大规模的、逼真的仿真平台上进行测试。

仿真平台会回放各种真实的或虚构的交通场景，来检验新模型在各种Corner Case下的表现。一次完整的回归测试，可能需要在数千个CPU或GPU节点上并行运行数百万个仿真案例。

#### 模型部署与在环验证（Deployment & HIL）

通过仿真验证的模型，会被部署到硬件在环（Hardware-in-the-Loop, HIL）测试台架上，甚至真实的测试车辆上，进行小范围的实路测试。

在实路测试中，系统会持续收集新模型表现不佳的场景，这些新的“困难场景”数据又会被回传到数据中心，进入下一个“数据闭环”的迭代。

### 13.1.2 自动驾驶对基础设施的极致需求

通过上述工作流分析，我们可以提炼出自动驾驶对基础设施的几个核心需求：

#### 海量的、分层的存储需求

PB到EB级的“数据湖”：需要一个成本极低、扩展性近乎无限的存储系统，来归档海量的原始路采数据。—— 这指向了分布式对象存储。

高性能的“训练数据仓库”：标注好的、用于训练的数据集，需要被一个能够支持数千个节点高并发、低延迟、高吞吐量读取的存储系统来承载。—— 这指向了分布式并行文件系统。

快速的检查点（Checkpoint）存储：训练过程中产生的模型检查点可能高达数百GB，需要被快速地写入，以减少因抢占或故障导致的训练中断时间。

#### 超大规模的异构计算需求

大规模GPU训练集群：模型训练是核心负载。需要一个拥有数千张顶级GPU卡、通过高速网络互联的计算集群，来支持长达数周甚至数月的、大规模的分布式训练任务。

大规模CPU/GPU仿真集群：仿真测试对计算的需求量同样巨大，且通常是大量可以并行的、相对独立的短任务，对CPU和GPU都有需求。

数据处理与分析集群：数据挖掘、数据清洗等ETL（提取、转换、加载）任务，通常运行在基于Spark或Flink的大数据集群上。

#### 极致性能的网络需求

高带宽、无损的计算网络：用于支持大规模分布式训练中的梯度和参数交换。网络的性能直接决定了集群的扩展效率。

高吞吐的存储网络：必须能够支撑起数千个节点同时从存储系统中高速拉取数据，避免“数据喂养”成为瓶颈。

高带宽的数据入口：需要有从外部（路采车队、标注中心）到数据中心的高速数据上传通道。

#### 对弹性和敏捷性的要求

多种工作负载并存：集群需要同时支持长周期的训练大任务、海量的仿真短任务、交互式的数据分析任务等。

资源抢占与调度：不同任务之间存在资源竞争。平台需要有能力支持任务的抢占式调度（例如，一个高优先级的模型发布前回归测试，可以抢占一个普通的探索性训练任务）和断点续训。

快速迭代：算法工程师需要能够快速地搭建实验环境、提交训练任务、获取结果，整个平台的易用性和敏捷性至关重要。

这些复杂而苛刻的需求，告诉我们不能简单地照搬传统HPC或互联网的架构。我们需要一种新的、融合两者优点的架构。

## 13.2 总体设计——基于云原生的高性能计算

面对自动驾驶模型训练这一复杂场景，我们的总体设计理念是构建一个“基于云原生的高性能计算（Cloud-Native HPC）”平台。

高性能计算（HPC）：我们将借鉴传统超算中心的设计思想，来构建平台的核心“引擎”。这意味着：

采用专门的、高密度的GPU计算节点。

采用专门的、无损的、低延迟的高性能计算网络（如InfiniBand或RoCE）来连接计算节点。

采用专门的、高性能的并行文件系统来支撑训练数据的读取。

目标是为大规模、紧耦合的分布式训练任务，提供极致的、可预测的裸金属性能。

云原生（Cloud-Native）：我们将全面拥抱以Kubernetes为核心的云原生技术栈，来构建平台的“操作系统”和“用户界面”。这意味着：
一切皆容器：所有的应用，无论是训练任务、仿真任务、还是数据处理服务，都将被封装在容器中运行。

统一的资源编排：使用Kubernetes作为唯一的、统一的资源调度和编排平台，来管理集群中所有的计算、存储和网络资源。

声明式API与自动化：通过Kubernetes的声明式API，实现基础设施的自动化部署、应用的弹性伸缩和自愈。

敏捷与弹性：利用云原生的生态，为用户提供灵活、按需、自助式的资源申请和环境管理能力，提升研发效率。

Cloud-Native HPC的本质，就是试图在同一个平台上，既能跑好像分布式AI训练这样对极致性能和通信延迟要求极高的“HPC类”应用，又能跑好像数据分析、仿真、在线服务这样需要弹性、敏捷和隔离性的“云类”应用。它试图将HPC的“肌肉”和Cloud的“大脑”完美地结合起来。

基于这一总体设计理念，我们将开始进行各个子系统的详细设计。

## 13.3 计算需求分析与设计实现

计算子系统是整个平台的核心，其设计直接决定了平台能够支持的模型规模和训练效率。

### 13.3.1 需求分析

GPU选型：自动驾驶模型，特别是BEV等多模态融合模型，参数量巨大，且大量使用Transformer结构。这要求GPU具备：

巨大的显存容量：以容纳巨大的模型、激活值和优化器状态。

极高的混合精度计算能力：特别是针对Transformer优化的能力。

极高的GPU间互联带宽：以支持高效的张量并行和流水线并行。

服务器选型：需要选择为大规模AI训练优化的、高密度的GPU服务器。

集群规模：这是一个关键的业务和成本决策。假设根据算法团队的规划，为了在合理的时间内（例如2周）完成一个SOTA（State-of-the-Art）模型的训练，他们估算出需要约1024张顶级GPU卡的持续算力。

调度需求：需要支持大规模分布式训练任务的批量提交、排队、抢占，并需要与容器平台（Kubernetes）集成。同时，为了提升利用率，需要支持GPU的共享调度。

### 13.3.2 设计与实现

#### GPU与服务器选型

GPU：毫无疑问，选择当前旗舰级的NVIDIA H100 80GB SXM5模块。其80GB HBM3显存、第四代Tensor Core对FP8的支持、以及Transformer引擎，完美契合了自动驾驶大模型的需求。

服务器：选择NVIDIA DGX H100或其认证的OEM厂商（如Supermicro, Dell, Inspur）生产的、搭载8张H100 SXM5的HGX H100 8-GPU服务器。这些服务器通过NVSwitch实现了8卡之间900 GB/s的全连接NVLink，是进行高效张量并行的理想平台。

#### 集群规模与物理布局

总规模：为了满足1024卡的算力需求，我们需要部署`1024 / 8 = 128`台HGX H100 8-GPU服务器。

物理单元：我们将这128台服务器组织成一个可扩展单元（Scalable Unit, SU）。这个SU是一个独立的、拥有完整计算、网络和存储资源的“巨型计算机”。未来需要扩容时，可以再复制一个或多个SU。

机柜布局：每台HGX H100服务器功耗极高（约10.2KW），对机柜的供电和散热能力提出严峻挑战。需要采用高密度的、支持液冷或先进风冷的机柜。假设一个机柜可以容纳4台HGX服务器，那么128台服务器需要32个机柜。

#### 训练策略与并行模式

对于一个将在1024卡集群上运行的超大模型，我们将采用3D混合并行策略：

张量并行（TP）：在单台HGX服务器内部的8个GPU之间，进行张量并行（例如，TP size = 8）。这得益于900 GB/s的NVLink带宽。

流水线并行（PP）：将模型的不同层，切分到不同的服务器上。例如，使用4台服务器（32个GPU）作为一个流水线并行组（PP size = 4，每个stage由一个8-GPU的TP组构成）。

数据并行（DP）：在多个这样的流水线并行组之间，进行数据并行。在我们的1024卡集群中，总共可以有`1024 / 32 = 32`个数据并行副本（DP size = 32）。

同时，我们将采用ZeRO-1/2来优化数据并行中的优化器状态和梯度存储，进一步节省显存。

#### 调度系统设计

底层编排：使用Kubernetes作为统一的资源编排层。

批量调度器：仅靠K8s默认调度器无法满足HPC类任务的需求。我们需要引入一个批量调度器（Batch Scheduler），如Volcano或Kube-Batch。

Gang Scheduling：批量调度器支持“成组调度”。一个1024卡的分布式训练任务，必须保证所有1024个Pod都同时被调度成功，才会开始运行。只要有一个Pod资源不足，整个任务就会处于等待状态，而不会启动部分Pod，造成资源浪费。

队列与优先级：支持创建不同的任务队列（如高优队列、普通队列），并支持任务的抢占。

GPU共享：

对于大规模训练任务，我们将通过K8s Device Plugin，为每个Pod分配独占的物理GPU。

对于开发、调试、小规模推理等任务，我们将启用NVIDIA MIG功能。在部分服务器上，将H100 GPU划分为多个GI实例，并通过MIG的Device Plugin上报给K8s，供用户按需申请。这极大地提升了GPU的利用率。

## 13.4 存储需求分析与设计实现

存储是支撑整个数据闭环的关键，其设计必须满足分层、高性能、高并发的需求。

### 13.4.1 需求分析

数据湖：需要一个EB级的、成本极低的存储池，用于归档每日产生的PB级原始路采数据（Raw Data）。数据的写入是持续的，但读取频率相对较低。

训练数据仓库：需要一个高性能的存储系统，来存放经过清洗和标注的、用于模型训练的数据集（约PB级）。这个系统必须能够承受1024个GPU节点、数万个data loader进程的高并发读取压力，提供TB/s级别的聚合读取带宽。必须支持POSIX接口和GPUDirect Storage。

模型检查点：训练过程中需要频繁地、快速地写入数百GB的模型检查点。

系统与应用存储：需要为所有服务器的操作系统、Kubernetes集群的etcd、以及各种中间件（如数据库、Kafka）提供高可用的持久化存储。

### 13.4.2 分层存储架构设计与实现

我们将设计一个经典的三层存储架构：

#### 冷数据层（数据湖）：分布式对象存储

技术选型：采用基于大容量HDD硬盘的Ceph集群，并启用其RGW（RADOS Gateway）功能，提供S3兼容的对象存储接口。

设计要点：

规模：部署数百个存储节点，每个节点挂载多块大容量（如18TB/20TB）的SATA HDD，构建一个EB级的存储池。

数据保护：采用纠删码（Erasure Coding, EC）代替多副本。例如，`k=8, m=3`的EC方案，可以将数据分成8个数据块和3个校验块，存放在11个不同的OSD上。它只需要1.375倍的存储开销，就能容忍任意3个OSD的故障，相比3副本（3倍开销）极大地节省了成本。

接入：数据采集和预处理集群通过S3 API，将数据写入这个数据湖。

#### 热数据层（训练数据仓库）：分布式并行文件系统

技术选型：采用Lustre文件系统，其后端存储全部使用高性能的NVMe SSD。

设计要点：

架构：部署一个高可用的MDS（元数据服务器）集群，以及一个由数十台OSS（对象存储服务器）组成的大规模数据服务器集群。每个OSS都配备多块高性能的NVMe SSD。

网络：所有Lustre的服务器（MDS, OSS）和计算节点（客户端）都必须连接到同一个高性能的存储网络上（我们将在下一节讨论）。

GPUDirect Storage支持：Lustre客户端需要配置为支持GPUDirect Storage。这将允许GPU在读取训练数据时，绕过CPU内存，实现从Lustre OSS直达GPU显存的数据路径。

容量与性能：整个Lustre集群的容量设计在PB级别，足以容纳当前和未来一段时间内的所有训练数据集。其聚合带宽通过增加OSS节点的数量，可以线性扩展到TB/s级别，满足1024卡集群的“喂养”需求。

工作流：训练任务开始前，一个数据准备作业会从冷数据层的Ceph对象存储中，将本次训练所需的数据子集，预取（Prefetch）到热数据层的Lustre文件系统中。

#### 平台与应用存储层：分布式块存储

技术选型：我们可以复用对象存储的Ceph集群，但在其中划分出一个专门由SSD硬盘组成的存储池（Pool），并在此池上启用RBD（RADOS Block Device）服务。

设计要点：

这个SSD池专门为需要低延迟和高IOPS的应用服务。

通过Kubernetes的Ceph CSI插件，我们可以动态地为Pod创建和挂载RBD类型的持久化卷（PV）。

应用：

为所有计算和管理节点的操作系统提供RBD启动盘。

为Kubernetes的etcd集群、Prometheus、数据库等有状态服务提供高可用的持久化存储。

作为模型检查点的临时写入位置，利用SSD的高写入性能。

通过这个分层存储设计，我们用最合适的成本，满足了不同场景的苛刻需求，构建了一个既“深不可测”又“快如闪电”的数据基座。

## 13.5 网络需求分析与设计实现

网络是连接计算和存储的命脉。对于我们这个1024卡规模的集群，网络设计是成败的关键。

### 13.5.1 需求分析

计算网络：

必须支持1024个GPU节点之间的大规模、高并发RDMA通信。

必须是无损的、低延迟的。

聚合带宽必须能够支撑起3D并行训练中的密集通信。

必须具备高可用性和可扩展性。

存储网络：

必须能够连接所有1024个计算节点和所有的Lustre、Ceph存储节点。

必须提供TB/s级别的聚合带宽，以满足并发数据加载的需求。

带外管理网络：

必须连接所有设备（服务器BMC、交换机、PDU等）的管理端口。

必须与所有数据网络物理隔离，确保最高级别的安全和可靠性。

### 13.5.2 多平面网络架构设计与实现

我们将采用物理上分离的多平面网络架构。

#### 计算网络（Compute Fabric）

技术选型：InfiniBand (IB)。对于1024卡这种超大规模的、对通信极度敏感的HPC类负载，IB以其原生的无损特性、硬件级的拥塞管理和极致的低延迟，是比RoCE更稳妥、性能更可预测的选择。我们将采用NVIDIA Quantum-2 400Gb/s InfiniBand平台。

拓扑架构：采用一个两层的Fat-Tree (Clos)架构。

Leaf层：部署若干台Leaf交换机。每台HGX H100服务器（拥有8张ConnectX-7 400G IB网卡）会以冗余的方式，将其中的4张网卡连接到一个Leaf交换机，另外4张连接到另一个Leaf交换机。

Spine层：部署一个由多台大型Director Switch（如NVIDIA QM9700）组成的Spine层。每个Leaf交换机都全连接到所有的Spine交换机上。

无阻塞设计：精心设计Leaf-Spine之间的上行链路数量，实现一个1:1无阻塞的网络，确保任意两个GPU节点之间都能获得全速率的400Gbps带宽。

SHARP技术：NVIDIA Quantum-2平台支持SHARP（Scalable Hierarchical Aggregation and Reduction Protocol）技术，可以将`All-Reduce`等集体通信操作，直接在网络交换机中进行硬件计算和聚合，而不是在终端GPU上完成。这可以极大地降低`All-Reduce`的延迟，进一步提升分布式训练的扩展效率。

#### 存储与业务网络（Storage & General Purpose Network）

为了节约成本和简化布线，我们将存储和业务流量承载在一个统一的、物理分离的以太网网络上。

技术选型：采用基于NVIDIA Spectrum-X或同级别厂商的、支持RoCE的400Gb/s以太网交换机。

拓扑架构：同样采用一个独立的Spine-Leaf Clos架构。

连接：所有计算节点、Lustre存储节点、Ceph存储节点以及管理/API服务器，都通过其以太网卡连接到这个网络。

逻辑隔离：

通过VLAN或VRF技术，在逻辑上将存储流量和业务流量进行隔离。

对于连接Lustre的存储流量，如果需要使用RDMA（Lustre over RDMA），则需要在这个网络的相关部分，配置无损以太网（PFC/ECN）。

对于普通的业务流量和连接Ceph的TCP/IP流量，则运行在标准的、有损的以太网VLAN上。

#### 带外管理网络（Out-of-Band Management Network）

技术选型：采用一对冗余的、简单的10GbE以太网交换机。

拓扑：简单的星型或树形拓扑。

连接：所有服务器的BMC端口、所有IB和以太网交换机的管理端口、PDU等，全部连接到这个完全独立的管理网络上。

通过这套物理上三平面分离的网络设计，我们为不同类型的流量提供了最优的、互不干扰的承载通道：用最昂贵、性能最好的InfiniBand网络来伺服最宝贵的计算流量；用一个高性能、可划分的以太网来承载存储和通用业务；用一个简单可靠的管理网络来保障整个集群的“生命线”。

## 13.6 本章小结

在本章中，我们进行了一次激动人心的、从理论到实践的“毕业设计”。我们以极具挑战性的自动驾驶模型训练为靶心，将本书前面所有章节学习到的知识体系，进行了一次系统性的、端到端的综合应用，完整地设计了一个世界级的、基于云原生HPC理念的GPU计算平台。

我们的设计之旅始于对需求来源的深刻洞察。通过剖析自动驾驶复杂的“数据闭环”工作流，我们提炼出了其对基础设施在存储、计算、网络、弹性等多个维度上的极致、多样的需求。这为我们后续所有的架构决策提供了清晰的指引。

在总体设计中，我们确立了“Cloud-Native HPC”这一核心指导思想，旨在鱼与熊掌兼得——既要HPC的极致性能，又要Cloud的敏捷与弹性。这决定了我们后续的技术选型将是“两条腿走路”：用最硬核的HPC技术构建性能引擎，用最先进的云原生技术构建管理和应用平台。

在具体的分系统设计与实现中，我们做出了如下关键决策：

在计算层面，我们选择了HGX H100 8-GPU服务器作为标准计算单元，构建了1024卡的集群规模，并设计了TP+PP+DP的3D混合并行训练策略。在调度上，我们采用Kubernetes + Volcano的组合来支持大规模批任务的成组调度，并通过MIG技术实现了GPU的细粒度共享，兼顾了大规模训练和日常研发的需求。

在存储层面，我们构建了一个分层异构的存储体系：用低成本的Ceph对象存储构建EB级的“数据湖”；用高性能的、全NVMe的Lustre并行文件系统构建TB/s级吞吐的“热数据仓库”，并通过GPUDirect Storage喂养GPU；用高IOPS的Ceph块存储为系统和应用提供高可用的持久化卷。

在网络层面，我们设计了物理三平面的分离式网络架构：用极致性能的400G InfiniBand网络和Fat-Tree拓扑来承载核心的计算流量，并利用SHARP等技术进行硬件加速；用一个独立的、高性能的以太网来承载存储和通用业务流量；用一个完全隔离的带外管理网来保障整个集群的生命线。

至此，我们不仅完成了一次纸面上的设计，更重要的是，我们建立了一套完整的设计思维框架。我们学会了如何从一个复杂的业务需求出发，进行层层分解，然后在每一个技术点上，基于我们深厚的知识储备，进行合理的权衡与选型，并最终将所有这些选择，有机地整合成一个协调、高效、优雅的整体架构。

这正是本书希望传递给每一位读者的核心价值——不仅仅是知识的集合，更是一种架构的智慧。希望通过这次旅程，你已经准备好，去迎接大模型时代的挑战，去设计和构建属于你自己的、强大的AI基础设施。前路浩荡，未来可期。
