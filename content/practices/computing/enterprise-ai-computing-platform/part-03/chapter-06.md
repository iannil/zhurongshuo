---
title: "第六章：训练平台技术支撑"
date: 2025-11-07T00:00:00+08:00
description: "在第五章中，我们已经为算力平台构建了坚如磐石的基础架构。这如同我们已经修建好了宽阔平整的F1赛道，并配备了顶级的监控和调度中心。现在，是时候让真正的主角——承载着模型迭代与创新使命的“训练赛车”——驶上赛道了。模型训练，是人工智能价值创造的源头，也是算力消耗最大、技术挑战最密集的环节。"
draft: false
hidden: false
tags: ["书稿"]
keywords: ["企业级人工智能算力平台：构建、运营与生态", "第六章：训练平台技术支撑"]
slug: "chapter-06"
---

在第五章中，我们已经为算力平台构建了坚如磐石的基础架构。这如同我们已经修建好了宽阔平整的F1赛道，并配备了顶级的监控和调度中心。现在，是时候让真正的主角——承载着模型迭代与创新使命的“训练赛车”——驶上赛道了。模型训练，是人工智能价值创造的源头，也是算力消耗最大、技术挑战最密集的环节。一个百亿、千亿乃至万亿参数的大模型，其训练过程动辄需要成百上千张GPU卡协同工作数周甚至数月，任何一个环节的疏漏，都可能导致“一着不慎，满盘皆输”的严重后果——巨大的算力成本沉没、宝贵的研发时间窗口错失。

因此，构建一套专业、深入、全方位的训练平台技术支撑体系，是我们将算力资源转化为模型能力的关键“催化剂”和“倍增器”。本章将聚焦于训练环节的技术支撑，围绕三个核心议题展开：

1. “车”的打造与调校：如何高效地部署、联调与优化一个大规模的训练集群，确保其从诞生之初就具备最佳的性能基线？（6.1 训练集群的部署、联调与优化）
2. “赛”的全程护航：在漫长而脆弱的训练过程中，如何为重点训练任务提供高可靠性保障，最大限度地缩短故障恢复时间（MTTR），确保“比赛”顺利完成？（6.2 重点训练任务的高可靠性保障）
3. “车手”的技能提升：如何帮助用户的训练模型更好地适配我们的平台，并对其进行深度性能调优，让“车手”能够将“赛车”的性能压榨到极致？（6.3 训练模型的适配、迁移与性能调优）

通过对这三个问题的系统性解答，我们将完整地呈现一个从硬件到软件、从平台到模型、从性能到稳定性的立体化训练技术支撑矩阵。

## 训练集群的部署、联调与优化

一个高性能的训练集群，绝不是简单地将一堆AI服务器连接起来就能自动实现的。它是一个软硬件紧密耦合的复杂系统，计算、存储、网络三大子系统必须像一个交响乐团的不同声部一样，被精确地“联调”与“优化”，才能合奏出恢弘的性能乐章。任何一个声部的“跑调”或“掉链子”，都会成为整个乐团表现的短板。本节将详细阐述我们如何通过系统化的工程方法，确保每一个新建或扩容的训练集群都能达到“出厂即巅峰”的状态。

### 训练集群集成联调：从“零件”到“整车”的精密组装

集成联调是集群部署的收官阶段，也是最关键的“临门一脚”。其目标是验证各个独立的子系统（计算、存储、网络）和AI平台软件栈，在组合到一起后，能否作为一个整体协同工作，达到设计预期。

#### 算存网子系统间的联调

算存联调：验证计算节点能否正确、高效地挂载和读写存储资源。我们会进行基准测试，例如，使用`fio`或`iozone`等工具，从多个计算节点并发地对共享文件系统进行大文件读写测试，确保其聚合带宽和IOPS能够满足大规模数据加载的需求。我们会模拟真实的数据加载场景，排查可能存在的挂载权限问题、网络抖动导致的读写性能下降等问题。

算网联调：这是联调的重中之重。核心是验证GPU之间的通信性能。

单节点内P2P通信测试：使用`p2pBandwidthLatencyTest`等工具，测试同一台服务器内所有GPU卡之间的点对点（P2P）带宽和延迟，验证NVLink/NVSwitch或PCIe链路是否工作正常。

跨节点RDMA通信测试：使用`ib_write_bw`、`perftest`等工具，测试任意两个计算节点之间通过RoCE或InfiniBand网络的RDMA（远程直接内存访问）性能，确保网络配置（如PFC、ECN）正确无误，能够达到线速性能。

NCCL All-Reduce性能基准测试：使用`nccl-tests`工具，在不同规模（2、4、8...全量节点）的节点上，运行All-Reduce基准测试。这是最能模拟真实分布式训练通信负载的测试，其测试结果（Bus Bandwidth）是衡量整个集群并行计算效率的“金标准”。我们会将测试结果与NVIDIA官方发布的同类硬件配置的基准进行对标，任何显著的差距都预示着网络或系统配置存在问题。

#### AI平台的对接联调

资源调度联调：验证上层的Kubernetes+Volcano/YunIkorn调度器，能否正确识别和调度底层的GPU资源。我们会进行一系列的调度测试，如提交一个需要8卡GPU的MPI任务，验证调度器能否找到一个完整的8卡节点来运行它；或者提交一个需要跨节点运行的任务，验证Gang Scheduling（组调度）能否确保所有Pods被同时拉起。

监控系统联调：验证监控组件（如Prometheus、DCGM-Exporter）能否成功采集到所有节点的“算-存-网”指标，并在Grafana仪表盘上正确展示。

存储接口联调：验证AI平台能否通过CSI（容器存储接口）插件，为训练任务动态地创建和挂载所需的存储卷。

产出：《训练集群集成联调报告》

这份报告会详细记录所有联调测试的项目、方法、过程数据和最终结果，并与设计目标进行对比。对于未达标的项目，会附上详细的根因分析和解决方案。只有当所有联调项目都测试通过，这个集群才被认为具备了“上线交付”的基本条件。

### 训练集群指标设计：为“性能”建立“度量衡”

联调保证了集群的“可用”，而要衡量其“好用”与否，就需要一套科学的、可量化的指标体系。我们设计的这套指标体系，旨在从多个维度全面评估一个训练集群的综合能力。

集群计算规模指标：

- 理论总算力（TFLOPS）：集群所有GPU卡理论峰值算力（如FP16或BF16）的总和。这是衡量集群“肌肉”有多大的基础指标。
- 有效总算力（EFLOPS）：通过运行标准的HPL-AI等混合精度Linpack基准测试，得到集群在实际运行中的有效算力。这个指标更能反映集群的真实计算能力。

集群利用率指标：

- 物理利用率：集群中已被分配出去的GPU卡数占总卡数的比例。
- 逻辑利用率：在已分配的GPU卡中，其真实计算单元的平均使用率。物理利用率高而逻辑利用率低，是典型的“占着茅坑不拉屎”现象，是运营优化的重点。

集群稳定性指标：

- 平均无故障时间（MTBF）：集群作为一个整体，能够持续无重大故障运行的平均时间。
- 节点故障率：单位时间内，出现硬件或软件故障的计算节点的比例。

集群并行效率指标：

扩展效率：在使用N个节点进行分布式训练时，其训练速度相对于使用单个节点时的加速比，再除以N。理想情况下，扩展效率接近100%。这个指标是衡量集群网络架构和通信效率的核心。

产出：《训练集群指标设计报告》

这份报告定义了上述指标的精确计算口径、数据采集来源和展示方式，使其成为后续所有性能测试和优化的“通用语言”。

### 训练集群监测：用“数据”透视“健康”

基于设计的指标体系，我们构建了统一的集群监测平台，对集群进行持续的、深入的“体检”。

统一监测：将所有指标数据汇入统一的监控平台，并通过Grafana创建“训练集群健康度”专属仪表盘，实现对所有集群状态的集中看、统一管。

训练集群压力测试：

- 测试对象：我们会选择公司内部的典型大模型（如“光明电力”语义模型）作为测试对象，因为它最能代表真实的高负载场景。
- 测试方法：在不同的集群规模下（如32、64、128节点），进行“拉力测试”。我们会让模型满负荷运行一段时间，监测其各项功能（如Check-pointing、日志记录）是否正常，性能指标（如吞吐量、扩展效率）是否能达到预期基线。

产出：《训练集群压力测试报告》，用于验证集群在高负载下的性能表现和稳定性。

训练集群长稳测试：

- 测试方法：我们会使用开源的、业界公认稳定的模型（如GPT-3、LLaMA），在多个新建的训练集群上，进行长达数天甚至一周的不间断训练。这个过程就像“煲机”，旨在暴露那些在短期测试中难以发现的、偶发性的软硬件问题（如内存泄漏、网络瞬断、GPU过热降频等）。
- 稳定性优化：在长稳测试中发现的任何问题，都会被记录、分析，并推动厂商或内部团队进行修复和优化，直到集群能够通过7x24小时的稳定性考验。

产出：《训练集群稳定性测试报告》，是集群能够承载关键训练任务的最终“通行证”。

### 训练集群调优：从“能跑”到“飞跑”的艺术

通过了上述所有测试，集群已经具备了良好的基础。但要让其性能最大化，还需要进行一系列精细化的调优工作。这如同F1赛车在出厂后，还需要根据赛道和车手的特点进行个性化的调校。

#### 训练集群性能瓶颈分析

数据驱动的瓶颈定位：

- 建立性能基准线：首先，我们会运行一系列标准的Benchmark程序，建立起这个集群在“裸奔”状态下的性能基准线（如内存带宽、网络延迟、存储IOPS等）。
- 剖析真实业务负载：选择一个典型的业务训练任务，利用NVIDIA Nsight等工具，对其进行端到端的性能剖析。剖析结果会清晰地展示出在一个训练迭代中，时间的详细分布：哪些时间花在了数据加载（I/O），哪些花在了CPU预处理，哪些花在了GPU计算，哪些花在了节点间通信。
- 量化瓶颈影响：通过剖析图，我们可以清晰地定位到导致GPU利用率低（所谓“GPU饥饿”）的核心原因，并量化其影响。例如，“有30%的时间，GPU在等待CPU完成数据预处理”。

#### 训练集群调度优化

面向不同业务的调度策略：

- 拓扑感知调度：针对需要高带宽通信的大规模训练任务，我们会配置调度器，使其具备“拓扑感知”能力。调度器会优先将一个任务的所有Pods调度到同一个交换机下，甚至同一个机架内，最大限度地减少跨交换机的通信延迟。
- 资源抢占与公平共享：针对由多个小用户共享的研发集群，我们会引入更精细的公平共享调度策略（如DRF - Dominant Resource Fairness），确保每个用户都能获得相对公平的资源份额，避免某个“大户”长期霸占所有资源。

#### 训练集群优化效果评估

- 量化优化成果：所有的调优操作，都必须以数据来衡量其效果。我们会进行调优前后的AB测试，量化各项关键指标的提升。
- 资源效率维度：GPU平均利用率提升了多少个百分点？
- 性能提升维度：典型模型的训练吞吐量提升了多少？端到端的训练时长缩短了多少小时？
- 成本节约维度：完成同一个训练任务，所消耗的总算力成本（卡时）降低了多少？

产出：

- 《训练集群资源优化服务方案》：详细记录了瓶颈分析的过程、调优的具体措施和操作步骤。
- 《训练集群优化效果评估报告》：用数据和图表，清晰地展示优化带来的价值，为持续改进提供数据支撑，并作为最佳实践沉淀到知识库中。

通过这一整套从部署、联调到优化的系统化工程方法，我们确保了每一个训练集群的基础设施都达到了最佳状态，为上层承载的训练任务，提供了一个坚实、高效、可靠的“起跑平台”。

## 重点训练任务的高可靠性保障（MTTR管理）

一个万卡级别的、持续数月的大模型训练任务，其脆弱性超乎想象。在如此长的时间尺度和如此大的集群规模下，硬件故障（GPU卡、服务器、交换机）几乎是必然会发生的事件。每一次中断，都意味着宝贵的算力时间被浪费。因此，仅仅追求高性能是远远不够的，我们必须将高可靠性置于同等重要的位置。

我们引入了MTTR（Mean Time To Recovery，平均修复时间）作为衡量训练任务保障能力的核心运营指标。MTTR越短，说明我们从故障发生到恢复训练的能力越强，对训练效率的影响就越小。我们的保障体系，围绕“训练前、训练中、训练后”三个阶段，构建了一个全流程的MTTR管理闭环。

### 训练前：开展集群健康检查，防患于未然

最好的故障处理，是让故障不发生。在任何一个重点训练任务（特别是大规模、长周期的大模型训练）启动之前，我们都会进行一次“军事演习”级别的、深入的集群健康检查。

深度隐患排查：

- 硬件压力测试：我们会使用专门的工具（如`gpu-burn`、`stress-ng`），对即将参与训练的所有计算节点，进行持续数小时的满负荷压力测试，提前暴露那些在低负载下不易发现的、不稳定的硬件组件（如体质不佳的GPU、散热不良的CPU）。
- 网络全面体检：对所有参与节点的网络链路，再次进行点对点的带宽和延迟测试，确保没有任何一条“亚健康”的链路混入集群。
- 固件与驱动版本一致性检查：自动化脚本会扫描所有节点，确保其BIOS、BMC、网卡、GPU的固件和驱动程序版本完全一致，杜绝因版本不匹配导致的莫名问题。

快速检查与诊断压测：

- 训练任务前快速检查：在任务启动前的最后时刻，我们会运行一个轻量级的、标准化的“快速检查脚本”，用几分钟时间对集群的核心状态进行最后一次确认。
- 故障诊断压测：我们会模拟一些常见的故障场景（如手动kill一个Pod、拔掉一根网线），验证平台的故障检测和自愈能力（如Pod自动拉起）是否符合预期。
- 资源池变更压测：如果训练过程中计划进行节点替换或扩容，我们会对变更操作进行预演和压测，确保其不会对正在运行的任务造成冲击。

产出：《训练集群检查报告》

这份报告会列出所有检查项的结果。只有当所有检查项都显示“通过”，并且没有任何“警告”或“风险”项时，我们才会正式批准该重点训练任务启动。这种“先体检、后上场”的模式，极大地规避了可预防性的软硬件问题。

### 训练中：集群故障快速感知与恢复

尽管做了周密的预防，但在长周期的训练中，故障仍然可能发生。此时，快速的“感知-定位-隔离-恢复”能力，是缩短MTTR的关键。

快速感知：

- 多维度监控：我们的监控系统，会对训练任务的多个关键指标进行实时监控。
- 任务日志：通过Loki等日志聚合系统，实时监控任务输出的日志，并设置关键词告警（如`Error`、`Exception`、`NCCL timeout`）。
- 核心性能指标：实时监控训练的进度（Steps/sec）、Loss值、GPU平均算力等。任何一个指标的异常下跌或停滞，都可能是故障的征兆。
- 智能告警：告警系统会自动关联来自不同源（日志、指标、硬件监控）的信息，进行智能降噪和聚合，第一时间向运维人员发出精准的告警。

快速定位：

- 日志分析与关联：当告警发生时，运维人员可以通过统一的监控平台，快速下钻到异常时间点的任务日志、系统日志、DCGM指标和网络遥测数据，进行跨域的关联分析。例如，发现训练Loss不收敛，同时监控到某张GPU卡的ECC错误计数异常增加，就可以初步定位是该GPU卡硬件问题。
- 故障知识库：我们将历次故障处理的经验，沉淀为《训练集群故障处理知识库》。当遇到相似的告警或日志模式时，可以快速检索到历史的解决方案。

快速恢复：

- 硬件类问题的快速隔离：一旦定位到是某个硬件（如GPU卡、服务器）的问题，我们的首要原则是“先恢复训练，后排查硬件”。
- 热替换/热插拔：对于支持热插拔的组件，直接进行在线更换。
- 节点隔离与替换：如果整台服务器故障，运维人员会立即将该故障节点标记为“不可调度”，并从备用资源池中调取一个健康的节点加入训练（需要训练框架支持弹性伸缩）。
- 软件类问题的快速回滚：如果是由于软件升级或配置变更导致的故障，立即执行预案，回滚到上一个稳定版本。
- 与训练框架的联动：现代的深度学习框架（如PyTorch FSDP）都具备良好的容错能力。只要故障节点的数量不超过一定阈值，框架本身就能够在损失部分节点的情况下，自动缩减训练规模并继续运行，或者从最近的检查点自动恢复。我们的任务是尽快补充健康的节点，让训练恢复到最大规模。

产出：《训练集群故障处理报告》
每一次故障处理过程，都必须被详细记录下来，包括故障现象、定位过程、解决方案、恢复时间（MTTR）等。

### 训练后：集群故障总结分析与持续优化

每一次故障，都是一次宝贵的学习机会。在重点训练任务结束后，我们会组织专门的复盘会，对训练期间发生的所有故障进行总结分析。

- 分析故障模式：对所有的《故障处理报告》进行归类和统计分析，识别出最高频、影响最大的故障模式（Failure Mode）。例如，我们可能会发现“某一批次的GPU卡，在运行超过1000小时后，出现显存不稳定的概率显著增加”。
- 优化故障处理流程：根据复盘结果，对现有的故障处理预案和操作手册进行修订和优化，使其更具针对性和可操作性。
- 驱动平台和硬件的改进：将分析出的、系统性的硬件或软件缺陷，反馈给厂商，推动其在下一代产品或下一个软件版本中进行修复。
- 沉淀训练任务保障知识库：将总结分析的成果，系统性地沉淀到知识库中，包括新的故障模式、更优的诊断方法、更快的恢复流程等，形成知识的闭环和能力的迭代。

产出：《训练集群保障总结报告》

这份报告是整个重点任务保障工作的最终沉淀，也是我们持续降低MTTR、提升集群可用率的“路线图”。

通过这套“事前检查、事中快反、事后复盘”的全流程高可靠性保障体系，我们将大规模训练从一种“听天由命”的高风险活动，转变为一种“尽在掌握”的确定性工程，为公司最核心的AI研发任务，提供了最坚实的稳定保障。

## 训练模型的适配、迁移与性能调优

平台和集群是“舞台”，模型才是舞台上真正的“舞者”。一个再好的舞台，如果舞者不适应，或者舞技不佳，也无法呈现出精彩的演出。训练模型的适配、迁移与调优服务，正是我们作为“舞台技术指导”，帮助“舞者”发挥出最佳水平的核心工作。这项服务旨在解决用户在模型开发和训练中遇到的三大类问题：模型在我们的平台上“跑不起来”（适配）、模型从别的平台迁移过来后“水土不服”（迁移）、模型能跑但“跑得太慢”（调优）。

### 训练模型算力需求评估与适配风险预估

在模型正式上平台训练之前，进行一次深入的“体检”，是避免后续大量返工和问题的关键第一步。

评估现有与所需算力：

我们会与用户一起，利用在第四章介绍的测算工具，对其模型的算力需求（计算、显存、通信）进行精准评估。
同时，我们会评估用户当前拥有的（或计划申请的）算力资源配额，进行“需求-供给”的匹配度分析。

确认目标平台的支持能力：

- 硬件兼容性检查：确认模型的计算精度要求（如FP32、TF32、FP16、BF16）是否被目标GPU硬件所支持。
- 软件栈兼容性检查：全面检查用户代码所依赖的核心库（CUDA、cuDNN、NCCL、PyTorch等）的版本，是否与我们平台提供的标准环境兼容。对于不兼容的部分，需要评估其升级或降级的可行性和工作量。

提前预估适配风险：

- 自定义算子（Custom Ops）：检查用户的模型代码中是否包含大量依赖于特定硬件平台或CUDA版本的自定义C++/CUDA算子。这是模型迁移中最常见的“坑”，因为这些算子往往需要重新编译甚至重写。
- 私有或闭源依赖：检查是否存在对私有代码库或闭源软件的依赖，这可能带来授权和兼容性问题。

产出：《训练模型算力适配评估方案》

这份方案会给出一个清晰的结论：该模型是“可直接运行”、“需要少量适配”还是“存在重大适配风险”。对于需要适配的模型，方案中会附上详细的适配路径图、预估工作量和责任分工。

### 模型适配与迁移服务：扫清“水土不服”的障碍

对于评估后需要进行适配和迁移的模型，我们的技术支持团队将提供专家级的“陪跑”服务。

制定迁移适配路径：根据评估报告，制定详细的、分步骤的迁移适配计划，确保整个过程有序、可控。

提供代码改造支撑：

- 环境依赖适配：协助用户将其代码封装为标准的Docker镜像，解决环境依赖问题。
- 自定义算子重构与开发：对于最棘手的自定义算子问题，我们的专家会与用户一起，进行代码的重构，使其能够在新平台的CUDA版本下成功编译运行。对于一些关键的性能瓶颈算子，我们甚至可以提供高性能算子的定制开发服务。
- 模型结构调整：在必要时，建议用户对模型的部分网络结构进行微调，以更好地适应目标硬件的计算特性（如，使用能被Tensor Core加速的卷积核尺寸）。
- 保持模型精度：在所有适配改造过程中，我们始终将“保持模型原有精度”作为第一原则。我们会建立一套基准测试流程，在改造的每一个关键节点，都对模型的精度进行回归测试，确保适配工作没有引入非预期的负面影响。

### 训练模型调优：压榨每一滴性能

模型能够成功运行后，性能调优的大幕才刚刚拉开。我们的目标是，通过系统化的方法，将模型的训练效率提升到极致。

资源使用分析与瓶颈定位：这是调优的起点。我们会再次使用NVIDIA Nsight等剖析工具，对模型在真实训练负载下的表现进行深度分析，精准定位是计算瓶颈（Compute-bound）、访存瓶颈（Memory-bound）还是I/O瓶颈（I/O-bound）。

#### 提供全栈式的性能调优服务

数据流水线优化：

指导用户使用高效的数据加载库（如DALI、FFCV）。

优化数据预处理逻辑，将其从CPU端卸载到GPU端执行。

开启多进程数据加载（`num_workers`），并启用内存锁定（`pin_memory`），确保数据能够被快速送入GPU。

混合精度训练（Mixed Precision Training）：指导用户开启AMP（Automatic Mixed Precision），利用GPU的Tensor Core单元，在几乎不损失精度的情况下，将训练速度提升2-3倍，并大幅降低显存占用。

分布式训练策略优化：

根据模型的特点和集群网络状况，为用户推荐最优的并行策略组合。例如，对于参数量巨大但激活值不大的模型，推荐使用ZeRO-3（一种高级的数据并行）+张量并行；对于层数很深的模型，推荐引入流水线并行。

我们会协助用户对其训练代码进行改造，以支持这些高级的并行策略。

- 梯度累积（Gradient Accumulation）：指导用户在显存不足以支持大Batch Size时，通过梯度累积技术，实现等效的大批量训练。
- 算子融合与编译优化：指导用户使用PyTorch 2.0的`torch.compile()`或NVIDIA的Apex等工具，将多个小算子自动融合成一个大的GPU Kernel，减少Kernel Launch的开销，提升计算效率。
- Checkpointing优化：对于需要频繁保存快照的大模型，指导用户使用异步的、分布式的Checkpointing方案，减少其对训练主流程的阻塞。

产出：《训练算力模型调优方案》
这份方案会像一份详细的“体检报告+健身计划”，清晰地列出模型的性能瓶颈、具体的优化建议（附带代码示例）和预期的性能提升目标。

### 适配后模型验证与部署实施服务

- 适配迁移验证：在完成所有的适配和调优工作后，我们会进行一次正式的、端到端的验证测试。
- 功能与核心性能指标评估：在标准数据集上，对模型的核心功能和精度指标进行最终验证，确保与迁移前完全一致。
- 稳定性与并发压力测试：进行长时间的稳定性测试，并模拟多用户并发提交训练任务的场景，验证模型在复杂负载下的稳定性。

产出：《训练算力模型适配迁移验证报告》

训练模型部署实施服务：

- 编制《训练模型部署实施方案》：我们将整个适配和调优后的模型、代码、环境、最佳实践参数，打包成一个标准化的“训练解决方案包”。
- 提供部署实施技术支撑：协助用户将这个方案包，在目标算力资源下进行规模化的部署和推广，确保其他团队或项目可以“一键复现”最佳的训练效果。

产出：《模型部署实施报告》

## 本章小结

第六章，我们深入探索了训练平台技术支撑的“无人区”。从集群的部署、联调与优化，我们确保了“舞台”的坚固与高效；通过重点任务的高可靠性保障，我们为“演出”提供了全程无忧的“护航”；最后，通过模型的适配、迁移与性能调优，我们帮助“舞者”展现出了最巅峰的“舞技”。这一系列环环相扣、层层深入的技术支撑服务，共同构成了一个强大的赋能体系。它不仅解决了用户在模型训练中可能遇到的各种技术难题，更重要的是，它将公司级的、顶尖的工程优化能力，以服务的形式，传递给了每一位AI开发者，真正实现了“让专业的人做专业的事”，让算法科学家可以专注于算法创新，让业务专家可以专注于场景挖掘，从而最大限度地加速了人工智能在企业内的创新与落地进程。
