---
title: 第十二章 评估的异化：当信任成为可计算的商品
date: 2025-10-19T00:00:00+08:00
draft: false
hidden: false
tags: ["书稿"]
keywords: ["信任的基底：从血缘到算法", "第十二章 评估的异化：当信任成为可计算的商品"]
slug: "chapter-12"
---

我们正站在一个历史的临界点上。信任，这个维系人类社会运转的、最古老、最核心的"神圣黏合剂"，正在经历一场前所未有的、由技术所驱动的深刻"变性"。在算法的全知凝视之下，在代码的冷酷逻辑之中，信任，正在从一种根植于人性、历史与关系的、不可言说的"德性"，异化为一种可以被精确计算、可以被大规模交易、可以被自动化管理的"商品"。

这种"信任的商品化"，其后果，远比我们想象的更为深远和令人不安。它不仅仅是改变了我们评估彼此的方式，它正在从根本上，重塑我们与权力之间的关系，重塑我们对"公正"与"惩罚"的理解，并最终，重塑我们作为"人"的、最核心的"主体性"地位。

当信任成为可计算的商品时，评估，便完成了其终极的"异化"。它不再是一种旨在促进理解、修复关系、维系社群和谐的"社会技艺"，而蜕变为一种旨在进行风险控制、规训行为、提升系统效率的"统治技术"。

在这套新的统治技术之下，一种全新的、更隐蔽、也更难反抗的"不自由"，正在降临。它不再以镣铐和监狱的粗暴形式出现，而是以"便利"、"安全"和"个性化推荐"的温柔面目，渗透进我们生活的每一个毛孔。这是一种"天鹅绒"般的牢笼，而我们，正心甘情愿地，甚至带着一丝感激，步入其中。

## 强制自愿：信息垄断下的不平等契约

算法评估这台宏伟机器的运转，其合法性的基石，建立在一个看似无可辩驳的现代法律原则之上："知情同意"(Informed Consent)。

每一次我们下载一个新的 App，注册一个新的网站，使用一个新的数字服务时，我们都会看到一个冗长的、充满了法律术语的《用户协议与隐私条款》。我们被要求，去仔细阅读它，理解它，并最终，勾选那个小小的方框："我已阅读并同意"。

通过这个"勾选"的动作，我们，似乎是在行使我们作为自由个体的"契约自主权"。我们，是在与平台公司，签订一份"自愿"的契约。在这份契约中，我们同意，用我们的个人数据，去交换平台所提供的免费服务。

这听起来，是一场公平的、两厢情愿的交易。然而，在这幅"自愿"的图景之下，隐藏着一个深刻的、结构性的"不平等"。这种不平等，源于交易双方，在信息、权力和可选择性上，存在的巨大鸿沟。其结果，是一种全新的、被学者肖莎娜·朱伯夫称为"监视资本主义"(Surveillance Capitalism)的剥削范式，以及一种我们可以称之为"强制自愿"(Coerced Voluntarism)的虚假契约关系。

第一，信息与知识的"绝对不对称"。

一份典型的《隐私条款》，其长度，可能达到数万字，其语言，充满了普通人难以理解的法律与技术术语。它会用极其模糊和宽泛的措辞，来描述平台将收集哪些数据(例如，"设备信息"、"日志信息"、"位置信息"、"其他相关信息")，以及将如何使用这些数据(例如，"用于改善我们的服务"、"用于个性化推荐"、"用于与我们的合作伙伴共享")。

作为一个普通用户，你，面临着一个"认知上不可能完成的任务"：

你无法完全理解，这些条款，在技术上，到底意味着什么。当它说要收集"设备信息"时，这是否包括你麦克风的录音，或者你相册里的照片？当它说要与"合作伙伴"共享时，这些"合作伙伴"究竟是谁？

你无法预见，你今天所交出的这些看似无害的数据，在未来，与成千上万个其他数据点相结合，被一个你一无所知的机器学习模型进行分析后，可能会被用来，推断出关于你的、哪些极其私密的、你甚至自己都不知道的"事实"(例如，你的政治倾向、你的性取向、你潜在的健康风险)。

而另一方，平台公司，则拥有着一支由顶尖的律师、数据科学家和产品经理所组成的庞大军团。他们，精确地知道，每一个条款背后的法律含义；他们，也精确地知道，要如何利用你所交出的每一个比特的数据，来最大化他们的商业利益。

在这场所谓的"契约谈判"中，双方的信息与知识，处于一种绝对不对称的状态。这与其说是一场平等的"签约"，不如说是一场精心设计的"认知俘获"。平台，利用了你的认知局限，诱导你签署了一份你永远无法真正"知情"的"同意书"。

第二，权力的"结构性失衡"。

更重要的是，即使你，作为一个极其聪明的、懂法律懂技术的用户，完全理解了这份协议背后的所有风险，并且，你并不同意其中的某些条款(例如，你不想让它访问你的通讯录)，你，在绝大多数情况下，也没有任何"讨价还价"的权力。

你不能说："我同意你们的服务条款，但我不同意你们的隐私条款。"

你也不能说："我同意你们收集我的消费数据，但我不同意你们收集我的位置数据。"

你所面临的，是一个"全有或全无"(Take-it-or-leave-it)的霸王条款。你要么，全盘接受它所有的、侵入性的数据索取要求；要么，你就被彻底地，排除在这项服务之外。

在一个数字化的、高度互联的现代社会，被排除在一项基础性的数字服务(如即时通讯、移动支付、搜索引擎)之外，其代价，是极其高昂的。它可能意味着，你将无法与朋友保持联系，无法在商店里进行支付，无法获取必要的信息，甚至无法找到一份工作。

当一项服务，已经从一个可有可无的"消费品"，演变为一种不可或缺的"社会基础设施"时，所谓的"自愿退出"的权利，便在很大程度上，成为了一种"理论上的自由"。

平台公司，通过其对这种"数字基础设施"的垄断，实际上，已经拥有了一种"准公共权力"。它们，有能力，去单方面地，为整个社会，设定"数据交换"的默认规则。而我们，作为被这个系统所"网罗"的个体，除了"同意"，几乎别无选择。

第三，信任的"单向度"评估。

在这种不平等的契约关系中，信任的流动，是完全单向的。

我们，被要求，去无条件地信任平台。我们要信任它会"负责任地"使用我们的数据，信任它的安全系统是"牢不可破"的，信任它的算法是"公平公正"的。而当它违背了这些信任时(例如，发生了大规模的数据泄露)，我们，除了在社交媒体上表达一下愤怒，几乎没有任何有效的、可以对其进行惩罚的手段。

而平台，却无需对我们，表现出任何对等的信任。相反，它所做的每一件事，其底层的逻辑，都是对我们的"深刻不信任"。它之所以要收集我们的一切数据，之所以要用算法来为我们评分，其根本目的，就是为了"绕过"对我们人性的信任，而去寻找一种更"可靠"的、基于数据的"确定性"。

我们，将自己最宝贵的、最私密的"内在世界"(我们的思想、情感、欲望)，以数据的形式，毫无保留地，敞开给平台。而平台，回馈给我们的，却是一个冷冰冰的、拒绝对我们敞开其"内在世界"(其算法的运作逻辑)的"黑箱"。

这就是"强制自愿"的本质：我们，被迫，在一个由信息垄断者所精心设计的、充满了不对称权力关系的框架内，去"自愿地"，交出我们用以反抗这种权力的、最后的武器——我们的数据隐私。

我们，签署的，不是一份平等的契约，而是一份"数字化的附庸契约"。在这份契约中，我们，不再是作为与平台平等的"用户"，而是作为被平台所"牧养"的"数据奶牛"而存在。我们存在的价值，就在于持续不断地，为这台巨大的、以"监视"为燃料的资本主义机器，提供其赖以生存的"数据养料"。而我们，为了换取那点便利的"草料"，还必须对这种"被牧养"的命运，表示"知情并同意"。

## 预防性惩罚：基于算法预测的信任剥夺

当算法评估的权力，与国家或公司的强制执行能力相结合时，一种全新的、更具争议性的治理模式，便浮现了出来。它不再满足于对"已经发生"的失信行为，进行"事后"的惩罚；它试图，更进一步，去对那些"未来可能发生"的失信行为，进行"事前"的干预与"预防"。

这就是"预防性惩罚"(Pre-emptive Punishment)或"预测性治理"(Predictive Governance)的兴起。其核心逻辑是：如果我们，能够通过一个足够精准的算法模型，去预测出某个人，在未来，有"很高的概率"，会成为一个"失信者"(例如，一个罪犯、一个贷款违约者、一个恐怖分子)，那么，我们，就有理由，在这个"坏事"发生之前，就对他/她，采取行动——剥夺其信任，限制其自由，降低其潜在的"社会风险"。

这种基于"预测"而非"事实"的惩罚逻辑，正在从科幻电影(如《少数派报告》)的想象，一步步地，走进我们的现实。

从"风险社会"到"算法风险社会"

社会学家乌尔里希·贝克曾指出，现代社会，是一个"风险社会"(Risk Society)。我们所面临的主要威胁，不再是来自外部自然的、可见的危险(如饥荒、瘟疫)，而是来自我们自身技术与社会系统内部的、不可见的、概率性的"风险"(如核泄漏、金融危机、气候变化)。

在这样一个社会里，治理的核心，不再是分配"善"，而是分配"恶"，即，如何去预测、评估、管理和分配那些无所不在的"风险"。

而算法，则为这种"风险治理"，提供了一个前所未有的、强大的"技术工具箱"。它将传统的、基于人类经验和直觉的"风险评估"，升级为一种基于海量数据和机器学习的、看似更"科学"、更"精准"的"算法化风险评估"。

预测性警务(Predictive Policing)：美国的许多警局，开始使用一种名为 PredPol 的算法系统。这个系统，通过分析历史犯罪数据、天气、社会事件等参数，来预测在未来的某个时间段，哪个街区，是"犯罪热点地区"。然后，警局，就会将有限的警力，优先部署到这些被算法所"标记"出来的区域，进行"预防性"的巡逻和盘查。

自动化保释与量刑系统：前文提到的 COMPAS 系统，就是一个典型的例子。法官，在决定一个被告，是否应该被保释、或者应该被判处多长的刑期时，会参考该系统，对这个被告"未来再犯风险"所给出的一个分数。一个被算法评估为"高风险"的个体，即使他/她此次所犯的罪行并不严重，也可能，会因为其"潜在的危险性"，而被拒绝保释，或被判处更长的刑期。

反恐监控与名单筛选：各国的情报机构，正在利用机器学习，来分析海量的通讯数据、网络行为数据、金融交易数据，试图从中，去"识别"出那些具有"恐怖主义倾向"的"潜在威胁分子"。一个人，可能会因为其在网上的搜索记录、社交网络上的言论、以及其人际交往的图谱，而被算法，标记为一个"高危个体"，并被秘密地，放入各种"禁飞名单"或"监控名单"之中。

在这些场景中，一个深刻的、颠覆性的法治原则转变，正在发生。传统的法治精神，其核心，是"无罪推定"(Presumption of Innocence)和"惩罚已然之罪"(Punishing the committed crime)。你，只有在你被证明，已经实施了某个具体的、非法的行为之后，才能受到惩罚。

而"预防性惩罚"的逻辑，则恰恰相反。它，是一种"有罪(风险)推定"(Presumption of Guilt/Risk)和"惩罚未然之罪"(Punishing the potential crime)。你，仅仅因为你的"数据画像"，与那些"历史上曾经犯过罪的人"的画像，具有高度的"相似性"，你，就被提前地，被算法，宣判为是一个"危险的"、"不可信的"人。

信任的"算法化剥夺"

这种基于预测的"信任剥夺"，其最可怕之处，在于它的"自我实现"与"循环论证"的性质。

让我们回到"预测性警务"的例子：

1. 算法，因为历史数据的偏见，将一个以少数族裔为主的、贫困的社区，标记为"犯罪热点地区"。
2. 于是，更多的警察，被派往这个社区。他们，自然地，就会在这个社区里，发现和逮捕更多的、哪怕是轻微的违法行为(例如，持有少量大麻、街头游荡)。
3. 这些新的"逮捕数据"，又被作为新的"证据"，喂回到算法模型之中，进一步"证实"并"强化"了算法最初的"预测"：看，这个社区，果然就是一个高犯罪率的地区！
4. 于是，在下一轮的预测中，这个社区，会被赋予更高的"风险权重"，吸引来更多的警力，导致更多的逮捕……

这是一个完美的"算法反馈闭环"(Algorithmic Feedback Loop)。一个最初可能只是基于统计偏见的"预测"，通过其对现实世界资源(警力)的"不公平分配"，最终，自我实现为一个"客观的事实"。

生活在这个被算法所"诅咒"的社区里的居民，就陷入了一个无法逃脱的"信任剥夺"的循环之中。他们，作为一个群体，被系统性地，预先地，打上了"不可信"的标签。他们，会受到更频繁的盘查、更严厉的监控、更苛刻的司法对待。而他们，在这种高压环境下，所产生的一切"反抗"或"不合作"的行为，又会被系统，解读为是他们"危险性"的进一步"确证"。

"数字化的贱民"

在这种"预防性惩罚"的体制下，我们，正在创造出一批"数字化的贱民"(Digital Untouchables)。

这些人，可能，从未真正地，犯下任何严重的罪行。但是，他们的"数据分身"(Data Double)，却因为种种他们自己也无法控制的原因(例如，他们的出身、他们的肤色、他们所居住的社区、他们不幸的家庭历史)，而被算法，永久地，钉在了"高风险"的耻辱柱上。

他们，将被系统性地，排除在正常的社会生活之外：

他们，将无法获得银行的贷款，去创业或购买房屋。

他们，将无法通过大公司的自动化招聘筛选。

他们，在穿越国境时，会受到更严格的盘查。

他们，会被房东，在租房背景审查中，直接拒绝。

他们，被剥夺的，不仅仅是信任，更是"机会"本身。他们，被困在了由算法所铸就的、一个关于他们"过去"的、统计学意义上的"牢笼"里，而通往一个不同的、开放的"未来"的可能性之门，则被冷酷地，关上了。

这是一种比任何有形的监狱，都更可怕的"惩罚"。因为它，是无形的、弥散性的、终身性的，而且，几乎是无法申诉的。你如何能去向一个算法，证明你"不会"在未来犯罪？你如何能去与一个概率，进行辩论？

当信任的评估与剥夺，不再需要等待一个"事实"的发生，而只需要一个"预测"的输出时，我们，就将权力，赋予了一个冷酷的、不眠不休的、没有同情心的"算法先知"。而这个先知，它所预测的未来，很可能，只是一个被它自己的偏见所"污染"了的、不断自我重复的"过去"而已。

## 人的"工具化"：在追求确定性中丧失主体性

信任，成为可计算的商品；惩罚，基于对未来的预测。这一系列深刻的"异化"，最终，将导向一个更为根本的、关乎我们"人之为人"的终极危机：人的"工具化"(Instrumentalization of the Human)。

在一个由算法所主导的评估体系中，人，不再被视为一个拥有内在价值、拥有自由意志、其本身就是"目的"的"主体"(Subject)；人，被降格为，一个可以被测量、被预测、被管理、用以实现某个更高系统目标(例如，效率最大化、风险最小化、社会稳定)的"客体"(Object)或"工具"(Instrumente)。

这种"工具化"，体现在我们与外部世界，以及与我们自身，关系的全面重塑之上。

外部关系：从"社会人"到"系统节点"

传统的社会理论，无论是涂尔干的"社会连带"，还是马克思的"阶级关系"，其核心，都是将人，理解为一个生活在复杂的、有机的"人际关系网络"之中的"社会人"。我们的行为，是由我们与他人之间的规范、情感、权力和文化所塑造的。

而算法评估的逻辑，则试图将这个复杂的"社会"，解构为一个由无数个孤立的、可计算的"系统节点"(System Nodes)所构成的"技术系统"。

在这个技术系统中：

我们与其他人的关系，不再是首要的。首要的，是我们每一个独立的节点，与那个中心的"算法大脑"之间的关系。我们，不再是相互对话，而是各自，在与那个"系统"，进行着单向的"数据交换"。

我们的行为，其意义，不再是由我们所处的那个具体的"社群文化"来赋予的。其意义，完全是由它作为"数据输入"，对于那个算法模型，会产生何种"输出结果"，来定义的。一个"微笑"，其意义，不再是表达友善，而是为了换取一个"五星好评"；一次"捐款"，其意义，不再是出于同情，而是为了提升自己的"信用分数"。

我们行动的目标，不再是去追求那些模糊的、难以量化的"善"(如友谊、荣誉、智慧)，而是去追求那些清晰的、由系统所设定的"指标"(如 KPI、评分、排名)。

我们，被从那个温暖的、有机的、充满了模糊性与可能性的"社会母体"中，被"拔出"，然后，像一个个标准化的"插头"一样，被"插入"到一个冰冷的、机械的、追求极致效率与确定性的"技术矩阵"之中。我们，成为了这台宏伟机器上，一个可以被替换的、功能性的"零部件"。

内部关系：从"完整的人"到"可优化的数据组合"

这种"工具化"，不仅体现在我们的外部关系上，更深刻地，体现在我们看待我们"自身"的方式上。

我们，开始用一种"算法的"眼光，来审视和管理我们自己。我们，将自己，这个曾经被认为是神秘的、深不可测的、拥有一个"灵魂"的"完整的人"，解构为一系列可以被追踪、被量化、被"优化"(Optimize)的"数据组合"。

这就是"量化自我"(Quantified Self)运动背后，所潜藏的深刻哲学转变。

我们，用智能手环，来优化我们的"睡眠分数"；我们，用健身 App，来优化我们的"卡路里摄入"；我们，用时间管理软件，来优化我们的"生产力指标"。

我们，开始像一个公司的 CEO，在管理一张"资产负债表"一样，去管理我们自己的"生命"。我们的人脉，是我们的"社交资本"；我们的知识，是我们的"人力资本"；我们的健康，是我们的"生物资本"。

我们人生的终极目标，似乎，就是成为一个"最优化的自己"(The Optimized Self)。一个睡眠充足、饮食健康、工作高效、社交广泛、情绪稳定、信用分数高企的、完美的"数据档案"。

在这个"自我优化"的无尽循环中，我们，成为了我们自己，最苛刻的"工头"，最冷酷的"数据分析师"。我们，将自己，分裂为一个"作为监督者的我"，和一个"作为被监督、被优化对象"的我。

那个曾经被我们用来体验世界、感受情感、进行道德抉择的、统一的"自我"，被瓦解了。取而代之的，是一个被数据所"异化"了的、时刻处在一种"自我监控"的焦虑之中的"项目经理式自我"。

在追求确定性中，丧失主体性

最终，这场以"追求确定性"为名的、宏大的算法评估革命，其所导向的，可能是"主体性"(Subjectivity)的全面丧失。

"主体性"，这个启蒙运动以来，西方哲学最核心的概念，其本质，是指人，作为一种拥有理性、自由意志和道德自主性的存在，有能力，去为自己立法，去创造意义，去成为自己生命故事的"作者"，而不是被动地，被外在的、无论是神圣的还是自然的"法则"所决定的"傀儡"。

而算法评估的统治，正是在从三个层面上，系统性地，瓦解着这种"主体性"：

1. 它用"相关性"取代了"因果性"，从而瓦解了"理性"。理性的核心，是去探寻事物背后的"为什么"。而算法的"黑箱"，则告诉我们，"为什么"不重要，重要的是，去发现那些能够带来精准预测的、不可解释的"是什么"的关联。我们，被鼓励，去放弃对世界"可理解性"的追求，而去拥抱一种纯粹的、工具性的"可操作性"。
2. 它用"预测"取代了"选择"，从而瓦解了"自由意志"。自由意志，体现在我们面对一个开放的未来时，做出"非决定性"选择的能力。而预测性算法，则试图，通过对我们过去数据的分析，来"关闭"这个开放的未来，来告诉我们，"你，最有可能，会做出什么选择"。而当这个"预测"，又通过各种"个性化推荐"和"助推"(Nudge)技术，反过来，塑造我们的选择时，我们，就陷入了一个"被预测的自由"的悖论之中。
3. 它用"分数"取代了"判断"，从而瓦解了"道德自主性"。道德自主性，体现在我们不依赖于外在的奖惩，而去依据我们内心的"道德律"，来做出"何为善"的判断。而一个无所不在的评分系统，则将我们的道德判断，外包给了那个算法。我们行动的善恶，不再取决于我们内心的尺度，而取决于它，会对我们的那个"分数"，产生何种影响。

一个完全由算法所评估和管理的世界，将是一个极其"确定"的世界。在这个世界里，风险，被最小化；效率，被最大化；每一个人的行为，都在其应有的、被计算好的"轨道"上，精确地运行。

然而，那，也将是一个不再有"惊奇"的世界。一个不再有真正的"冒险"的世界。一个不再有"浪子回头"的"第二次机会"的世界。一个不再需要我们，去运用我们那不完美的理性、去承担我们那沉重的自由、去进行我们那痛苦的道德挣扎的"人的世界"。

在那个由算法所谱写的、完美的"天堂"里，我们，或许，会获得前所未有的安全与舒适。但我们，为此付出的代价，可能是我们，之所以为"人"的、那份最宝贵的、充满了不确定性的"可能性"本身。而这或许，才是评估的终极"异化"——我们，在用一种最精密的"评估"，去最终"取消"那个唯一值得被评估的、我们自己的"主体性"。
