<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><meta name=google-site-verification content="8_xpI-TS3tNV8UPug-Q6Ef3BhKTcy0WOG7dEdAcm2zk"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="祝融"><link rel=dns-prefetch href=//cdn.jsdelivr.net><link rel=preconnect href=https://cdn.jsdelivr.net crossorigin><link rel=dns-prefetch href=//www.googletagmanager.com><link rel=preconnect href=https://www.googletagmanager.com crossorigin><link rel=canonical href=https://zhurongshuo.com/practices/season-4/intelligent-computing-center-construction-guide/part-01/chapter-04/><title>祝融说。 第4章 GPU服务器的设计与实现</title><meta property="og:title" content="第4章 GPU服务器的设计与实现"><meta property="og:url" content="https://zhurongshuo.com/practices/season-4/intelligent-computing-center-construction-guide/part-01/chapter-04/"><meta property="og:site_name" content="祝融说。"><meta property="og:locale" content="zh_CN"><meta property="og:type" content="article"><meta property="article:published_time" content="2025-12-07T00:00:00+08:00"><meta property="article:modified_time" content="2025-12-07T00:00:00+08:00"><meta property="article:tag" content="书稿"><meta name=description content="在前三章中，我们已经建立了一个自顶向下的完整认知：我们理解了AI大模型对算力的极致渴求，熟悉了驾驭算力的软件栈，也深入剖析了算力之源——GPU芯片的内部微观架构。现在，我们拥有了最强大的发动机（GPU），也知道了如何驾驶它（软件栈）。接下来的任务，就是围绕这颗发动机，设计并制造出一台性能均衡、稳定可靠、能够发挥其极限潜能的超级跑车——GPU服务器。
"><meta property="og:description" content="在前三章中，我们已经建立了一个自顶向下的完整认知：我们理解了AI大模型对算力的极致渴求，熟悉了驾驭算力的软件栈，也深入剖析了算力之源——GPU芯片的内部微观架构。现在，我们拥有了最强大的发动机（GPU），也知道了如何驾驶它（软件栈）。接下来的任务，就是围绕这颗发动机，设计并制造出一台性能均衡、稳定可靠、能够发挥其极限潜能的超级跑车——GPU服务器。
"><meta property="og:image" content="https://zhurongshuo.com/images/favicon.ico"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="第4章 GPU服务器的设计与实现"><meta name=twitter:description content="在前三章中，我们已经建立了一个自顶向下的完整认知：我们理解了AI大模型对算力的极致渴求，熟悉了驾驭算力的软件栈，也深入剖析了算力之源——GPU芯片的内部微观架构。现在，我们拥有了最强大的发动机（GPU），也知道了如何驾驶它（软件栈）。接下来的任务，就是围绕这颗发动机，设计并制造出一台性能均衡、稳定可靠、能够发挥其极限潜能的超级跑车——GPU服务器。
"><meta name=twitter:image content="https://zhurongshuo.com/images/favicon.ico"><meta name=keywords content="智算中心建设指南：大模型算力的基础架构,第4章 GPU服务器的设计与实现"><link rel="shortcut icon" href=https://zhurongshuo.com/images/favicon.ico><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/normalize.css><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/animate-custom.css><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/zozo.css><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/remixicon-custom.css><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/highlight.css><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","headline":"第4章 GPU服务器的设计与实现","description":"在前三章中，我们已经建立了一个自顶向下的完整认知：我们理解了AI大模型对算力的极致渴求，熟悉了驾驭算力的软件栈，也深入剖析了算力之源——GPU芯片的内部微观架构。现在，我们拥有了最强大的发动机（GPU），也知道了如何驾驶它（软件栈）。接下来的任务，就是围绕这颗发动机，设计并制造出一台性能均衡、稳定可靠、能够发挥其极限潜能的超级跑车——GPU服务器。\n","datePublished":"2025-12-07T00:00:00\u002b08:00","dateModified":"2025-12-07T00:00:00\u002b08:00","author":{"@type":"Person","name":"祝融"},"publisher":{"@type":"Organization","name":"祝融说。","logo":{"@type":"ImageObject","url":"https:\/\/zhurongshuo.com\/images\/favicon.ico"}},"image":"https:\/\/zhurongshuo.com\/images\/favicon.ico","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/zhurongshuo.com\/practices\/season-4\/intelligent-computing-center-construction-guide\/part-01\/chapter-04\/"},"keywords":"书稿"}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"首页","item":"https:\/\/zhurongshuo.com\/"},{"@type":"ListItem","position":2,"name":"practices","item":"https:\/\/zhurongshuo.com\/practices/"},{"@type":"ListItem","position":3,"name":"第4章 GPU服务器的设计与实现","item":"https:\/\/zhurongshuo.com\/practices\/season-4\/intelligent-computing-center-construction-guide\/part-01\/chapter-04\/"}]}</script></head><body><div class="main animate__animated animate__fadeInDown"><div class="nav_container animated fadeInDown"><div class=site_nav id=site_nav><ul><li><a href=https://zhurongshuo.com/>首页</a></li><li><a href=https://zhurongshuo.com/start/>开始</a></li><li><a href=https://zhurongshuo.com/advanced/>进阶rc</a></li><li><a href=https://zhurongshuo.com/posts/>归档</a></li><li><a href=https://zhurongshuo.com/tags/>标签</a></li><li><a href=https://zhurongshuo.com/about/>关于</a></li></ul></div><div class=menu_icon><a id=menu_icon><i class=ri-menu-line></i></a></div></div><div class="header animated fadeInDown"><div class=site_title_container><div class=site_title><h1><a href=https://zhurongshuo.com/><span class=web-font>祝融说。</span></a></h1></div><div class=description><p class=sub_title>法不净空，觉无性也。</p><div class=my_socials><a href=https://zhurongshuo.com/books/ title=book-open><i class=ri-book-open-line></i></a>
<a href=https://zhurongshuo.com/practices/ title=trophy><i class=ri-trophy-line></i></a>
<a href=https://zhurongshuo.com/gallery/ title=gallery><i class=ri-gallery-line></i></a>
<a href=https://zhurongshuo.com/about/ title=game><i class=ri-game-line></i></a>
<a href type=application/rss+xml title=rss target=_blank><i class=ri-rss-fill></i></a></div></div></div></div><div class=content><div class=post_page><div class="post animate__animated animate__fadeInDown"><div class="post_title post_detail_title"><h2><a href=https://zhurongshuo.com/practices/season-4/intelligent-computing-center-construction-guide/part-01/chapter-04/>第4章 GPU服务器的设计与实现</a></h2><span class=date>2025.12.07</span></div><div class="post_content markdown"><p>在前三章中，我们已经建立了一个自顶向下的完整认知：我们理解了AI大模型对算力的极致渴求，熟悉了驾驭算力的软件栈，也深入剖析了算力之源——GPU芯片的内部微观架构。现在，我们拥有了最强大的发动机（GPU），也知道了如何驾驶它（软件栈）。接下来的任务，就是围绕这颗发动机，设计并制造出一台性能均衡、稳定可靠、能够发挥其极限潜能的超级跑车——GPU服务器。</p><p>GPU服务器远非将几张GPU卡简单地插在一块普通服务器主板上。特别是在大模型时代，一台顶级的GPU服务器，其本身就是一件凝聚了系统工程、高速信号、散热设计、电源管理等领域顶尖智慧的精密仪器。它的设计目标只有一个：消除一切可能存在的瓶颈，确保8个甚至16个昂贵的GPU能够7x24小时不间断地、以最高效率协同工作。</p><p>本章，我们将以业界公认的“黄金标准”和参考设计——NVIDIA DGX系列——作为我们的核心研究对象。通过对DGX服务器抽丝剥茧般的分析，我们将学习到一台顶级GPU服务器的设计精髓。虽然本章目录以DGX A100为例，但其设计理念和核心架构对理解最新的DGX H100同样至关重要，我们将结合两者进行讲解，让读者能够洞悉其演进脉络。</p><p>我们将首先初识NVIDIA DGX，理解它为何不仅仅是一台服务器，而是一个集成了硬件、软件和支持服务的“AI超级计算机 in a box”。接着，我们将深入DGX A100的总体设计，一窥其内部乾坤。然后，我们将像解剖精密机械一样，逐一剖析其四大关键子系统：CPU与内存子系统、PCIe子系统、NVLink子系统，以及其他辅助子系统（如存储、网络、电源和散热）。</p><p>通过本章的学习，你将能够回答以下问题：为什么需要为GPU配备如此强大的CPU和海量的内存？PCIe交换机在其中扮演了什么关键角色？NVLink和NVSwitch是如何构建起GPU之间的高速公路的？如何为这些“电老虎”提供稳定、冗余的电力，并带走它们产生的巨量热能？最终，你将具备评估、设计和理解一台高性能GPU服务器所需的全方位系统级视野。</p><h2 id=41-初识nvidia-dgx>4.1 初识NVIDIA DGX</h2><p>在深入技术细节之前，我们有必要先理解NVIDIA DGX的定位和价值。DGX并非一个单纯的硬件品牌，而是NVIDIA为企业和研究机构提供的、一个旨在“开箱即用”的、全栈式的AI基础设施解决方案。</p><h3 id=411-dgx的诞生背景消除ai基础设施的复杂性>4.1.1 DGX的诞生背景：消除AI基础设施的复杂性</h3><p>在深度学习爆发的早期，企业和研究机构在构建自己的GPU计算平台时，面临着巨大的挑战：</p><ul><li>硬件选型的困惑：应该选择哪款GPU？搭配什么样的CPU？需要多大内存？主板和机箱是否兼容？网络和存储如何配置？这些问题充满了“坑”。</li><li>系统集成的噩梦：硬件组装起来只是第一步。接下来是安装操作系统、GPU驱动、CUDA工具包、cuDNN等一系列底层软件。版本之间的兼容性问题、驱动冲突、性能调优等，往往会耗费数周甚至数月的时间。</li><li>软件栈的缺失：即使硬件和驱动跑通了，上层的容器环境（如Docker）、资源调度器（如Slurm）、主流AI框架（TensorFlow, PyTorch）及其所有依赖项的安装和优化，又是一个巨大的工程。</li><li>缺乏统一支持：当系统出现问题时，责任难以界定。是硬件故障（GPU？CPU？主板？）、驱动问题，还是框架的bug？用户不得不在多个供应商之间来回“踢皮球”。</li></ul><p>NVIDIA敏锐地看到了这些痛点，并意识到，要推动AI的普及，就必须提供一个“交钥匙”的解决方案，让数据科学家和研究人员能够从繁杂的基础设施搭建中解脱出来，专注于算法和模型本身。DGX应运而生。</p><h3 id=412-dgx的价值主张ai超级计算机-in-a-box>4.1.2 DGX的价值主张：“AI超级计算机 in a box”</h3><p>DGX的价值可以概括为以下几点：</p><ol><li>极致优化的硬件平台：DGX服务器的硬件设计代表了NVIDIA在特定时期内，对其最新GPU进行系统集成的最佳实践。从主板拓扑、信号完整性、电源分配到散热风道，每一个细节都经过了精心的设计和严格的验证，旨在最大化多GPU系统的整体性能和稳定性。它是一个经过“预调优”的、均衡的系统，而非简单的硬件堆砌。</li><li>全栈集成的软件环境：购买DGX，你得到的不仅仅是硬件。每台DGX都预装了DGX OS，这是一个基于Ubuntu的、经过优化的操作系统，包含了所有必需的驱动和底层软件。更重要的是，NVIDIA提供了NGC（NVIDIA GPU Cloud）软件仓库的访问权限。NGC中包含了大量预先构建、经过测试和优化的Docker容器，涵盖了所有主流的AI框架、HPC应用、数据科学工具等。用户只需要一条<code>docker pull</code>命令，就能获得一个立即可用、性能最优的开发环境。</li><li>企业级的支持与服务：NVIDIA为DGX客户提供端到端的企业级支持。无论遇到硬件、软件还是性能问题，用户都只需要联系NVIDIA的专家团队，就能获得一站式的解决方案。这种“单一责任方”的服务模式，为企业在生产环境中大规模部署AI应用提供了信心保障。</li><li>可扩展的参考架构：DGX不仅仅是单台服务器。NVIDIA还提供了基于DGX和高性能网络（如InfiniBand）的DGX POD和DGX SuperPOD参考架构。这些架构为构建从几十卡到数万卡规模的AI超级计算机提供了经过验证的设计蓝图，涵盖了计算、网络、存储、软件管理等所有方面。</li></ol><h3 id=413-dgx家族的演进>4.1.3 DGX家族的演进</h3><p>DGX系列自2016年发布第一代DGX-1以来，紧随NVIDIA GPU架构的更新换代而演进：</p><p>DGX-1: 基于Pascal P100 GPU，首次引入了8-GPU通过NVLink互联的混合立方体网格（Hybrid Cube-Mesh）拓扑。</p><p>DGX-2: 基于Volta V100 GPU，是一个拥有16个V100 GPU的庞然大物。它首次引入了NVSwitch芯片，实现了16个GPU之间的全连接NVLink Fabric，总带宽高达2.4 TB/s，是当时性能最强的单体AI服务器。</p><p>DGX A100: 基于Ampere A100 GPU，回归到更紧凑的8-GPU设计。它采用了第三代NVLink和第二代NVSwitch，8个GPU之间的双向总带
宽高达4.8 TB/s。本章将主要剖析其设计。</p><p>DGX H100: 基于Hopper H100 GPU，是当前的旗舰产品。它同样采用8-GPU设计，但升级到了第四代NVLink和第三代NVSwitch，8个GPU之间的双向总带宽进一步提升至7.2 TB/s。同时，它还与Grace Hopper超级芯片的设计理念一脉相承，展现了异构融合的未来趋
势。</p><p>通过对DGX的初步认识，我们明白它不仅仅是一个硬件产品，更是一种设计哲学和生态系统。接下来，我们将深入这台“AI盒子”的内部，一探其究竟。</p><h2 id=42-nvidia-dgx-a100的总体设计>4.2 NVIDIA DGX A100的总体设计</h2><p>DGX A100服务器是一个标准的6U机架式服务器，其内部结构紧凑而精密，所有组件都以模块化的方式进行布局，以便于维护和升级。打开其顶盖，我们可以看到一个清晰的、为最大化性能和散热效率而设计的布局。</p><h3 id=421-物理布局与核心模块>4.2.1 物理布局与核心模块</h3><p>DGX A100的内部可以被划分为几个关键的功能区域：</p><ol><li>GPU基板（GPU Baseboard）：这是整个服务器的核心和灵魂。一块巨大的、多层PCB板横亘在机箱中部，上面承载着8个A100 SXM4模块和6个NVSwitch芯片。这8个GPU和6个NVSwitch共同构成了一个全连接的NVLink Fabric。</li><li>CPU模组与内存：在机箱的前部（靠近冷通道），是CPU计算模组。DGX A100配备了2个AMD EPYC 7002系列CPU（例如7742，64核）。每个CPU旁边都插满了DIMM内存插槽，总共支持高达2TB的DDR4内存。</li><li>网络与PCIe扩展区域：在机箱的后部（靠近热通道），是I/O扩展区域。这里包含了：<ol><li>8个Mellanox ConnectX-6 200Gb/s InfiniBand/Ethernet网卡，专用于GPU之间的高速分布式训练通信（GPUDirect RDMA）。</li><li>1个双端口的Mellanox ConnectX-6 200Gb/s网卡，用于存储网络。</li><li>1个管理网口和多个USB接口。</li><li>2个NVMe U.2 SSD，用作高速操作系统盘和缓存。</li></ol></li><li>电源与散热系统：</li></ol><p>电源：机箱后部下方是多个（通常是6个）热插拔的冗余电源模块（PSU），为这台功耗高达6.5KW的猛兽提供稳定、可靠的电力。</p><p>散热：整个机箱内部被设计成一个严格的前进后出的风道。前部的CPU和内存区域由一组风扇墙负责冷却，而中部的GPU和NVSwitch这个最大的“热源”，则由另一组更强大的风扇墙负责。GPU模块本身也覆盖着巨大的被动散热片。</p><h3 id=422-逻辑架构以数据为中心的设计>4.2.2 逻辑架构：以数据为中心的设计</h3><p>从逻辑上看，DGX A100的架构体现了典型的“胖GPU节点”设计思想。其核心拓扑可以概括为：</p><ul><li>GPU集群为中心：8个A100 GPU通过NVSwitch构成了一个紧密耦合的计算核心。它们之间的通信带宽（4.8 TB/s）远高于它们与系统
其他部分的通信带宽。</li><li>双CPU作为“服务员”：2个强大的AMD EPYC CPU负责操作系统的运行、数据预处理、任务调度、网络I/O处理等所有“杂活”。每个CPU通过PCIe总线与4个GPU和4个高速网卡相连，形成一个相对独立的NUMA域。</li><li>PCIe交换机作为“立交桥”：为了连接CPU和数量众多的GPU、网卡等PCIe设备，DGX A100内部使用了多个PCIe Switch芯片。这些交换机起到了扩展PCIe通道数量和优化数据路径的关键作用。</li><li>分离的网络平面：DGX A100明确区分了不同用途的网络：<ul><li>计算网络（Compute Fabric）：8个专用的200G网卡，主要用于多节点分布式训练时的梯度同步（All-Reduce）。</li><li>存储网络（Storage Fabric）：1个专用的200G网卡，用于连接外部的高性能并行文件系统或对象存储。</li><li>带内/带外管理网络（In-Band/Out-of-Band Management）：用于服务器的管理和监控。</li></ul></li></ul><p>这种清晰的、以GPU为中心的设计，确保了数据流在系统内部能够以最高效的方式进行。无论是GPU之间、GPU与CPU之间，还是GPU与外部网络/存储之间，都拥有专门的、高带宽的通道，避免了资源争抢和瓶颈。</p><h2 id=43-nvidia-dgx-a100-cpu与内存子系统的设计>4.3 NVIDIA DGX A100 CPU与内存子系统的设计</h2><p>在GPU服务器中，CPU的角色虽然不是主计算力，但它作为系统的“大脑”和“大管家”，其性能和配置直接决定了整个系统的木桶短板。一个羸弱的CPU子系统，会导致GPU长时间处于“饥饿”等待状态，造成巨大的算力浪费。</p><h3 id=431-为何需要强大的cpu>4.3.1 为何需要强大的CPU？</h3><p>在AI训练流程中，CPU主要承担以下关键任务：</p><h4 id=数据加载与预处理data-loading--preprocessing>数据加载与预处理（Data Loading & Preprocessing）</h4><p>AI训练的第一步是从存储系统中读取海量原始数据（如图片、文本）。这个过程本身就是I/O密集型的。</p><p>原始数据往往不能直接送入GPU。它们需要经过一系列预处理操作，如图像解码、裁剪、缩放、旋转、归一化等。这些操作通常是在CPU上由多个并行的“data loader”工作进程来完成的。</p><p>如果CPU的处理速度跟不上GPU消耗数据的速度，GPU就会空闲，训练效率大打折扣。这就是所谓的“Input Pipeline Bottleneck”。拥有更多核心、更高主频的CPU，能够支持更多的并行data loader进程，从而提供更高的数据吞吐量。</p><h4 id=操作系统与任务调度>操作系统与任务调度</h4><p>CPU负责运行Linux操作系统、DGX OS软件栈、资源调度器（如Slurm）以及用户的Python主程序。</p><h4 id=网络io处理>网络I/O处理</h4><p>在分布式训练中，CPU需要处理网络协议栈，将数据打包/解包，并通过PCIe总线在网卡和内存之间传输数据。尽管有GPUDirect RDMA技术可以绕过CPU，但在许多场景下CPU的参与仍然不可避免。</p><h4 id=部分ai算子的执行>部分AI算子的执行</h4><p>虽然绝大部分计算都在GPU上，但总有一些不适合在GPU上运行的、或者框架尚未实现GPU版本的算子，会在CPU上执行。</p><h3 id=432-dgx-a100的cpu选型amd-epyc>4.3.2 DGX A100的CPU选型：AMD EPYC</h3><p>DGX A100选择搭载2颗AMD EPYC 7742 CPU。这一选择是基于多方面考虑的：</p><p>极高的核心数量：每颗EPYC 7742拥有64个物理核心，两颗组成一个拥有128核/256线程的强大计算平台。如此多的核心，能够轻松应对大规模并行的数据预处理任务。</p><p>丰富的PCIe 4.0通道：这是最关键的因素。每颗AMD EPYC 7002系列CPU原生支持128条PCIe 4.0通道。两颗CPU就提供了256条通道。这海量的I/O带宽，是连接8个GPU、8个高速网卡以及其他众多设备的根本保障。相比之下，同时代的Intel Xeon CPU在PCIe通道数量上处于劣势。</p><p>高内存带宽与容量：每颗EPYC CPU支持8个内存通道，整个系统支持16通道的DDR4-3200内存，最大容量可达2TB。这为海量数据的暂存和预处理提供了充足的空间。</p><h3 id=433-numa架构与cpu-gpu亲和性>4.3.3 NUMA架构与CPU-GPU亲和性</h3><p>在像DGX A100这样的双CPU系统中，内存访问架构是NUMA（Non-Uniform Memory Access）的。这意味着，每个CPU访问其“本地”连接的内存（Local Memory）速度最快，而访问另一个CPU连接的内存（Remote Memory）则需要通过CPU之间的互联总线（如AMD的Infinity Fabric），延迟会更高。</p><p>这种NUMA架构对GPU服务器的性能至关重要，必须进行亲和性（Affinity）绑定：</p><ul><li>拓扑设计：DGX A100的设计是均衡的。CPU 0通过PCIe总线连接到GPU 0-3和4张网卡。CPU 1连接到GPU 4-7和另外4张网卡。</li><li>软件绑定：在软件层面，必须确保：<ul><li>处理GPU 0-3相关任务的进程（如data loader），应该被绑定（pinning）到CPU 0的物理核心上运行。</li><li>这些进程分配的内存，也应该优先从CPU 0的本地内存中获取。</li><li>当GPU 0-3需要通过网络发送数据时，应该优先使用与CPU 0相连的那4张网卡。</li></ul></li><li>目的：通过这种严格的亲和性绑定，可以确保绝大部分数据传输都发生在NUMA节点内部，避免了代价高昂的跨NUMA节点访问，从而最大化I/O性能。DGX OS和NVIDIA的软件栈已经为用户处理好了这些复杂的绑定工作。</li></ul><h3 id=434-系统内存越大越好吗>4.3.4 系统内存：越大越好吗？</h3><p>DGX A100支持高达2TB的系统内存。如此大的内存主要用于：</p><ul><li>数据集缓存：对于那些可以完全载入内存的数据集，将其一次性读入系统内存，可以极大地加速后续的训练过程，避免了每次epoch都从慢速的外部存储中读取。</li><li>数据预处理缓冲区：data loader需要大量的内存作为缓冲区，来存放预处理好的数据批次，以备GPU随时取用。</li><li>支持新兴技术（如ZeRO-Offload）：微软DeepSpeed的ZeRO-Offload技术，可以将一部分GPU显存中放不下的模型参数或优化器状态，“卸载”到CPU的系统内存中。这需要海量的CPU内存作为“显存扩展”。</li></ul><p>总之，CPU与内存子系统虽非主角，但其强大的多核性能、海量的PCIe通道和巨大的内存容量，是支撑GPU集群高效运转的坚实地基。</p><h2 id=44-nvidia-dgx-a100-pci-e子系统的设计>4.4 NVIDIA DGX A100 PCI-E子系统的设计</h2><p>PCI Express（PCIe）是现代服务器中连接CPU与各种高速外设（如GPU、网卡、NVMe SSD）的标准总线。在GPU服务器中，PCIe子系统的设计优劣，直接决定了数据能否顺畅地在CPU、GPU、网络和存储之间流动。</p><h3 id=441-pcie-40带宽翻倍>4.4.1 PCIe 4.0：带宽翻倍</h3><p>DGX A100是首批全面拥抱PCIe 4.0标准的服务器之一。相比PCIe 3.0，PCIe 4.0将每条通道（lane）的带宽翻了一倍，从约1 GB/s提升到了约2 GB/s。</p><p>一个典型的PCIe x16插槽，在PCIe 4.0下的双向带宽可达64 GB/s，而在PCIe 3.0下只有32 GB/s。</p><p>这种带宽的翻倍，对于需要与CPU进行大量数据交换的GPU和高速网卡来说至关重要，能够有效缓解CPU-GPU之间的通信瓶颈。</p><h3 id=442-pcie通道的僧多粥少问题>4.4.2 PCIe通道的“僧多粥少”问题</h3><p>尽管DGX A100配备了拥有256条PCIe 4.0通道的强大CPU，但要连接的设备实在太多了：</p><ul><li>8个A100 GPU，每个都需要一个x16的连接。<code>8 * 16 = 128</code>条。</li><li>8个ConnectX-6计算网卡，每个也需要一个x16的连接。<code>8 * 16 = 128</code>条。</li><li>1个ConnectX-6存储网卡，需要一个x16的连接。<code>1 * 16 = 16</code>条。</li><li>2个NVMe SSD，每个需要x4的连接。<code>2 * 4 = 8</code>条。</li></ul><p>还有南桥芯片、管理控制器等其他设备。</p><p>总需求远远超过了CPU能提供的256条通道。如何解决这个问题？答案是引入PCIe交换机（PCIe Switch）。</p><h3 id=443-pcie交换机的关键作用>4.4.3 PCIe交换机的关键作用</h3><p>PCIe交换机就像网络交换机一样，它有一个上行端口（Upstream Port）连接到CPU（或另一个交换机），以及多个下行端口（Downstream Ports）连接到终端设备。它的作用是：</p><ol><li>通道扩展（Fan-out）：一个交换机可以用一个x16的上行通道，扩展出多个x16、x8或x4的下行通道。</li><li>点对点通信（Peer-to-Peer, P2P）：这是PCIe交换机在GPU服务器中最重要的功能。连接到同一个PCIe交换机下的两个设备（例如两个GPU，或者一个GPU和一个网卡），它们之间可以进行直接的点对点数据传输，而无需经过CPU或系统内存。这极大地降低了通信延迟，并解放了CPU和内存带宽。</li></ol><h3 id=444-dgx-a100的pcie拓扑>4.4.4 DGX A100的PCIe拓扑</h3><p>DGX A100的PCIe拓扑设计是一个精巧的杰作，旨在最大化P2P通信的效率。其大致结构如下：</p><p>双NUMA节点设计：整个系统被对称地划分为两个由PCIe交换机组成的“域”，分别连接到CPU 0和CPU 1。</p><p>三层交换结构：</p><ol><li>第一层（CPU直连）：每个CPU使用其128条PCIe通道，分出多个x16链路，连接到多个第二层的PCIe交换机。</li><li>第二层（主交换机）：DGX A100主板上集成了多个大型的PCIe 4.0交换机芯片（例如Broadcom PEX系列）。每个CPU连接的4个
GPU和4个计算网卡，被巧妙地分组，连接到这些主交换机上。</li><li>第三层（设备连接）：主交换机再向下连接到各个PCIe设备。</li></ol><p>精心设计的P2P路径：</p><ul><li>GPU-GPU P2P：任意两个GPU之间的PCIe P2P通信路径都经过了优化。</li><li>GPUDirect RDMA：这是最关键的P2P应用。DGX A100的拓扑确保了每一个GPU都与一个专用的计算网卡位于同一个PCIe交换机的“管辖范围”下。这意味着，当这个GPU需要通过网络发送数据时（例如分布式训练中的梯度同步），它可以利用GPUDirect RDMA技术，让数据直接从GPU显存流向网卡，然后发送出去，全程无需CPU的干预，也无需在系统内存中进行任何数据拷贝。这对于降低分布式训练的通信延迟至关重要。</li><li>GPUDirect Storage：类似的，GPU也可以通过PCIe P2P，直接从NVMe SSD中读取数据，绕过CPU，实现更高的数据加载带宽。</li></ul><p>通过这种复杂而高效的PCIe交换网络，DGX A100为系统内部的所有高速设备构建了一个四通八达的“数据立交桥系统”，确保了任何两个需要通信的节点之间，都有最短、最快的路径。</p><h2 id=45-nvidia-dgx-a100-nvlink子系统的设计>4.5 NVIDIA DGX A100 NVLink子系统的设计</h2><p>如果说PCIe是连接GPU与外部世界的“国道”，那么NVLink就是GPU之间专用的、封闭的、拥有无限速权的“F1赛道”。在大模型训练中，张量并行和流水线并行都需要在GPU之间进行极其频繁和海量的数据交换，NVLink子系统的性能直接决定了模型并行的效率。</p><h3 id=451-第三代nvlink与sxm4形态>4.5.1 第三代NVLink与SXM4形态</h3><p>第三代NVLink：DGX A100搭载的A100 GPU支持第三代NVLink。每个A100 GPU拥有12个NVLink 3.0链路，每个链路的双向带宽为
50 GB/s。因此，每个A100 GPU的总NVLink带宽高达 600 GB/s。</p><p>SXM4模块：为了承载如此高密度的I/O接口，数据中心版的A100 GPU采用了SXM4这种专门的板上芯片（Mezzanine）形态，而不是标准的PCIe卡形态。SXM4模块通过一个高密度的连接器安装在GPU基板上，这个连接器上集成了供电、管理以及所有的NVLink和PCIe信号。</p><h3 id=452-nvswitch构建全连接fabric的核心>4.5.2 NVSwitch：构建全连接Fabric的核心</h3><p>虽然每个GPU有600 GB/s的带宽，但如何将8个GPU高效地连接起来？如果采用两两直连的方式，一个拥有12个链路的GPU最多只能直连少数几个邻居，无法实现全局的高效通信。</p><p>NVIDIA为此设计了NVSwitch芯片。NVSwitch是一个专门用于路由NVLink流量的高速交换芯片。</p><p>第二代NVSwitch：DGX A100中使用了6个第二代NVSwitch芯片。每个NVSwitch芯片都拥有多个NVLink端口。</p><p>全连接（All-to-All）拓扑：DGX A100的GPU基板设计是其技术皇冠上的明珠。它通过这6个NVSwitch，巧妙地实现了8个A100 GPU之间的全连接。这意味着：</p><p>任意两个GPU之间，都存在一条由12条NVLink链路组成的、总带宽高达600 GB/s的直接通信路径。</p><p>这8个GPU组成了一个带宽无阻塞的NVLink Fabric，其聚合双向带宽高达 4.8 TB/s (<code>8 * 600 GB/s / 2</code>，除以2是因为每条
连接被计算了两次)。</p><p>工作原理：每个A100 GPU将其12个NVLink链路分成6组，每组2个链路，分别连接到6个NVSwitch芯片中的一个。当GPU 0想要与GPU 1通信时，它会将数据包通过连接到某个NVSwitch的链路上发送出去，NVSwitch会像网络交换机一样，根据数据包的目的地址，将其转发到连接着GPU 1的链路上。由于每个GPU都与所有6个NVSwitch相连，因此保证了任意两个GPU之间都通路。</p><h3 id=453-nvlink子系统的价值>4.5.3 NVLink子系统的价值</h3><p>这个全连接、高带宽的NVLink Fabric是DGX A100能够高效执行大规模模型并行的根本原因：</p><ul><li>高效的张量并行：在进行张量并行时，需要在GPU之间进行频繁的<code>All-Reduce</code>或<code>All-Gather</code>操作。NVLink Fabric为这些集体通信操作提供了硬件层面的超低延迟和超高带宽支持，使得跨越多达8个GPU进行张量并行成为可能。</li><li>流畅的流水线并行：在流水线并行中，需要将每一层的激活值从一个GPU传递到下一个GPU。NVLink同样为这种点对点传输提供了最优路径。</li><li>统一的编程模型：对于开发者来说，这8个GPU在逻辑上就像一个拥有海量显存和计算能力的“巨型单体GPU”。他们在使用NCCL等库进行多GPU通信时，无需关心底层的物理拓扑，因为任意两个GPU之间的通信性能都是对称和一致的。</li></ul><p>对比DGX H100：DGX H100升级到了第四代NVLink（每个H100有18个链路，总带宽900 GB/s）和第三代NVSwitch。其8-GPU的全连接聚合带宽高达7.2 TB/s，进一步提升了模型并行的效率。</p><h2 id=46-其他辅助子系统的设计>4.6 其他辅助子系统的设计</h2><p>一台顶级GPU服务器的卓越，体现在其对每一个细节的关注。除了计算和通信这三大核心子系统，电源、散热、存储和管理等辅助子系统同样至关重要。</p><h3 id=461-存储子系统>4.6.1 存储子系统</h3><p>操作系统盘：DGX A100配备了2个高速的NVMe U.2 SSD，配置为RAID 1（镜像），用于安装DGX OS和存储日志。NVMe协议和PCIe直连带来了极低的延迟和高吞吐量，确保系统启动和运行的流畅。</p><p>内部数据盘：DGX A100还提供了4个NVMe U.2插槽，用于构建一个高达30TB的内部高速数据缓存池。这对于存放需要频繁访问的中小型数据集，或者作为外部存储的“一级缓存”非常有用。</p><p>外部存储连接：对于大模型训练，数据集通常存放在外部的、由数百块硬盘组成的并行文件系统（如Lustre, GPFS）或对象存储（如Ceph）中。DGX A100通过其专用的200G存储网卡连接到这些外部存储，以获得持续、高带宽的数据供给。</p><h3 id=462-电源子系统>4.6.2 电源子系统</h3><p>高功耗：一台满负荷运行的DGX A100服务器，其峰值功耗可达6.5千瓦（kW）。这相当于同时打开三台家用空调。</p><p>冗余设计：为了保证企业级的可靠性，DGX A100采用了N+N冗余的电源设计。它配备了6个3kW的电源模块（PSU），分为两组，每组3个。在正常工作时，只需要每组中的一部分PSU供电即可。如果任何一个PSU发生故障，其负载会立刻被同组的其他PSU接管，而不会导致服务器停机。</p><p>高转换效率：这些PSU通常都是80 Plus钛金或白金认证的高效电源，以减少从交流电到直流电转换过程中的能量损失，降低数据中心PUE。</p><h3 id=463-散热子系统>4.6.3 散热子系统</h3><p>巨大的热量：6.5kW的电能输入，绝大部分最终都会转化为热量。如何有效地将这些热量从狭小的6U机箱空间内带走，是一个巨大的工程
挑战。</p><p>风冷设计：DGX A100采用了先进的风冷（Air Cooling）设计。</p><p>严格的风道隔离：机箱内部通过隔板被划分为多个独立风道，确保冷空气能够精确地流经每一个发热部件（CPU、GPU、NVSwitch、内存、电源），而不会形成热空气回流。</p><p>冗余风扇墙：系统内部部署了多个由高速、暴力风扇组成的热插拔风扇墙。这些风扇同样是冗余配置的，即使有风扇损坏，系统也能维持足够的风量来保证安全运行。</p><p>优化的散热片：所有的核心芯片，特别是SXM4形态的GPU模块，都覆盖着经过流体力学仿真的、鳍片密度和形状都经过优化的巨型被动散热片，以最大化与空气的接触面积。</p><p>对数据中心环境的要求：这种强大的风冷系统，也对数据中心的制冷能力和机柜布局提出了极高要求。必须保证机柜前方有持续、低温的冷空气供给（冷通道），后方有足够的空间排出热空气（热通道）。</p><h3 id=464-管理子系统>4.6.4 管理子系统</h3><p>BMC（Baseboard Management Controller）：DGX A100内置了一个独立的BMC芯片。这是一个微型计算机，它拥有自己的处理器、内存和网络接口，独立于主CPU运行。</p><p>带外管理：通过BMC，管理员可以对服务器进行完全的带外（Out-of-Band）管理。即使服务器处于关机、死机或操作系统崩溃的状态，只要插着电源线和管理网线，管理员就可以远程：</p><ul><li>开关机、重启服务器。</li><li>监控所有硬件传感器的状态（温度、电压、风扇转速）。</li><li>访问虚拟的键盘、显示器和鼠标（KVM over IP）。</li><li>挂载ISO镜像进行远程系统安装。</li></ul><p>这对于大规模数据中心的自动化运维和故障快速响应至关重要。</p><h2 id=47-本章小结>4.7 本章小结</h2><p>在本章中，我们以NVIDIA DGX A100为蓝本，完成了一次对顶级GPU服务器设计与实现的深度探索。我们不再将GPU服务器视为一个简单的硬件堆砌，而是理解了它是一个为了单一目标——最大化多GPU协同计算效率——而精心设计的、各子系统间精密平衡的复杂系统。</p><p>我们的剖析之旅遵循了系统工程的逻辑：</p><p>我们首先认识到DGX的价值，它通过提供一个软硬件全栈优化、开箱即用、并有企业级支持的平台，极大地降低了企业部署和应用AI的门槛，是业界的“黄金参考设计”。</p><p>在总体设计层面，我们看到了其以8-GPU NVLink Fabric为绝对核心，辅以强大的双CPU、海量内存和分离式高速网络的清晰架构，所有设计都为数据的高效流动服务。</p><p>在CPU与内存子系统中，我们理解了强大的多核CPU和海量RAM对于数据预处理、I/O调度以及应对NUMA架构挑战的重要性。AMD EPYC凭借其丰富的PCIe通道和核心数量，成为了理想的选择。</p><p>在PCIe子系统中，我们揭示了PCIe交换机的魔力。它不仅扩展了通道，更重要的是通过构建优化的P2P路径，实现了GPUDirect RDMA等关键技术，打通了GPU与网络、存储之间的高速直连通道。</p><p>在NVLink子系统中，我们见证了DGX设计的精髓。通过6个NVSwitch芯片，实现了8个A100 GPU之间带宽高达4.8 TB/s的全连接，为
高效的模型并行（特别是张量并行）提供了无与伦比的硬件基础。</p><p>最后，在电源、散热、存储和管理等辅助子系统中，我们看到了企业级服务器对冗余、可靠性和可维护性的极致追求。从N+N冗余电源到精密的散热风道，再到强大的带外管理BMC，每一个细节都为保障这台“算力怪兽”的稳定运行而设计。</p><p>通过对DGX A100的深入学习，我们掌握了一套评估和理解任何高端GPU服务器的设计框架。无论是面对下一代的DGX H100，还是来自其他厂商的竞品，我们都能够透过其市场宣传，直击其架构核心，分析其拓扑设计的优劣、瓶颈所在以及真实的应用场景。这正是作为一个高级系统架构师或AI基础设施工程师所必备的核心能力。在接下来的章节中，我们将把视野从单台服务器扩展到整个集群，探讨如何将成百上千台这样的服务器连接起来，构建真正的AI超级计算机。</p></div><div class=post_footer><div class=meta><div class=info><span class="field tags"><i class=ri-stack-line></i>
<a href=https://zhurongshuo.com/tags/%E4%B9%A6%E7%A8%BF/>书稿</a></span></div></div></div></div><div class=doc_comments></div></div></div></div><a id=back_to_top href=# class=back_to_top><i class=ri-arrow-up-s-line></i></a><footer class=footer><div class=powered_by><a href=https://varkai.com>Designed by VarKai, </a><a href=http://www.gohugo.io/>Proudly published with Hugo,</a></div><div class=footer_slogan><span>法不净空，觉无性也。</span></div><div class=powered_by style=margin-top:10px;font-size:14px><a href=https://zhurongshuo.com/>Copyright © 2010-2025 祝融说 zhurongshuo.com All Rights Reserved.</a></div></footer><script defer src=https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><link href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.css rel=stylesheet integrity="sha256-7qiTu3a8qjjWtcX9w+f2ulVUZSUdCZFEK62eRlmLmCE=" crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin=anonymous></script><script defer src=https://zhurongshuo.com/js/zozo.js></script><script type=text/javascript async src="https://cdn.jsdelivr.net/npm/mathjax@2.7.8/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["[[","]]"]],processEscapes:!0,processEnvironments:!0,skipTags:["script","noscript","style","textarea","pre"],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}}),MathJax.Hub.Queue(function(){var e,t=MathJax.Hub.getAllJax();for(e=0;e<t.length;e+=1)t[e].SourceElement().parentNode.className+=" has-jax"})</script><style>code.has-jax{font:inherit;font-size:100%;background:inherit;border:inherit;color:#515151}</style><script>window.GA_MEASUREMENT_ID="G-KKJ5ZEG1NB",window.GA_CONFIG={enableReadingTime:!0,enableScrollDepth:!0,enableOutboundLinks:!0,enableDownloads:!0,lazyLoadTimeout:3e3}</script><script defer src=https://zhurongshuo.com/js/ga-optimizer.js></script></body></html>