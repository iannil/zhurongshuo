<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><meta name=google-site-verification content="8_xpI-TS3tNV8UPug-Q6Ef3BhKTcy0WOG7dEdAcm2zk"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="祝融"><link rel=dns-prefetch href=//cdn.jsdelivr.net><link rel=preconnect href=https://cdn.jsdelivr.net crossorigin><link rel=dns-prefetch href=//www.googletagmanager.com><link rel=preconnect href=https://www.googletagmanager.com crossorigin><link rel=canonical href=https://zhurongshuo.com/practices/season-4/intelligent-computing-center-construction-guide/part-01/chapter-03/><title>祝融说。 第3章 GPU硬件架构剖析</title><meta property="og:title" content="第3章 GPU硬件架构剖析"><meta property="og:url" content="https://zhurongshuo.com/practices/season-4/intelligent-computing-center-construction-guide/part-01/chapter-03/"><meta property="og:site_name" content="祝融说。"><meta property="og:locale" content="zh_CN"><meta property="og:type" content="article"><meta property="article:published_time" content="2025-12-07T00:00:00+08:00"><meta property="article:modified_time" content="2025-12-07T00:00:00+08:00"><meta property="article:tag" content="书稿"><meta name=description content="在前两章中，我们已经从AI算法的需求和软件堆栈的实现两个维度，理解了为何GPU成为了大模型时代的算力基石。我们知道了GPU擅长大规模并行计算，也了解了CUDA、PyTorch等软件是如何将这种能力释放出来的。然而，对于一个致力于构建顶级算力中心的架构师或工程师而言，仅仅停留在软件抽象层面是远远不够的。如同要造一辆高性能赛车，你必须深入了解其发动机的每一个缸体、每一个活塞、每一条管路。
"><meta property="og:description" content="在前两章中，我们已经从AI算法的需求和软件堆栈的实现两个维度，理解了为何GPU成为了大模型时代的算力基石。我们知道了GPU擅长大规模并行计算，也了解了CUDA、PyTorch等软件是如何将这种能力释放出来的。然而，对于一个致力于构建顶级算力中心的架构师或工程师而言，仅仅停留在软件抽象层面是远远不够的。如同要造一辆高性能赛车，你必须深入了解其发动机的每一个缸体、每一个活塞、每一条管路。
"><meta property="og:image" content="https://zhurongshuo.com/images/favicon.ico"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="第3章 GPU硬件架构剖析"><meta name=twitter:description content="在前两章中，我们已经从AI算法的需求和软件堆栈的实现两个维度，理解了为何GPU成为了大模型时代的算力基石。我们知道了GPU擅长大规模并行计算，也了解了CUDA、PyTorch等软件是如何将这种能力释放出来的。然而，对于一个致力于构建顶级算力中心的架构师或工程师而言，仅仅停留在软件抽象层面是远远不够的。如同要造一辆高性能赛车，你必须深入了解其发动机的每一个缸体、每一个活塞、每一条管路。
"><meta name=twitter:image content="https://zhurongshuo.com/images/favicon.ico"><meta name=keywords content="智算中心建设指南：大模型算力的基础架构,第3章 GPU硬件架构剖析"><link rel="shortcut icon" href=https://zhurongshuo.com/images/favicon.ico><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/normalize.css><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/animate-custom.css><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/zozo.css><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/remixicon-custom.css><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/highlight.css><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","headline":"第3章 GPU硬件架构剖析","description":"在前两章中，我们已经从AI算法的需求和软件堆栈的实现两个维度，理解了为何GPU成为了大模型时代的算力基石。我们知道了GPU擅长大规模并行计算，也了解了CUDA、PyTorch等软件是如何将这种能力释放出来的。然而，对于一个致力于构建顶级算力中心的架构师或工程师而言，仅仅停留在软件抽象层面是远远不够的。如同要造一辆高性能赛车，你必须深入了解其发动机的每一个缸体、每一个活塞、每一条管路。\n","datePublished":"2025-12-07T00:00:00\u002b08:00","dateModified":"2025-12-07T00:00:00\u002b08:00","author":{"@type":"Person","name":"祝融"},"publisher":{"@type":"Organization","name":"祝融说。","logo":{"@type":"ImageObject","url":"https:\/\/zhurongshuo.com\/images\/favicon.ico"}},"image":"https:\/\/zhurongshuo.com\/images\/favicon.ico","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/zhurongshuo.com\/practices\/season-4\/intelligent-computing-center-construction-guide\/part-01\/chapter-03\/"},"keywords":"书稿"}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"首页","item":"https:\/\/zhurongshuo.com\/"},{"@type":"ListItem","position":2,"name":"practices","item":"https:\/\/zhurongshuo.com\/practices/"},{"@type":"ListItem","position":3,"name":"第3章 GPU硬件架构剖析","item":"https:\/\/zhurongshuo.com\/practices\/season-4\/intelligent-computing-center-construction-guide\/part-01\/chapter-03\/"}]}</script></head><body><div class="main animate__animated animate__fadeInDown"><div class="nav_container animated fadeInDown"><div class=site_nav id=site_nav><ul><li><a href=https://zhurongshuo.com/>首页</a></li><li><a href=https://zhurongshuo.com/start/>开始</a></li><li><a href=https://zhurongshuo.com/advanced/>进阶rc</a></li><li><a href=https://zhurongshuo.com/posts/>归档</a></li><li><a href=https://zhurongshuo.com/tags/>标签</a></li><li><a href=https://zhurongshuo.com/about/>关于</a></li></ul></div><div class=menu_icon><a id=menu_icon><i class=ri-menu-line></i></a></div></div><div class="header animated fadeInDown"><div class=site_title_container><div class=site_title><h1><a href=https://zhurongshuo.com/><span class=web-font>祝融说。</span></a></h1></div><div class=description><p class=sub_title>法不净空，觉无性也。</p><div class=my_socials><a href=https://zhurongshuo.com/books/ title=book-open><i class=ri-book-open-line></i></a>
<a href=https://zhurongshuo.com/practices/ title=trophy><i class=ri-trophy-line></i></a>
<a href=https://zhurongshuo.com/gallery/ title=gallery><i class=ri-gallery-line></i></a>
<a href=https://zhurongshuo.com/about/ title=game><i class=ri-game-line></i></a>
<a href type=application/rss+xml title=rss target=_blank><i class=ri-rss-fill></i></a></div></div></div></div><div class=content><div class=post_page><div class="post animate__animated animate__fadeInDown"><div class="post_title post_detail_title"><h2><a href=https://zhurongshuo.com/practices/season-4/intelligent-computing-center-construction-guide/part-01/chapter-03/>第3章 GPU硬件架构剖析</a></h2><span class=date>2025.12.07</span></div><div class="post_content markdown"><p>在前两章中，我们已经从AI算法的需求和软件堆栈的实现两个维度，理解了为何GPU成为了大模型时代的算力基石。我们知道了GPU擅长大规模并行计算，也了解了CUDA、PyTorch等软件是如何将这种能力释放出来的。然而，对于一个致力于构建顶级算力中心的架构师或工程师而言，仅仅停留在软件抽象层面是远远不够的。如同要造一辆高性能赛车，你必须深入了解其发动机的每一个缸体、每一个活塞、每一条管路。</p><p>本章，我们将化身为硬件工程师，手持“显微镜”，深入到GPU芯片的硅基层面，剖析其内部的微观架构。我们的目标是回答一系列根本性问题：GPU是如何组织起成千上万个计算核心的？它与CPU的核心设计理念有何本质不同？数据在GPU内部是如何流动、计算和存储的？最新一代的GPU架构（如NVIDIA的Hopper）又带来了哪些革命性的创新来专门应对大模型的挑战？</p><p>我们将首先从GPU的总体设计哲学入手，建立一个宏观的框架，理解其“吞吐量优先”的核心思想。接着，我们将以当前数据中心GPU的巅峰之作——NVIDIA H100（其核心为GH100芯片）——作为主要解剖对象，逐一解析其流式多处理器（SM）、第四代Tensor Core、Transformer引擎、DPX指令集以及高带宽内存（HBM）等关键部件。最后，我们会简要介绍基于同一Hopper架构的其他GPU型号，以形成一个完整的产品谱系认知。</p><p>通过本章的学习，你将不再把GPU看作一个神秘的“黑盒子”，而是能够以架构师的眼光，洞悉其设计的精妙与权衡，理解不同技术参数（如CUDA核心数、Tensor Core性能、内存带宽）背后的真实物理意义，并最终能够根据业务需求，做出最明智、最专业的硬件选型决策。</p><h2 id=31-gpu的总体设计>3.1 GPU的总体设计</h2><p>要理解GPU的微观架构，首先必须牢牢把握其宏观的设计哲学。GPU和CPU是计算机体系结构中两种截然不同演化路径的产物，它们的设计目标、资源分配和核心权衡存在根本性差异。</p><h3 id=311-cpu-vs-gpu延迟优化与吞吐量优化的对决>3.1.1 CPU vs. GPU：延迟优化与吞吐量优化的对决</h3><p>我们可以用一个生动的比喻来区分CPU和GPU：</p><p>CPU如同一位学识渊博、身怀绝技的瑞士军刀式专家。 他能够以极快的速度独立处理各种复杂、多变、且前后关联的任务（串行任务）。为了做到这一点，他需要一个豪华的办公室（大容量缓存）、复杂的决策流程（乱序执行、分支预测等控制逻辑），以及快速调取档案的能力（低延迟内存访问）。他的目标是最小化单个任务的完成时间（Latency）。</p><p>GPU如同一支纪律严明、分工明确的庞大施工队。 这支队伍由成千上万名工人组成，每个工人只掌握少数几项简单技能（如搬砖、砌墙），但他们可以同时开工，协同完成一项巨大的、可被分解为大量重复性子任务的工程（并行任务）。队伍的管理相对简单，不需要复杂的独立决策，更重要的是保证所有工人都有活干，材料供应能跟上。它的目标是最大化单位时间内的总工作量（Throughput）。</p><p>这种设计哲学的差异，直接体现在了芯片的晶体管资源分配上：</p><p>CPU芯片：大量的晶体管被用于构建复杂的控制单元（Control Logic）和大容量缓存（Cache）。真正的算术逻辑单元（ALU）只占芯片面积的一小部分。这种“重控制、轻计算”的资源配比，确保了CPU在处理复杂指令流和减少单个任务延迟方面的卓越性能。</p><p>GPU芯片：绝大部分晶体管被用来构建海量的算术逻辑单元（ALU）。控制逻辑和缓存则相对简单和精简。这种“轻控制、重计算”的资源配比，使得GPU能够将几乎所有的“火力”都集中在并行数据处理上，从而实现惊人的原始计算吞吐量。</p><h3 id=312-gpu的宏观架构从gpc到sm>3.1.2 GPU的宏观架构：从GPC到SM</h3><p>一个现代的高端数据中心GPU芯片（如NVIDIA的GH100）是一个高度模块化和层次化的复杂系统。我们可以从宏观到微观，逐层剥开它的结构：</p><ol><li>完整GPU芯片（Full GPU）：这是最顶层的物理实体，例如一块完整的GH100芯片。它包含了所有的计算、内存和互联资源。</li><li>图形处理集群（Graphics Processing Cluster, GPC）：整个GPU芯片被划分为若干个GPC。GPC可以被看作是GPU内部一个相对独立的“大计算分区”。每个GPC都拥有自己的光栅化引擎（Rasterizer）等图形相关部件（这些在通用计算中不常用），但更重要的是，它包含了多个流式多处理器（SM）。</li><li>纹理处理集群（Texture Processing Cluster, TPC）：在NVIDIA的架构中，GPC内部又进一步划分为TPC。每个TPC通常包含两个SM和一个多边形形态引擎（PolyMorph Engine）。</li><li>流式多处理器（Streaming Multiprocessor, SM）：SM是GPU架构中最核心、最基本的计算单元。 我们可以将SM理解为GPU内部的一颗“迷你CPU核心”，但它本身又是高度并行的。一个GPU的并行计算能力，很大程度上就取决于它拥有多少个SM，以及每个SM的内部设计。所有的CUDA线程块（Blocks）都是被调度到某个具体的SM上执行的。SM内部包含了执行指令所需的一切资源：CUDA核心、Tensor Core、寄存器文件、共享内存、L1缓存等。</li><li>内存子系统：与计算单元并行存在的，是庞大的内存子系统。它包括位于芯片外部、通过极高带宽接口连接的HBM（High Bandwidth Memory）显存，以及芯片内部的多级缓存体系（L1 Cache、Shared Memory、L2 Cache）。</li><li>互联子系统：负责GPU与外部世界以及GPU之间通信的接口。包括连接CPU和外部设备的PCIe（Peripheral Component Interconnect Express）总线，以及专用于GPU之间高速互联的NVLink。</li></ol><p>一个形象的比喻：整个GPU芯片就像一个大型工厂。工厂被划分为几个GPC车间。每个车间里有几个TPC生产线。每条生产线上最重要的部分，是SM这个核心工作站。CUDA程序下发的一个个线程块（Blocks），就像一个个施工任务单，被调度到不同的SM工作站上去完成。SM工作站内部有各种工具（CUDA核心、Tensor Core）和临时物料架（寄存器、共享内存）。所有工作站共享一个巨大的中央仓库（HBM显存），并通过高效的内部物流系统（缓存体系）和外部运输通道（PCIe、NVLink）来获取和输送物料。</p><h3 id=313-simt执行模型gpu并行性的实现机制>3.1.3 SIMT执行模型：GPU并行性的实现机制</h3><p>CPU执行的是MIMD（Multiple Instruction, Multiple Data）模型，每个核心可以独立执行不同的指令。而GPU为了简化控制逻辑、节省芯片面积，采用了一种更高效的并行执行模型——SIMT（Single Instruction, Multiple Threads）。</p><p>Warp/Wavefront：GPU调度和执行的基本单位并不是单个线程，而是由32个线程组成的一个Warp（NVIDIA术语）或64个线程组成的Wavefront（AMD术语）。SM在任何一个时钟周期，都会选择一个“活跃”的Warp，并向这个Warp中的所有32个线程分派同一条指令。</p><p>执行过程：这32个线程会同时在SM内部不同的计算单元上，执行这条相同的指令，但处理的是各自不同的数据。例如，执行一条<code>c[i] = a[i] + b[i]</code>的指令，Warp中的32个线程会分别计算<code>c[0]=a[0]+b[0]</code>, <code>c[1]=a[1]+b[1]</code>, ..., <code>c[31]=a[31]+b[31]</code>。</p><p>分支分化（Branch Divergence）：SIMT模型最大的挑战来自于条件分支。如果一个Warp中的线程遇到了<code>if-else</code>语句，并且根据各自的数据，一些线程需要进入<code>if</code>分支，另一些需要进入<code>else</code>分支，会发生什么？</p><p>此时，Warp会发生分支分化。硬件会串行地执行这两个分支。首先，它会执行<code>if</code>分支，此时只有需要进入该分支的线程是活跃的，其他线程则被“屏蔽”掉（不执行操作）。<code>if</code>分支执行完毕后，硬件再执行<code>else</code>分支，此时原来执行<code>if</code>的线程被屏蔽，而需要进入<code>else</code>的线程变为活跃。</p><p>这意味着，如果一个Warp内的线程执行路径不一致，其总的执行时间是所有分支路径执行时间的总和，而不是其中最长的一条。这会严重降低SIMT的执行效率。因此，在编写高性能CUDA代码时，一个重要的优化原则就是尽量避免Warp内的分支分化。</p><p>理解了SIMT模型，我们就明白了GPU是如何用一个简单的控制单元来管理成千上万个线程的：通过将它们编组成Warp，以32个为一组进行统一调度和指令分发，极大地摊薄了控制逻辑的成本。</p><h2 id=32-nvidia-gh100芯片架构剖析>3.2 Nvidia GH100芯片架构剖析</h2><p>NVIDIA H100是基于第九代数据中心GPU架构——Hopper——的旗舰产品，其核心是GH100芯片。H100是专为加速大规模AI和HPC应用而设计的怪兽级处理器，它在上一代Ampere架构（A100）的基础上进行了多项重大创新。我们将以满血版的GH100芯片为例，对其关键部件进行深入剖析。</p><p>GH100芯片关键规格（满血版）：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>制程工艺：台积电4N定制工艺（TSMC 4N）
</span></span><span class=line><span class=cl>晶体管数量：800亿
</span></span><span class=line><span class=cl>GPC数量：8个
</span></span><span class=line><span class=cl>TPC数量：72个（每个GPC 9个）
</span></span><span class=line><span class=cl>SM数量：144个（每个TPC 2个）
</span></span><span class=line><span class=cl>HBM接口：6个HBM3或5个HBM2e控制器，5120位显存位宽
</span></span><span class=line><span class=cl>L2缓存：60MB
</span></span><span class=line><span class=cl>NVLink：第四代NVLink，总带宽900 GB/s
</span></span></code></pre></div><p>注意：实际销售的H100 SXM5版本启用了132个SM，而PCIe版本则启用了114个SM。这是为了保证良率而进行的常规操作。</p><h3 id=321-hopper-sm架构新一代计算心脏>3.2.1 Hopper SM架构：新一代计算心脏</h3><p>每个GH100的SM都是一个强大的并行处理引擎，相比A100的GA100 SM，它在保持核心结构的同时，进行了显著增强。一个Hopper SM内部主要包含：</p><ul><li>128个FP32 CUDA核心：这是执行标准单精度浮点运算的基础单元。与Ampere SM一样，Hopper SM也支持FP32和INT32的并发执行。</li><li>第四代Tensor Core：这是Hopper架构中最重要的升级之一。Tensor Core是专门用于加速矩阵乘加运算的硬件单元。第四代Tensor Core引入了FP8（8位浮点）数据格式的支持，并提供了惊人的计算吞吐量。</li><li>Transformer引擎（Transformer Engine）：这是一个软硬件协同的创新技术，与第四代Tensor Core紧密配合，能够智能地、动态地在FP16和FP8之间选择和切换精度，以在不牺牲准确率的前提下，极大地加速Transformer模型的训练和推理。</li><li>DPX指令集：一套新的指令，用于加速动态规划（Dynamic Programming）算法，在基因测序、蛋白质结构预测、路径优化等领域有重要应用。</li><li>256 KB的寄存器文件（Register File）：每个SM拥有大量的寄存器，供线程存储私有变量。</li><li>192 KB的可配置L1/共享内存：这块高速的片上内存可以被灵活配置。例如，可以配置为128KB共享内存 + 64KB L1缓存，或者其他组合，以适应不同应用的需求。共享内存的容量和带宽相比Ampere都有提升。</li><li>L0指令缓存：用于缓存指令，提升指令拾取效率。</li><li>Warp调度器：负责从分配到该SM的线程块中，选择活跃的Warp进行调度执行。</li></ul><h3 id=322-第四代tensor-core与fp8格式>3.2.2 第四代Tensor Core与FP8格式</h3><p>Tensor Core自Volta架构引入以来，一直是NVIDIA GPU在AI领域保持领先地位的“杀手锏”。Hopper的第四代Tensor Core将这一优势推向了新的高度。</p><p>矩阵乘加（MMA）：Tensor Core的核心功能是高效执行<code>D = A * B + C</code>的矩阵乘加操作。一个Tensor Core可以在一个时钟周期内完成一个小型矩阵的乘法和累加。</p><p>支持的数据精度：</p><ul><li>FP64: 用于传统HPC的双精度计算。</li><li>TF32 (Tensor Float 32): Ampere架构引入，拥有FP32的动态范围和FP16的精度，在无需修改代码的情况下就能对FP32运算进行加速。</li><li>FP16/BF16: 深度学习训练常用的半精度格式。</li><li>INT8/INT4: 用于深度学习推理，追求极致性能。</li><li>FP8 (E4M3 / E5M2): Hopper架构的独门绝技。 FP8是比FP16更低精度的浮点格式，它进一步减少了数据量和计算量。FP8有两种变体：E4M3（4位指数，3位尾数）有更高的精度，适合前向传播中的激活值；E5M2（5位指数，4位尾数）有更广的动态范围，适合反向传播中的梯度。</li></ul><p>性能飞跃：凭借对FP8的支持，H100的理论AI峰值性能相比A100实现了数量级的飞跃。在启用FP8和稀疏性（Sparsity）特性时，H100 SXM5的峰值算力可达4000 TFLOPS（4 PFLOPS），是A100（312 TFLOPS at FP16）的十多倍。</p><h3 id=323-transformer引擎为大模型量身定制的自动挡>3.2.3 Transformer引擎：为大模型量身定制的“自动挡”</h3><p>Transformer模型已经成为当今大语言模型和许多视觉模型的基础架构。其计算量巨大，对硬件提出了严峻挑战。Hopper的Transformer引擎正是为了应对这一挑战而生。</p><p>工作原理：Transformer引擎是一个软硬件结合的系统。它会逐层扫描Transformer网络中的张量，并利用NVIDIA的启发式算法和对张量统计信息的分析，自动地、动态地决定每一层、每一个张量是应该使用FP16精度还是FP8精度进行计算和存储。</p><p>智能精度转换：在需要保持高精度的部分（如梯度累加），引擎会使用FP16或FP32；而在可以容忍更低精度的部分（如权重和激活值），则会自动转换为FP8进行计算。这个转换过程对用户是透明的，开发者只需要启用Transformer引擎，而不需要手动管理精度。硬件会负责FP8和FP16之间的快速转换。</p><p>带来的好处：</p><ul><li>性能提升：通过最大化利用FP8带来的计算加速，显著提升Transformer模型的训练和推理速度。NVIDIA声称，在大型语言模型上，H100的训练速度可以达到A100的6倍。</li><li>内存节省：使用FP8存储权重和激活值，可以大幅降低显存占用，使得在单卡上能够容纳更大的模型或使用更大的批量大小。
易用性：开发者无需成为混合精度专家，就能享受到FP8带来的好处。</li></ul><h3 id=324-异步执行与新一代多实例gpumig>3.2.4 异步执行与新一代多实例GPU（MIG）</h3><p>线程块集群（Thread Block Clusters）：Hopper架构引入了“线程块集群”的概念。一个集群是一组在物理上保证被调度到同一个GPC内的线程块。集群内的线程块可以通过一种新的机制进行高效的数据交换和同步，这被称为分布式共享内存（Distributed Shared Memory, D-SMEM）。它允许一个线程块中的线程，直接通过原子操作（load, store, atomic）访问集群内其他SM上的共享内存，而无需经过全局内存，极大地扩展了SM之间协作的粒度和效率。</p><p>异步执行单元（Asynchronous Execution Units）：</p><p>Tensor Memory Accelerator (TMA)：H100引入了TMA，这是一个专门用于实现全局内存和共享内存之间异步数据拷贝的硬件单元。在之前的架构中，这种数据搬运需要专门的CUDA核心来执行指令。现在，主计算线程（CUDA核心或Tensor Core）可以向TMA下发一个拷贝任务，然后立即回头继续执行计算，而TMA则在后台独立地完成数据传输。这种计算与数据搬运的深度重叠，进一步提升了SM的利用率。</p><p>异步事务屏障（Asynchronous Transaction Barrier）：用于管理和同步那些依赖于异步操作（如TMA拷贝）完成的线程。</p><p>第二代多实例GPU（MIG）：</p><p>MIG是Ampere架构引入的功能，允许将一张物理GPU安全地、硬件隔离地分割成最多7个独立的GPU实例。每个实例都有自己专属的计算资源（SM）、内存和带宽。这对于云服务提供商来说非常有用，可以将一张昂贵的GPU租给多个小任务量的用户，提升资源利用率。</p><p>Hopper的第二代MIG在此基础上提供了更强的安全性和隔离性。它为每个MIG实例都提供了专用的硬件资源，并支持在云环境中进行安全的租户隔离。同时，通过线程块集群特性，MIG实例内部的协同计算能力也得到了增强。</p><h3 id=325-hbm3与l2缓存数据供给的生命线>3.2.5 HBM3与L2缓存：数据供给的生命线</h3><p>再强的计算引擎也需要充足的数据供给。Hopper在内存子系统上也进行了重大升级。</p><p>HBM3（High Bandwidth Memory 3）：H100是首款采用HBM3显存的GPU。相比A100使用的HBM2e，HBM3在每个引脚的数据速率、通道数等方面都有提升。</p><p>H100 SXM5版本：配备了80GB的HBM3显存，总带宽高达 3.35 TB/s。</p><p>H100 PCIe版本：配备了80GB的HBM2e显存，总带宽为 2 TB/s。
这恐怖的内存带宽是确保H100数千个核心不会“饿肚子”的关键保障，对于访存密集型的大模型应用至关重要。</p><p>L2缓存：GH100拥有高达60 MB的L2缓存（GA100为40MB）。L2缓存是所有SM共享的最后一级片上缓存。更大的L2缓存可以有效减少对外部HBM显存的访问次数，降低访存延迟，提升有效带宽，对于那些数据重用性高的应用有显著的性能提升。</p><h3 id=326-第四代nvlink与nvlink网络>3.2.6 第四代NVLink与NVLink网络</h3><p>第四代NVLink：单个H100 GPU拥有18个第四代NVLink通道，每个通道的单向带宽为25 GB/s，双向带宽50 GB/s。总的双向带宽高达900 GB/s，是A100（600 GB/s）的1.5倍。这为单台服务器内部多GPU之间进行高效的张量并行和流水线并行提供了坚实的硬件基
础。</p><p>NVLink网络（NVLink Network）：这是一个突破性的创新。在HGX H100 8-GPU服务器主板上，通过新的NVLink Switch芯片，实现了8个H100 GPU之间的全连接（All-to-All）NVLink。更重要的是，通过外部的NVLink Switch系统（NVSwitch），可以将最多32台HGX H100服务器（共256个GPU）连接成一个巨大的、拥有完整NVLink Fabric的单一计算域。</p><p>这意味着，在这256个GPU的集群中，任意两个GPU之间都可以通过NVLink进行直接通信，其通信性能远超传统的基于InfiniBand/RoCE的以太网。这使得跨越多台服务器进行大规模的张量并行和流水线并行成为可能，极大地简化了超大模型分布式训练的拓扑设计和通信开销。</p><h2 id=33-其他hopper架构的gpu>3.3 其他Hopper架构的GPU</h2><p>虽然H100是Hopper家族的旗舰，但NVIDIA也推出了基于同一核心架构的其他产品，以满足不同市场和应用场景的需求。</p><h3 id=331-nvidia-h100-cnx>3.3.1 NVIDIA H100 CNX</h3><p>H100 CNX是一款将H100 GPU与NVIDIA ConnectX-7智能网卡（SmartNIC）融合在一块PCIe卡上的产品。</p><p>目标场景：主流AI和数据中心服务器。在这些服务器中，PCIe带宽和CPU的参与往往成为网络通信的瓶颈。</p><p>核心优势：GPU和智能网卡之间的直接PCIe连接，以及ConnectX-7的硬件卸载引擎，使得GPU可以绕过CPU，直接进行网络通信（GPUDirect RDMA）。这显著降低了网络通信的延迟，释放了CPU资源，非常适合于大规模、I/O密集型的分布式AI训练和数据分析任务。</p><h3 id=332-nvidia-grace-hopper超级芯片-gh200>3.3.2 NVIDIA Grace Hopper超级芯片 (GH200)</h3><p>GH200是NVIDIA在异构计算领域的一个里程碑式的产品，它将一个基于ARM架构的Grace CPU和一个Hopper GPU通过一种超高速的片上互联技术——NVLink-C2C (Chip-to-Chip)——封装在同一个模块中。</p><p>NVLink-C2C：这是GH200的“黑科技”。它提供了高达 900 GB/s 的双向带宽，直接连接CPU的内存和GPU的内存。这比传统的PCIe
5.0（128 GB/s）快了7倍。</p><p>统一连贯内存（Unified Coherent Memory）：借助NVLink-C2C，GH200实现了CPU和GPU之间的单一内存地址空间。CPU可以直接访问GPU的全部HBM显存，而GPU也可以直接访问CPU的LPDDR5X内存。这意味着：</p><p>巨大的内存池：单个GH200可以为GPU提供高达624 GB的快速访问内存（Hopper的80GB HBM3 + Grace的544GB LPDDR5X）。这使得能够处理超大规模图神经网络、推荐系统、向量数据库等过去受限于GPU显存容量的应用。</p><p>编程简化：开发者不再需要显式地在CPU和GPU内存之间拷贝数据，系统硬件会自动处理数据的迁移和一致性，大大简化了异构计算的编程。</p><p>目标场景：巨型AI推理、大规模图计算、高性能计算（HPC）。对于那些模型极大，无法完全放入HBM显存，或者需要CPU和GPU进行频繁、海量数据交换的应用，GH200提供了前所未有的性能和易用性。多个GH200超级芯片也可以通过NVLink网络连接起来，构建更强大的计算集群。</p><h2 id=34-本章小结>3.4 本章小结</h2><p>在本章中，我们完成了对GPU硬件架构的一次深度“解剖”。通过这次旅程，我们从根本上理解了GPU之所以成为大模型时代核心引擎的物理基础。</p><p>我们首先确立了GPU的总体设计哲学：一个为实现极致吞吐量而生的大规模并行处理器。其“重计算、轻控制”的资源分配策略，以及通过SIMT模型高效管理数万线程的机制，使其与CPU的“低延迟”设计理念分道扬镳，走上了一条专为并行计算优化的道路。</p><p>随后，我们将焦点对准了当前算力之巅的NVIDIA H100 (GH100)。我们逐一拆解了其内部的关键部件，形成了一幅精细的架构图：
Hopper SM是其计算心脏，集成了FP32核心、寄存器、共享内存等基础部件。</p><p>第四代Tensor Core及其对FP8格式的支持，是H100算力实现数量级飞跃的“核武器”。</p><p>Transformer引擎则如同一位智能的“自动挡”驾驶员，通过软硬件协同，将FP8的威力以一种对用户透明、无感的方式释放出来，专为加速当今最主流的大模型架构而生。</p><p>线程块集群、TMA异步拷贝等新特性，进一步挖掘了SM内部和SM之间的并行潜力，将计算与数据搬运的重叠做到了极致。</p><p>在数据供给方面，高达3.35 TB/s的HBM3内存和60MB的L2缓存共同构成了强大的数据生命线，确保了计算核心永不“饥饿”。</p><p>在互联方面，900 GB/s的第四代NVLink和革命性的NVLink网络，为构建256卡规模的全互联GPU集群铺平了道路，使得超大规模模型并行成为现实。</p><p>最后，我们还概览了H100 CNX和Grace Hopper超级芯片 (GH200)等Hoポーター家族的衍生产品，看到了GPU正通过与智能网卡、高性能CPU的深度融合，不断拓展其应用边界，解决异构计算中的内存墙和通信墙问题。</p><p>通过本章的學習，我们对GPU的理解不再停留于“有很多核心”的模糊概念。现在，当我们看到一张H100的规格表时，我们能够理解“132个SM”、“4 PFLOPS FP8算力”、“3.35 TB/s带宽”这些数字背后所代表的深层架构设计和技术含义。这种深入骨髓的硬件认知，将是我
们未来进行算力中心规划、硬件选型、性能优化和故障排查时，做出正确决策的坚实基础。</p></div><div class=post_footer><div class=meta><div class=info><span class="field tags"><i class=ri-stack-line></i>
<a href=https://zhurongshuo.com/tags/%E4%B9%A6%E7%A8%BF/>书稿</a></span></div></div></div></div><div class=doc_comments></div></div></div></div><a id=back_to_top href=# class=back_to_top><i class=ri-arrow-up-s-line></i></a><footer class=footer><div class=powered_by><a href=https://varkai.com>Designed by VarKai, </a><a href=http://www.gohugo.io/>Proudly published with Hugo,</a></div><div class=footer_slogan><span>法不净空，觉无性也。</span></div><div class=powered_by style=margin-top:10px;font-size:14px><a href=https://zhurongshuo.com/>Copyright © 2010-2025 祝融说 zhurongshuo.com All Rights Reserved.</a></div></footer><script defer src=https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><link href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.css rel=stylesheet integrity="sha256-7qiTu3a8qjjWtcX9w+f2ulVUZSUdCZFEK62eRlmLmCE=" crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin=anonymous></script><script defer src=https://zhurongshuo.com/js/zozo.js></script><script type=text/javascript async src="https://cdn.jsdelivr.net/npm/mathjax@2.7.8/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["[[","]]"]],processEscapes:!0,processEnvironments:!0,skipTags:["script","noscript","style","textarea","pre"],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}}),MathJax.Hub.Queue(function(){var e,t=MathJax.Hub.getAllJax();for(e=0;e<t.length;e+=1)t[e].SourceElement().parentNode.className+=" has-jax"})</script><style>code.has-jax{font:inherit;font-size:100%;background:inherit;border:inherit;color:#515151}</style><script>window.GA_MEASUREMENT_ID="G-KKJ5ZEG1NB",window.GA_CONFIG={enableReadingTime:!0,enableScrollDepth:!0,enableOutboundLinks:!0,enableDownloads:!0,lazyLoadTimeout:3e3}</script><script defer src=https://zhurongshuo.com/js/ga-optimizer.js></script></body></html>