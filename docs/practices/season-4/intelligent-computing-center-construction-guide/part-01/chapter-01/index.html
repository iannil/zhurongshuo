<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><meta name=google-site-verification content="8_xpI-TS3tNV8UPug-Q6Ef3BhKTcy0WOG7dEdAcm2zk"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="祝融"><link rel=dns-prefetch href=//cdn.jsdelivr.net><link rel=preconnect href=https://cdn.jsdelivr.net crossorigin><link rel=dns-prefetch href=//www.googletagmanager.com><link rel=preconnect href=https://www.googletagmanager.com crossorigin><link rel=canonical href=https://zhurongshuo.com/practices/season-4/intelligent-computing-center-construction-guide/part-01/chapter-01/><title>祝融说。 第1章：AI与大模型时代对基础架构的需求</title><meta property="og:title" content="第1章：AI与大模型时代对基础架构的需求"><meta property="og:url" content="https://zhurongshuo.com/practices/season-4/intelligent-computing-center-construction-guide/part-01/chapter-01/"><meta property="og:site_name" content="祝融说。"><meta property="og:locale" content="zh_CN"><meta property="og:type" content="article"><meta property="article:published_time" content="2025-12-07T00:00:00+08:00"><meta property="article:modified_time" content="2025-12-07T00:00:00+08:00"><meta property="article:tag" content="书稿"><meta name=description content="随着信息技术的飞速发展，我们正处在一个由数据驱动、智能赋能的全新时代。人工智能（Artificial Intelligence, AI）不再是科幻小说中的遥远构想，而是已经渗透到社会生产、日常生活的方方面面，成为推动第四次工业革命的核心引擎。近年来，以GPT系列、LLaMA等为代表的大规模预训练模型（Large Language Models, LLMs）取得了突破性进展，其强大的自然语言理解、生成、推理能力，以及在多模态领域的延伸，预示着通用人工智能（AGI）的曙光，也由此开启了波澜壮阔的“大模型时代”。
"><meta property="og:description" content="随着信息技术的飞速发展，我们正处在一个由数据驱动、智能赋能的全新时代。人工智能（Artificial Intelligence, AI）不再是科幻小说中的遥远构想，而是已经渗透到社会生产、日常生活的方方面面，成为推动第四次工业革命的核心引擎。近年来，以GPT系列、LLaMA等为代表的大规模预训练模型（Large Language Models, LLMs）取得了突破性进展，其强大的自然语言理解、生成、推理能力，以及在多模态领域的延伸，预示着通用人工智能（AGI）的曙光，也由此开启了波澜壮阔的“大模型时代”。
"><meta property="og:image" content="https://zhurongshuo.com/images/favicon.ico"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="第1章：AI与大模型时代对基础架构的需求"><meta name=twitter:description content="随着信息技术的飞速发展，我们正处在一个由数据驱动、智能赋能的全新时代。人工智能（Artificial Intelligence, AI）不再是科幻小说中的遥远构想，而是已经渗透到社会生产、日常生活的方方面面，成为推动第四次工业革命的核心引擎。近年来，以GPT系列、LLaMA等为代表的大规模预训练模型（Large Language Models, LLMs）取得了突破性进展，其强大的自然语言理解、生成、推理能力，以及在多模态领域的延伸，预示着通用人工智能（AGI）的曙光，也由此开启了波澜壮阔的“大模型时代”。
"><meta name=twitter:image content="https://zhurongshuo.com/images/favicon.ico"><meta name=keywords content="智算中心建设指南：大模型算力的基础架构,第1章：AI与大模型时代对基础架构的需求"><link rel="shortcut icon" href=https://zhurongshuo.com/images/favicon.ico><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/normalize.css><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/animate-custom.css><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/zozo.css><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/remixicon-custom.css><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/highlight.css><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","headline":"第1章：AI与大模型时代对基础架构的需求","description":"随着信息技术的飞速发展，我们正处在一个由数据驱动、智能赋能的全新时代。人工智能（Artificial Intelligence, AI）不再是科幻小说中的遥远构想，而是已经渗透到社会生产、日常生活的方方面面，成为推动第四次工业革命的核心引擎。近年来，以GPT系列、LLaMA等为代表的大规模预训练模型（Large Language Models, LLMs）取得了突破性进展，其强大的自然语言理解、生成、推理能力，以及在多模态领域的延伸，预示着通用人工智能（AGI）的曙光，也由此开启了波澜壮阔的“大模型时代”。\n","datePublished":"2025-12-07T00:00:00\u002b08:00","dateModified":"2025-12-07T00:00:00\u002b08:00","author":{"@type":"Person","name":"祝融"},"publisher":{"@type":"Organization","name":"祝融说。","logo":{"@type":"ImageObject","url":"https:\/\/zhurongshuo.com\/images\/favicon.ico"}},"image":"https:\/\/zhurongshuo.com\/images\/favicon.ico","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/zhurongshuo.com\/practices\/season-4\/intelligent-computing-center-construction-guide\/part-01\/chapter-01\/"},"keywords":"书稿"}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"首页","item":"https:\/\/zhurongshuo.com\/"},{"@type":"ListItem","position":2,"name":"practices","item":"https:\/\/zhurongshuo.com\/practices/"},{"@type":"ListItem","position":3,"name":"第1章：AI与大模型时代对基础架构的需求","item":"https:\/\/zhurongshuo.com\/practices\/season-4\/intelligent-computing-center-construction-guide\/part-01\/chapter-01\/"}]}</script></head><body><div class="main animate__animated animate__fadeInDown"><div class="nav_container animated fadeInDown"><div class=site_nav id=site_nav><ul><li><a href=https://zhurongshuo.com/>首页</a></li><li><a href=https://zhurongshuo.com/start/>开始</a></li><li><a href=https://zhurongshuo.com/advanced/>进阶rc</a></li><li><a href=https://zhurongshuo.com/posts/>归档</a></li><li><a href=https://zhurongshuo.com/tags/>标签</a></li><li><a href=https://zhurongshuo.com/about/>关于</a></li></ul></div><div class=menu_icon><a id=menu_icon><i class=ri-menu-line></i></a></div></div><div class="header animated fadeInDown"><div class=site_title_container><div class=site_title><h1><a href=https://zhurongshuo.com/><span class=web-font>祝融说。</span></a></h1></div><div class=description><p class=sub_title>法不净空，觉无性也。</p><div class=my_socials><a href=https://zhurongshuo.com/books/ title=book-open><i class=ri-book-open-line></i></a>
<a href=https://zhurongshuo.com/practices/ title=trophy><i class=ri-trophy-line></i></a>
<a href=https://zhurongshuo.com/gallery/ title=gallery><i class=ri-gallery-line></i></a>
<a href=https://zhurongshuo.com/about/ title=game><i class=ri-game-line></i></a>
<a href type=application/rss+xml title=rss target=_blank><i class=ri-rss-fill></i></a></div></div></div></div><div class=content><div class=post_page><div class="post animate__animated animate__fadeInDown"><div class="post_title post_detail_title"><h2><a href=https://zhurongshuo.com/practices/season-4/intelligent-computing-center-construction-guide/part-01/chapter-01/>第1章：AI与大模型时代对基础架构的需求</a></h2><span class=date>2025.12.07</span></div><div class="post_content markdown"><p>随着信息技术的飞速发展，我们正处在一个由数据驱动、智能赋能的全新时代。人工智能（Artificial Intelligence, AI）不再是科幻小说中的遥远构想，而是已经渗透到社会生产、日常生活的方方面面，成为推动第四次工业革命的核心引擎。近年来，以GPT系列、LLaMA等为代表的大规模预训练模型（Large Language Models, LLMs）取得了突破性进展，其强大的自然语言理解、生成、推理能力，以及在多模态领域的延伸，预示着通用人工智能（AGI）的曙光，也由此开启了波澜壮阔的“大模型时代”。</p><p>然而，冰山之上是模型令人惊艳的能力，冰山之下则是支撑其训练与推理的、规模空前庞大的计算、存储和网络基础设施。大模型的“大”，不仅体现在其动辄千亿甚至万亿的参数量，更体现在其对基础架构提出的极致需求上。模型的复杂性与性能，与其所消耗的计算资源呈现出近乎刚性的正相关关系。因此，理解AI特别是大模型对基础架构的特殊需求，是构建高效、稳定、可扩展的大模型算力中心的逻辑起点。</p><p>本章将作为全书的开篇，旨在为读者描绘一幅宏观画卷。我们将首先回归本源，探讨“AI”这一概念的内涵与演进；接着，我们将初步窥探作为现代AI核心驱动力的机器学习算法；然后，通过剖析一个经典的线性回归案例，我们将直观地揭示机器学习算法的计算本质；最后，也是本章的重点，我们将系统性地分析机器学习算法对计算机硬件，特别是对CPU、GPU、TPU/NPU等不同计算单元的特殊需求，阐明为何GPU能够成为当前AI计算的主力引擎，以及为何我们需要为大模型时代构建全新的基础架构。</p><h2 id=11-我们在谈论ai时到底在谈论什么>1.1 我们在谈论AI时，到底在谈论什么</h2><p>“人工智能”这个词汇，自1956年在达特茅斯会议上被正式提出以来，其内涵与外延一直在不断演变和丰富。对于公众而言，AI常常与拥有自我意识的机器人、电影中的“天网”或无所不能的数字助理联系在一起。而在学术界和产业界，AI的定义则更为严谨和务实。</p><h3 id=111-ai的历史演进与流派之争>1.1.1 AI的历史演进与流派之争</h3><p>AI的发展史并非一帆风顺，而是经历了数次期望膨胀与泡沫破裂的“AI寒冬”。我们可以将其大致划分为几个阶段：</p><ol><li>早期萌芽与黄金时代（1950s - 1970s）：达特茅斯会议的召开标志着AI作为一个独立学科的诞生。早期的研究者们充满乐观，认为用机器模拟人类智能的全部特征指日可待。这一时期的主流是符号主义（Symbolism），也被称为逻辑主义或“好老式人工智能”（GOFAI）。符号主义者认为，智能的核心是知识的表示和逻辑推理。他们致力于构建复杂的知识库和专家系统，通过形式化的符号操作来解决问题，例如定理证明、棋类游戏等。早期的跳棋程序、LISP语言的诞生都是这一时期的杰出成果。</li><li>第一次AI寒冬（1970s - 1980s）：随着研究的深入，人们发现符号主义面临着巨大挑战。首先是知识获取瓶颈，为复杂现实世界构建完备、无矛盾的知识库几乎是不可能的。其次是计算复杂性，许多问题的搜索空间随着规模的增长呈指数级爆炸，超出了当时计算机的算力极限。当承诺的宏伟目标迟迟未能实现，政府和企业的资助开始枯竭，AI研究进入了第一个低谷期。</li><li>联结主义（Connectionism）的复兴与第二次AI寒冬（1980s - 2000s）：与符号主义并行发展的另一大流派是联结主义。其灵感来源于人脑的神经网络结构，认为智能产生于大量简单的、相互连接的处理单元（神经元）的集体行为，而非复杂的符号操作。早在1943年，McCulloch和Pitts就提出了M-P神经元模型，1958年，Frank Rosenblatt发明了“感知机”（Perceptron），可以看作是现代神经网络的雏形。然而，1969年Marvin Minsky在其著作《感知机》中指出了单层感知机无法解决“异或”（XOR）等线性不可分问题，这极大地打击了联结主义的研究热情。尽管后来反向传播算法的重新发现以及多层感知机（MLP）的提出理论上解决了这一问题，但由于当时算力不足、数据匮乏，训练深度网络的实际效果并不理想。加之专家系统商业化的泡沫破裂，AI在90年代迎来了第二次寒冬。</li><li>深度学习与大模型时代（2010s - 至今）：进入21世纪，两个关键因素的成熟为AI的再次爆发铺平了道路：大数据（Big Data）和高性能计算（High-Performance Computing, HPC）。互联网的普及产生了海量的数据，为机器学习模型提供了充足的“养料”；而以图形处理器（GPU）为代表的并行计算硬件，其强大的浮点运算能力恰好契合了神经网络训练的需求。</li></ol><p>2012年，Alex Krizhevsky等人使用深度卷积神经网络（CNN）AlexNet在ImageNet图像识别竞赛中以碾压性优势夺冠，其错误率远低于此前基于传统方法的模型。这一事件被视为深度学习革命的引爆点，标志着联结主义的全面胜利。此后，深度学习在计算机视觉、自然语言处理、语音识别等领域高歌猛进，催生了ResNet、BERT、Transformer等一系列里程碑式的模型。</p><p>而当前我们所处的大模型时代，可以看作是深度学习“规模定律”（Scaling Law）的极致体现。研究发现，当模型参数量、数据量和计算量三者协同增长时，模型的性能会持续、可预测地提升。这驱动了业界向着更大、更深、更复杂的模型结构演进，最终诞生了像GPT-3这样参数量达到1750亿的庞然大物。</p><h3 id=112-现代ai的核心定义数据驱动的学习>1.1.2 现代AI的核心定义：数据驱动的学习</h3><p>综上所述，当我们今天谈论AI时，我们主要谈论的是以机器学习（Machine Learning, ML），特别是深度学习（Deep Learning, DL）为代表的数据驱动方法。其核心思想不再是像符号主义那样，由人类专家预先编写好规则和逻辑，而是设计一个具有大量可调参数的模型（Model），然后让这个模型从海量数据（Data）中自动学习（Learn）出隐藏的模式和规律。</p><p>这个过程可以类比于教育一个孩子。我们不会告诉他“有四条腿、毛茸茸、会喵喵叫的动物是猫”这样僵硬的规则，而是会给他看成千上万张猫的图片（数据），让他自己在大脑中形成对“猫”这个概念的抽象认知（模型）。当他下次看到一只从未见过的猫时，也能准确地识别出来，这就是泛化（Generalization）能力。</p><p>因此，现代AI可以被定义为：一个计算机科学领域，致力于研究和构建能够从经验（数据）中学习，并利用所学知识执行特定任务或做出预测的智能体（系统）。</p><p>这个定义包含三个关键要素：</p><ul><li>任务：AI系统被设计用来解决的特定问题，如图像分类、机器翻译、自动驾驶等。</li><li>经验：通常以大规模数据集的形式存在，是模型学习的来源。</li><li>性能：衡量AI系统完成任务好坏的指标，如准确率、召回率、响应时间等。学习的目标就是通过不断从经验中汲取信息，来持续提升在任务上的性能。</li></ul><p>理解了AI的这一定义，我们就能明白，现代AI系统的构建，本质上是一个以数据为中心、以计算为手段的工程实践。而大模型，正是这一实践在当前技术水平下最前沿、最极致的体现。它对基础架构的需求，也正源于其对数据和计算近乎贪婪的渴求。</p><h2 id=12-机器学习算法初窥>1.2 机器学习算法初窥</h2><p>机器学习是实现人工智能的核心技术和方法论。它研究的是计算机如何模拟或实现人类的学习行为，以获取新的知识或技能，并重新组织已有的知识结构，从而不断改善自身的性能。根据学习方式和数据类型的不同，机器学习算法通常可以分为三大类：监督学习、无监督学习和强化学习。</p><h3 id=121-监督学习supervised-learning>1.2.1 监督学习（Supervised Learning）</h3><p>监督学习是目前应用最广泛、最成熟的机器学习方法。其核心特点是训练数据中包含了明确的“答案”，即标签（Label）。模型的目标是学习从输入（Input，也称特征，Feature）到输出（Output，即标签）的映射关系。</p><p>工作原理：就像学生跟着老师做练习题，每一道题（输入数据）都有一个标准答案（标签）。学生通过对比自己的答案和标准答案，不断纠正自己的解题思路。在监督学习中，模型根据输入数据进行预测，然后将预测结果与真实标签进行比较，计算出一个损失（Loss）或误差（Error）。接着，通过优化算法（如梯度下降）来调整模型内部的参数，使得损失值最小化。这个过程反复进行，直到模型在未见过的数据上也能做出足够准确的预测。</p><p>主要任务类型：</p><ul><li>分类（Classification）：预测的目标是一个离散的类别。例如，判断一封邮件是否为垃圾邮件（二分类），或者识别一张图片中的动物是猫、狗还是鸟（多分类）。</li><li>回归（Regression）：预测的目标是一个连续的数值。例如，根据房屋的面积、位置、房龄等特征预测其售价，或者根据历史天气数据预测明天的气温。</li></ul><p>典型算法：线性回归、逻辑回归、支持向量机（SVM）、决策树、随机森林，以及各种类型的神经网络（如CNN用于图像分类，RNN/Transformer用于序列数据回归或分类）。</p><h3 id=122-无监督学习unsupervised-learning>1.2.2 无监督学习（Unsupervised Learning）</h3><p>与监督学习相反，无监督学习的训练数据没有标签。模型需要自己从数据中发现结构、模式或关系。</p><p>工作原理：这好比一位人类学家进入一个未知部落，没有任何向导和字典，只能通过观察人们的行为、语言、交往方式，来自己总结出这个部落的社会结构、风俗习惯。无监督学习算法试图在数据中寻找内在的“结构”。</p><p>主要任务类型：</p><ul><li>聚类（Clustering）：将数据集中的样本划分为若干个簇（Cluster），使得同一个簇内的样本彼此相似，而不同簇的样本差异较大。例如，根据用户的购买行为将他们划分为“高价值用户”、“潜力用户”、“流失风险用户”等群体，以便进行精准营销。</li><li>降维（Dimensionality Reduction）：在保留数据主要信息的前提下，将高维数据转换为低维数据。这既可以用于数据可视化（如将几百维的数据降到2维或3维以便在图上展示），也可以作为其他机器学习任务的预处理步骤，以去除噪声、减少计算量。</li><li>关联规则挖掘（Association Rule Mining）：发现数据项之间有趣的关联关系。最经典的例子就是“啤酒与尿布”的故事，即发现超市中购买尿布的顾客很可能也会购买啤酒。</li></ul><p>典型算法：K-均值聚类（K-Means）、层次聚类、主成分分析（PCA）、t-SNE、Apriori算法。</p><h3 id=123-强化学习reinforcement-learning>1.2.3 强化学习（Reinforcement Learning）</h3><p>强化学习关注的是智能体（Agent）如何在一个环境（Environment）中采取一系列行动（Action），以最大化其获得的累积奖励（Reward）。</p><p>工作原理：强化学习的模式类似于训练宠物。当宠物做出正确的动作（如“坐下”），我们给它一个奖励（如零食）；当它做出错误的动作，可能会有轻微的惩罚或没有奖励。通过不断的试错（Trial and Error），宠物学会了如何根据指令做出正确的行为以获得最多的奖励。在强化学习中，智能体在环境中观察到一个状态（State），然后选择一个行动。环境根据这个行动，会转移到下一个状态，并反馈给智能体一个奖励信号（正向或负向）。智能体的目标是学习一个策略（Policy），即一个从状态到行动的映射，使得长期累积奖励最高。</p><p>应用场景：强化学习在需要做出一系列决策以达成最终目标的场景中表现出色，如：</p><ul><li>游戏AI：AlphaGo击败世界顶尖围棋选手，以及在星际争霸、Dota 2等复杂电子游戏中达到超人水平的AI，都是强化学习的杰作。</li><li>机器人控制：控制机械臂完成抓取、装配等复杂任务，或者让双足机器人学会行走。</li><li>资源调度与优化：数据中心的能源管理、网络流量的动态路由、金融交易策略的制定等。</li></ul><p>典型算法：Q-Learning、SARSA、深度Q网络（DQN）、策略梯度（Policy Gradient）、A3C、PPO。</p><p>这三类学习范式并非完全独立，它们之间也存在交叉，例如半监督学习（结合少量有标签数据和大量无标签数据）、自监督学习（从无标签数据中自动生成标签，是当今大模型训练的核心思想之一）等。但无论哪种算法，其底层都离不开大量的数学运算，这为我们接下来剖析其硬件需求奠定了基础。</p><h2 id=13-一元线性回归算法剖析>1.3 一元线性回归算法剖析</h2><p>为了具体而微地理解机器学习算法的计算本质，我们选择最简单、最经典的监督学习算法——一元线性回归（Univariate Linear Regression）——来进行详细剖析。尽管它很简单，但其所蕴含的“模型定义-损失函数-优化算法”三段式思想，是贯穿几乎所有现代深度学习模型的通用范式。</p><p>假设我们是一家房地产中介，收集到了一批房屋的销售数据，包括房屋的面积（平方米）和其对应的售价（万元）。我们希望建立一个模型，能够根据一个新房屋的面积，来预测其可能的售价。</p><h3 id=131-模型定义>1.3.1 模型定义</h3><p>我们的直觉是，房屋的售价很可能与其面积成正比，面积越大，售价越高。这种关系可以用一条直线来近似描述。在数学上，一条直线的方程可以表示为：</p><p>$$ \hat{y} = wx + b $$</p><p>在这里：</p><ul><li>$x$ 是输入特征，即房屋的面积。</li><li>$\hat{y}$ (读作 y-hat) 是模型的预测输出，即预测的售价。我们用带帽子的符号来区别于真实的售价 $y$。</li><li>$w$ 是权重（Weight），代表直线的斜率。它衡量了房屋面积 $x$ 对售价 $\hat{y}$ 的影响程度。</li><li>$b$ 是偏置（Bias）或截距，代表当房屋面积为0时（虽然不现实，但这是模型的基准值）的预测售价。</li></ul><p>这个方程就是我们定义的模型。我们的任务，就是通过学习，找到一对最优的参数 $(w, b)$，使得这条直线能够最好地“拟合”我们手中的数据点。</p><h3 id=132-损失函数loss-function>1.3.2 损失函数（Loss Function）</h3><p>如何衡量一条直线“拟合得好不好”？对于每一个真实的房屋数据 $(x_i, y_i)$（其中 $x_i$ 是第 $i$ 个房屋的面积， $y_i$ 是其真实售价），我们的模型会给出一个预测值 $\hat{y_i} = wx_i + b$。预测值与真实值之间的差距，就是模型的误差（Error）。</p><p>$$ \text{Error}_i = \hat{y_i} - y_i $$</p><p>我们希望模型在整个数据集上的总体误差尽可能小。直接将所有误差相加可能会因为正负抵消而产生误导。因此，我们通常使用均方误差（Mean Squared Error, MSE）作为损失函数。它计算了所有样本的预测误差的平方的平均值：</p><p>$$ L(w, b) = \frac{1}{N} \sum_{i=1}^{N} (\hat{y_i} - y_i)^2 = \frac{1}{N} \sum_{i=1}^{N} (wx_i + b - y_i)^2 $$</p><p>其中，$N$ 是数据集中样本的总数。</p><p>平方操作有两个好处：一是确保误差值为非负，二是会放大较大的误差，使得模型对那些“离谱”的预测点更加敏感。</p><p>求和与取平均则综合了模型在所有数据点上的表现，给出了一个全局的、可量化的评估指标。</p><p>这个损失函数 $L(w, b)$ 是一个关于参数 $w$ 和 $b$ 的函数。我们的目标，就变成了在数学上寻找一组 $(w, b)$，使得 $L(w, b)$ 的值最小。这在数学上被称为一个优化问题。</p><h3 id=133-优化算法梯度下降gradient-descent>1.3.3 优化算法：梯度下降（Gradient Descent）</h3><p>如何找到让损失函数最小的 $(w, b)$ 呢？最常用、也是最重要的优化算法之一就是梯度下降。</p><p>我们可以把损失函数 $L(w, b)$ 想象成一个二维的山谷地貌，其中 $w$ 和 $b$ 是我们在地图上的坐标，而 $L$ 的值是该坐标点的高度。我们的目标是走到山谷的最低点。</p><p>梯度下降的策略非常直观：</p><ol><li>随机初始化：首先，我们随机选择一个出发点 $(w_0, b_0)$。</li><li>计算梯度：在当前位置，我们计算出山谷地貌在这一点的梯度（Gradient）。梯度是一个向量，指向函数值增长最快的方向。在我们的例子中，梯度是损失函数 $L$ 分别对 $w$ 和 $b$ 的偏导数：$(\frac{\partial L}{\partial w}, \frac{\partial L}{\partial b})$。</li><li>更新参数：我们想要下山，所以应该沿着梯度的相反方向迈出一步。步子的大小由一个叫做学习率（Learning Rate），记为 $\alpha$，的超参数控制。学习率太小，下山速度慢；学习率太大，可能会一步迈到山谷对面，导致震荡甚至无法收敛。参数的更新规则如下：</li></ol><p>$$ w_{\text{new}} = w_{\text{old}} - \alpha \frac{\partial L}{\partial w} $$
$$ b_{\text{new}} = b_{\text{old}} - \alpha \frac{\partial L}{\partial b} $$</p><ol start=4><li>迭代：我们重复步骤2和3，不断地计算梯度、更新参数。每一次迭代，我们都会向着山谷更低的位置移动一点。理论上，经过足够多的迭代，我们就能逼近或到达山谷的最低点，此时的 $(w, b)$ 就是我们要求的最优解。</li></ol><h3 id=134-计算过程的本质>1.3.4 计算过程的本质</h3><p>现在，我们把整个训练过程串起来，并关注其中的计算步骤：</p><p>初始化：随机生成 $w$ 和 $b$ (两个标量)。</p><p>循环（Epochs）：</p><ol><li><p>前向传播（Forward Pass）：
对于数据集中的每一个样本 $x_i$，执行一次乘法和一次加法：$\hat{y_i} = w \cdot x_i + b$。
如果数据集有 $N$ 个样本，这一步需要 $N$ 次乘法和 $N$ 次加法。</p></li><li><p>计算损失：
对于每一个样本，计算误差的平方：$(\hat{y_i} - y_i)^2$。
将所有平方误差相加，然后除以 $N$。
这一步涉及 $N$ 次减法、 $N$ 次乘法、 $(N-1)$ 次加法和1次除法。</p></li><li><p>反向传播（Backward Pass / 计算梯度）：
根据微积分的链式法则，计算损失函数对 $w$ 和 $b$ 的偏导数：
$$ \frac{\partial L}{\partial w} = \frac{1}{N} \sum_{i=1}^{N} 2(wx_i + b - y_i) \cdot x_i $$
$$ \frac{\partial L}{\partial b} = \frac{1}{N} \sum_{i=1}^{N} 2(wx_i + b - y_i) $$
计算这两个梯度值，同样涉及到大量的、在整个数据集上进行的乘法和加法运算。</p></li><li><p>更新参数：
执行两次乘法（$\alpha \cdot$ 梯度）和两次减法来更新 $w$ 和 $b$。</p></li></ol><p>这个简单的例子揭示了机器学习训练的核心计算模式：大量重复的、基于整个数据集的、简单的数学运算（主要是乘法和加法）。</p><p>当我们从一元线性回归扩展到多元线性回归（即用多个特征，如面积、房龄、楼层等来预测房价），输入 $x$ 和权重 $w$ 就从标量变成了向量（Vector）。前向传播就变成了向量的点积运算。</p><p>而当我们进入深度学习领域，模型由许多层神经元组成，每一层都可以看作是一次线性变换（矩阵乘法）和一次非线性激活。此时，模型的参数 $w$ 变成了巨大的矩阵（Matrix），输入数据 $x$ 也常常被组织成矩阵或更高维的张量（Tensor）。训练一个深度神经网络，其核心计算就变成了在海量数据上反复执行大规模的矩阵乘法和加法。</p><p>这个发现至关重要，它直接决定了何种类型的计算硬件更适合执行机器学习任务。</p><h2 id=14-机器学习算法对计算机硬件的特殊需求>1.4 机器学习算法对计算机硬件的特殊需求</h2><p>通过对线性回归的剖析，我们已经知道，机器学习，特别是深度学习，其计算负载呈现出一种非常独特的模式。这种模式与传统通用计算任务（如网页浏览、文档处理、数据库查询等）截然不同，从而对底层硬件提出了特殊的要求。</p><h3 id=141-机器学习算法的核心运算特征>1.4.1 机器学习算法的核心运算特征</h3><p>我们可以将机器学习算法的运算特征总结为以下几点：</p><h4 id=大规模并行性massive-parallelism>大规模并行性（Massive Parallelism）</h4><p>无论是前向传播计算每个样本的预测值，还是反向传播计算每个样本对梯度的贡献，这些计算在样本之间是相互独立的。这意味着我们可以同时对成千上万个数据样本进行相同的运算，而不需要等待彼此的结果。</p><p>在神经网络中，同一层内的神经元的计算也是相互独立的。一个拥有1024个神经元的层，其输出可以被看作是1024个独立的点积运算，这些都可以并行执行。</p><p>这种数据并行和模型并行的特性，使得算法天然地适合在拥有大量计算核心的并行处理器上运行。</p><h4 id=计算密集型compute-intensive>计算密集型（Compute-Intensive）</h4><p>训练一个大模型可能需要数以万亿（Trillions）甚至千万亿（Peta）次的浮点运算（FLOPs）。例如，训练GPT-3一次的计算量据估计高达 3.14 x 10^23 FLOPs。</p><p>核心运算是矩阵乘法（GEMM: General Matrix-Matrix Multiplication）和卷积（Convolution）。这些操作虽然数学上简单，但规模巨大。一个大模型的权重矩阵可能包含数十亿个元素，与输入数据进行一次乘法就需要惊人的计算量。</p><h4 id=高内存带宽需求high-memory-bandwidth-requirement>高内存带宽需求（High Memory Bandwidth Requirement）</h4><p>计算再快，也需要数据喂给它。在训练过程中，巨大的模型参数（权重矩阵）和一批批的训练数据（mini-batch）需要被频繁地从主内存加载到计算单元中。</p><p>如果数据传输的速度（内存带宽）跟不上计算单元消耗数据的速度，计算单元就会处于“空闲”等待状态，造成算力的严重浪费。这种现象被称为“内存墙”（Memory Wall）。</p><p>因此，对于AI计算硬件而言，拥有极高的内存带宽与拥有强大的计算能力同等重要。</p><h4 id=对浮点精度的容忍度tolerance-for-reduced-floating-point-precision>对浮点精度的容忍度（Tolerance for Reduced Floating-Point Precision）</h4><p>传统的科学计算通常要求双精度浮点（FP64）以保证结果的精确性。然而，研究发现，神经网络对噪声具有一定的鲁棒性，其训练和推理过程并不总是需要那么高的精度。</p><p>使用较低的精度，如单精度（FP32）、半精度（FP16）甚至8位整数（INT8），可以带来显著的好处：</p><ul><li>更快的计算速度：在相同的硬件逻辑下，处理低精度数据的速度更快。例如，一个32位的计算单元理论上可以同时处理两个16位的操作。</li><li>更低的内存占用：模型参数和中间结果占用的内存减半（FP32 -> FP16），可以训练更大的模型，或者使用更大的批量大小（batch size）。</li><li>更低的功耗：数据搬运和计算的能耗都随之降低。</li></ul><p>因此，现代AI硬件通常会针对这些混合精度计算进行专门优化。例如，NVIDIA的Tensor Core和Google的TPU都支持bfloat16 (BF16) 和FP16等格式。</p><p>了解了这些核心特征，我们就可以来评估不同类型的处理器在应对机器学习任务时的优劣了。</p><h3 id=142-使用cpu实现机器学习算法和并行加速>1.4.2 使用CPU实现机器学习算法和并行加速</h3><p>中央处理器（Central Processing Unit, CPU）是我们最熟悉的计算机核心。它被设计成一个通用的、低延迟的处理器。</p><p>架构特点：</p><ul><li>少而强的核心：典型的服务器CPU拥有数十个（如32, 64, 128）非常强大和复杂的计算核心。</li><li>复杂的控制逻辑：每个核心都擅长处理复杂的指令流，包括大量的分支预测、乱序执行等，以优化单线程性能。</li><li>大容量缓存（Cache）：CPU拥有多级高速缓存（L1, L2, L3），用于存储常用数据，以弥补主内存（DRAM）访问速度的不足。</li><li>为串行任务优化：CPU的设计哲学是尽可能快地完成一个接一个的复杂任务。</li></ul><p>在机器学习中的表现：</p><p>对于小规模的模型和数据，或者那些逻辑复杂、分支较多的算法（如决策树），CPU完全可以胜任。</p><p>然而，当面对深度学习这种大规模并行计算任务时，CPU的架构瓶颈就显现出来了。其几十个核心，相对于神经网络中数百万甚至数十亿次的并行运算需求，简直是杯水车薪。大部分时间里，绝大多数计算任务都在排队等待被少数几个核心处理。</p><p>CPU的并行加速尝试：</p><p>SIMD（Single Instruction, Multiple Data）：这是CPU进行并行计算的主要手段。现代CPU都支持SIMD指令集，如SSE、AVX（Advanced Vector Extensions）等。AVX512指令可以在一个时钟周期内对一个512位的向量（例如16个FP32浮点数或32个FP16浮点数）执行相同的操作。通过使用专门优化的数学库（如Intel MKL-DNN, OpenBLAS），可以在CPU上实现一定程度的并行加速。</p><p>多线程/多核并行：利用CPU的多个核心，将任务（如一个mini-batch的数据）拆分给不同的核心去处理。这需要通过OpenMP或TBB等多线程编程框架来实现。</p><p>结论：尽管通过软件优化可以压榨出CPU的并行潜力，但其“通用”的设计初衷决定了它在处理“高度特化”的大规模并行计算负载时，效率远不如专门为此设计的硬件。CPU更适合扮演“总管”的角色，负责操作系统的运行、数据预处理、任务调度等控制密集型工作，而将计算密集型的“重活”卸载给更专业的协处理器。</p><h3 id=143-机器学习算法的主力引擎gpu>1.4.3 机器学习算法的主力引擎——GPU</h3><p>图形处理器（Graphics Processing Unit, GPU）的崛起，是深度学习革命能够发生的最重要的硬件基础。有趣的是，GPU最初并非为AI而生。</p><p>架构特点：</p><ul><li>众核架构（Many-Core Architecture）：与CPU相反，GPU拥有成千上万个（例如NVIDIA A100拥有6912个CUDA Core）相对简单、功耗较低的计算核心。</li><li>为并行任务优化：GPU的设计哲学是为了实时渲染三维图形。渲染一帧画面，需要对屏幕上的数百万个像素点执行相似的着色、变换等计算。这些像素点的计算彼此独立，天然就是大规模并行任务。GPU的架构正是为了高效处理这种任务而生。</li><li>高吞吐量设计：GPU的设计目标是最大化吞吐量（Throughput），即单位时间内完成的总工作量，而不是像CPU那样追求低延迟（Latency），即完成单个任务的速度。</li><li>极高内存带宽：为了喂饱数千个计算核心，GPU通常配备了专用的、极高带宽的显存（如GDDR6X, HBM2e, HBM3），其带宽远超CPU所连接的DDR内存。</li></ul><p>从图形渲染到通用计算（GPGPU）：</p><p>研究人员敏锐地发现，GPU处理像素的并行计算模式，与科学计算（如物理模拟、信号处理）中的许多算法，特别是线性代数运算，惊人地相似。一个矩阵的元素，就可以被看作是屏幕上的一个像素。</p><p>NVIDIA于2007年推出的CUDA（Compute Unified Device Architecture）平台，是一个革命性的里程碑。它提供了一套完整的编程模型、API和工具链，让开发者可以不再通过模拟图形API（如OpenGL, DirectX）的复杂方式，而是用类似C/C++的语言直接在GPU上编写通用并行计算程序。这极大地降低了GPGPU的门槛，开启了GPU作为通用计算加速器的时代。</p><p>GPU与深度学习的天作之合：</p><p>深度学习的核心运算——大规模矩阵乘法，完美地契合了GPU的架构优势。一个大的矩阵乘法可以被分解为成千上万个小的、独立的点积运算，然后分配给GPU的数千个核心去同时执行。</p><p>GPU的高内存带宽，确保了巨大的模型参数和数据能够被快速地送入计算核心，避免了算力闲置。</p><p>为了进一步加速AI负载，现代GPU还集成了专用的硬件单元。例如，NVIDIA从Volta架构开始引入Tensor Core。这是一个专门为混合精度矩阵乘加运算（<code>D = A * B + C</code>）设计的硬件电路，可以在一个时钟周期内完成一个4x4的FP16矩阵乘法和FP32累加，其理论峰值性能远超普通的CUDA Core。这使得GPU在执行深度学习训练和推理任务时，效率得到了指数级的提升。</p><p>结论：GPU凭借其大规模并行计算架构、高内存带宽以及针对AI运算的专用硬件（如Tensor Core），成为了当前机器学习，尤其是大模型训练和推理任务中无可争议的主力引擎。构建大模型算力中心，在很大程度上就是构建一个由成百上千张高端GPU卡组成的、高效协同工作的计算集群。</p><h3 id=144-机器学习算法的新引擎tpu和npu>1.4.4 机器学习算法的新引擎——TPU和NPU</h3><p>当一个计算任务的模式变得足够固定和重要时，为其设计专用集成电路（Application-Specific Integrated Circuit, ASIC）就成为了一种自然的选择。ASIC为了一个特定的任务而“量身定制”，可以舍弃所有无关的通用逻辑，从而在性能和能效比上达到极致。TPU和NPU就是这一思想的产物。</p><p>TPU（Tensor Processing Unit）：</p><p>TPU是Google为加速其内部的神经网络负载（从搜索、翻译到广告）而专门设计的ASIC。</p><p>其核心创新是脉动阵列（Systolic Array）。这是一个由大量简单的处理单元（乘法-累加器，MAC）组成的二维网格。数据像血液在心脏中一样，以固定的节奏“脉动”地流过这个阵列。权重参数被预先加载到阵列的MAC单元中，而输入数据则逐个时钟周期地送入阵列。在每个时钟周期，每个MAC单元都会执行一次乘加运算，并将结果传递给相邻的单元。</p><p>这种架构的优势在于：</p><ul><li>极高的计算密度：在一个芯片上可以集成非常大规模的脉动阵列（如TPUv3拥有两个128x128的脉动阵列）。</li><li>极高的能效比：数据在芯片内部规律地流动，极大地减少了对高功耗的内存读写需求。大部分时间，数据都在计算单元之间直接传递。</li></ul><p>TPU的设计完全服务于神经网络计算，它在执行大规模矩阵乘法时效率极高，但在处理通用计算或复杂控制流时则能力有限。Google通过构建由数千个TPU组成的Pod，形成了强大的、专门用于AI训练和推理的超级计算机。</p><p>NPU（Neural-network Processing Unit）：</p><ul><li>NPU是一个更宽泛的概念，泛指所有用于加速神经网络运算的处理器。TPU是NPU的一种，但NPU也包括了来自其他厂商的各种设计。</li><li>NPU通常被集成在端侧设备（Edge Devices）中，如智能手机（例如苹果的Neural Engine、高通的Hexagon DSP）、智能摄像头、自动驾驶汽车的域控制器等。</li><li>与数据中心级的TPU追求极致的峰值性能不同，端侧NPU更侧重于能效比（Performance per Watt）。它们需要在有限的功耗和散热预算下，高效地执行推理任务。</li></ul><p>各种NPU的设计百花齐放，但其核心思想大同小异：通过硬件固化卷积、矩阵乘法、激活函数等常用神经网络算子，实现比CPU或通用DSP高出几个数量级的性能和效率。</p><p>结论：TPU和NPU代表了AI硬件发展的“特化”方向。它们通过为神经网络的核心运算模式定制硬件，实现了无与伦比的性能和能效。虽然它们的通用性不如GPU，但在其擅长的领域内，它们是极其强大的新引擎。在未来，我们很可能会看到一个由CPU（负责控制）、GPU（负责通用并行计算和部分AI任务）、以及NPU/TPU（负责重度AI任务）等异构计算单元协同工作的计算新范式。</p><h2 id=15-本章小结>1.5 本章小结</h2><p>本章作为全书的基石，带领读者完成了一次从宏观到微观的认知之旅，旨在建立起对大模型时代基础架构需求的根本性理解。</p><p>我们首先回顾了人工智能曲折而辉煌的发展历程，明确了我们今天所谈论的AI，其核心是基于数据驱动的机器学习，而深度学习和大模型是这一范式下最前沿的成果。</p><p>接着，我们通过对监督学习、无监督学习和强化学习这三大机器学习范式的初步介绍，了解了不同算法解决问题的思路。为了深入探究其计算本质，我们详细剖析了一个最基础的一元线性回归案例。这个简单的例子清晰地揭示了所有复杂机器学习算法背后共通的计算模式：基于海量数据、大规模、高并行的、以乘加运算为主的密集计算。</p><p>最后，我们将这一核心计算特征作为“标尺”，去衡量和分析了不同类型的计算硬件。我们认识到：</p><p>CPU作为通用处理器，其“少而强”的串行优化核心架构，难以高效应对AI负载的“人海战术”。它更适合扮演系统“大脑”的角色。</p><p>GPU凭借其为图形渲染而生的“众核”并行架构和高带宽内存，与深度学习的计算模式“一拍即合”，通过GPGPU和CUDA生态的成熟，以及Tensor Core等专用硬件的加持，成为了当前AI计算，特别是大模型训练的绝对主力。</p><p>TPU/NPU等ASIC，则代表了硬件发展的终极“特化”形态，它们将神经网络的核心运算固化到硬件电路中，以追求极致的性能与能效比，构成了AI计算生态中不可或缺的新引擎。</p><p>通过本章的学习，我们应该深刻地认识到，大模型对基础架构的需求，并非简单的堆砌服务器，而是由其内在的算法特性所决定的、对大规模并行计算能力、高内存与网络带宽、以及异构计算协同的系统性、结构性需求。这为我们后续章节深入探讨GPU硬件、服务器设计、网络与存储架构、以及云原生平台等具体技术细节，奠定了坚实的理论基础。从下一章开始，我们将正式进入这些激动人心的技术世界。</p></div><div class=post_footer><div class=meta><div class=info><span class="field tags"><i class=ri-stack-line></i>
<a href=https://zhurongshuo.com/tags/%E4%B9%A6%E7%A8%BF/>书稿</a></span></div></div></div></div><div class=doc_comments></div></div></div></div><a id=back_to_top href=# class=back_to_top><i class=ri-arrow-up-s-line></i></a><footer class=footer><div class=powered_by><a href=https://varkai.com>Designed by VarKai, </a><a href=http://www.gohugo.io/>Proudly published with Hugo,</a></div><div class=footer_slogan><span>法不净空，觉无性也。</span></div><div class=powered_by style=margin-top:10px;font-size:14px><a href=https://zhurongshuo.com/>Copyright © 2010-2025 祝融说 zhurongshuo.com All Rights Reserved.</a></div></footer><script defer src=https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><link href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.css rel=stylesheet integrity="sha256-7qiTu3a8qjjWtcX9w+f2ulVUZSUdCZFEK62eRlmLmCE=" crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin=anonymous></script><script defer src=https://zhurongshuo.com/js/zozo.js></script><script type=text/javascript async src="https://cdn.jsdelivr.net/npm/mathjax@2.7.8/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["[[","]]"]],processEscapes:!0,processEnvironments:!0,skipTags:["script","noscript","style","textarea","pre"],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}}),MathJax.Hub.Queue(function(){var e,t=MathJax.Hub.getAllJax();for(e=0;e<t.length;e+=1)t[e].SourceElement().parentNode.className+=" has-jax"})</script><style>code.has-jax{font:inherit;font-size:100%;background:inherit;border:inherit;color:#515151}</style><script async src="https://www.googletagmanager.com/gtag/js?id=G-KKJ5ZEG1NB"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KKJ5ZEG1NB")</script></body></html>