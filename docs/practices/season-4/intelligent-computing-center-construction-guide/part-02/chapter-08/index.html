<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><meta name=google-site-verification content="8_xpI-TS3tNV8UPug-Q6Ef3BhKTcy0WOG7dEdAcm2zk"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="祝融"><link rel=dns-prefetch href=//cdn.jsdelivr.net><link rel=preconnect href=https://cdn.jsdelivr.net crossorigin><link rel=dns-prefetch href=//www.googletagmanager.com><link rel=preconnect href=https://www.googletagmanager.com crossorigin><link rel=canonical href=https://zhurongshuo.com/practices/season-4/intelligent-computing-center-construction-guide/part-02/chapter-08/><title>祝融说。 第8章 GPU虚拟化调度方案</title><meta property="og:title" content="第8章 GPU虚拟化调度方案"><meta property="og:url" content="https://zhurongshuo.com/practices/season-4/intelligent-computing-center-construction-guide/part-02/chapter-08/"><meta property="og:site_name" content="祝融说。"><meta property="og:locale" content="zh_CN"><meta property="og:type" content="article"><meta property="article:published_time" content="2025-12-07T00:00:00+08:00"><meta property="article:modified_time" content="2025-12-07T00:00:00+08:00"><meta property="article:tag" content="书稿"><meta name=description content="在第七章中，我们已经探讨了“板卡级”的算力调度技术，核心是解决如何将一张物理GPU卡共享给多个虚拟机或容器的问题。我们学习了GPU直通、vGPU、MIG以及基于容器的设备插件等多种技术。这些技术可以被看作是GPU虚拟化的“第一层境界”，它们大多依赖于硬件厂商提供的原生支持，在隔离性、性能和实现方式上各有侧重，但共同的目标是将物理GPU转化为可被上层调度系统识别和分配的、粒度或粗或细的“GPU资源块”。
"><meta property="og:description" content="在第七章中，我们已经探讨了“板卡级”的算力调度技术，核心是解决如何将一张物理GPU卡共享给多个虚拟机或容器的问题。我们学习了GPU直通、vGPU、MIG以及基于容器的设备插件等多种技术。这些技术可以被看作是GPU虚拟化的“第一层境界”，它们大多依赖于硬件厂商提供的原生支持，在隔离性、性能和实现方式上各有侧重，但共同的目标是将物理GPU转化为可被上层调度系统识别和分配的、粒度或粗或细的“GPU资源块”。
"><meta property="og:image" content="https://zhurongshuo.com/images/favicon.ico"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="第8章 GPU虚拟化调度方案"><meta name=twitter:description content="在第七章中，我们已经探讨了“板卡级”的算力调度技术，核心是解决如何将一张物理GPU卡共享给多个虚拟机或容器的问题。我们学习了GPU直通、vGPU、MIG以及基于容器的设备插件等多种技术。这些技术可以被看作是GPU虚拟化的“第一层境界”，它们大多依赖于硬件厂商提供的原生支持，在隔离性、性能和实现方式上各有侧重，但共同的目标是将物理GPU转化为可被上层调度系统识别和分配的、粒度或粗或细的“GPU资源块”。
"><meta name=twitter:image content="https://zhurongshuo.com/images/favicon.ico"><meta name=keywords content="智算中心建设指南：大模型算力的基础架构,第8章 GPU虚拟化调度方案"><link rel="shortcut icon" href=https://zhurongshuo.com/images/favicon.ico><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/normalize.css><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/animate-custom.css><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/zozo.css><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/remixicon-custom.css><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/highlight.css><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","headline":"第8章 GPU虚拟化调度方案","description":"在第七章中，我们已经探讨了“板卡级”的算力调度技术，核心是解决如何将一张物理GPU卡共享给多个虚拟机或容器的问题。我们学习了GPU直通、vGPU、MIG以及基于容器的设备插件等多种技术。这些技术可以被看作是GPU虚拟化的“第一层境界”，它们大多依赖于硬件厂商提供的原生支持，在隔离性、性能和实现方式上各有侧重，但共同的目标是将物理GPU转化为可被上层调度系统识别和分配的、粒度或粗或细的“GPU资源块”。\n","datePublished":"2025-12-07T00:00:00\u002b08:00","dateModified":"2025-12-07T00:00:00\u002b08:00","author":{"@type":"Person","name":"祝融"},"publisher":{"@type":"Organization","name":"祝融说。","logo":{"@type":"ImageObject","url":"https:\/\/zhurongshuo.com\/images\/favicon.ico"}},"image":"https:\/\/zhurongshuo.com\/images\/favicon.ico","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/zhurongshuo.com\/practices\/season-4\/intelligent-computing-center-construction-guide\/part-02\/chapter-08\/"},"keywords":"书稿"}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"首页","item":"https:\/\/zhurongshuo.com\/"},{"@type":"ListItem","position":2,"name":"practices","item":"https:\/\/zhurongshuo.com\/practices/"},{"@type":"ListItem","position":3,"name":"第8章 GPU虚拟化调度方案","item":"https:\/\/zhurongshuo.com\/practices\/season-4\/intelligent-computing-center-construction-guide\/part-02\/chapter-08\/"}]}</script></head><body><div class="main animate__animated animate__fadeInDown"><div class="nav_container animated fadeInDown"><div class=site_nav id=site_nav><ul><li><a href=https://zhurongshuo.com/>首页</a></li><li><a href=https://zhurongshuo.com/start/>开始</a></li><li><a href=https://zhurongshuo.com/advanced/>进阶rc</a></li><li><a href=https://zhurongshuo.com/posts/>归档</a></li><li><a href=https://zhurongshuo.com/tags/>标签</a></li><li><a href=https://zhurongshuo.com/about/>关于</a></li></ul></div><div class=menu_icon><a id=menu_icon><i class=ri-menu-line></i></a></div></div><div class="header animated fadeInDown"><div class=site_title_container><div class=site_title><h1><a href=https://zhurongshuo.com/><span class=web-font>祝融说。</span></a></h1></div><div class=description><p class=sub_title>法不净空，觉无性也。</p><div class=my_socials><a href=https://zhurongshuo.com/books/ title=book-open><i class=ri-book-open-line></i></a>
<a href=https://zhurongshuo.com/practices/ title=trophy><i class=ri-trophy-line></i></a>
<a href=https://zhurongshuo.com/gallery/ title=gallery><i class=ri-gallery-line></i></a>
<a href=https://zhurongshuo.com/about/ title=game><i class=ri-game-line></i></a>
<a href type=application/rss+xml title=rss target=_blank><i class=ri-rss-fill></i></a></div></div></div></div><div class=content><div class=post_page><div class="post animate__animated animate__fadeInDown"><div class="post_title post_detail_title"><h2><a href=https://zhurongshuo.com/practices/season-4/intelligent-computing-center-construction-guide/part-02/chapter-08/>第8章 GPU虚拟化调度方案</a></h2><span class=date>2025.12.07</span></div><div class="post_content markdown"><p>在第七章中，我们已经探讨了“板卡级”的算力调度技术，核心是解决如何将一张物理GPU卡共享给多个虚拟机或容器的问题。我们学习了GPU直通、vGPU、MIG以及基于容器的设备插件等多种技术。这些技术可以被看作是GPU虚拟化的“第一层境界”，它们大多依赖于硬件厂商提供的原生支持，在隔离性、性能和实现方式上各有侧重，但共同的目标是将物理GPU转化为可被上层调度系统识别和分配的、粒度或粗或细的“GPU资源块”。</p><p>然而，随着AI应用的场景越来越多样化，特别是云原生AI平台的蓬勃发展，仅仅满足于这种“资源块”式的分配，已经无法满足业界对极致弹性、极致利用率和极致灵活性的追求。例如：</p><p>一个用户可能只需要100MB的显存和极少量的算力来运行一个TensorBoard服务，为他分配一个哪怕是最小的MIG实例（如5GB显存）也是巨大的浪费。</p><p>一个推理任务可能在高峰期需要大量算力，而在低谷期几乎空闲，我们能否实现算力的“按需伸缩”，而不是静态绑定？</p><p>我们能否将集群中所有GPU的显存和算力真正地“池化”，形成一个统一的、巨大的虚拟GPU资源池，然后像CPU和内存一样，按需、精确地（例如，以1MB显存、1%算力为单位）切割给任意一个容器？</p><p>为了实现这些更宏伟的目标，我们需要进入GPU虚拟化的“第二层境界”。本章，我们将专题深入探讨各种GPU虚拟化调度方案。这不再仅仅是资源的分配，而是通过更深层次的技术手段，对GPU的算力和显存资源进行虚拟化、池化和精细化管理。这些方案往往更具“侵入性”，通过对CUDA API、驱动程序甚至硬件指令的拦截和重定向，创造出一种“以假乱真”的虚拟GPU（vGPU）环境。</p><p>我们将从三个维度来全面考察这个领域的生态图景：</p><ol><li>NVIDIA的官方GPU虚拟化调度方案：我们将重新梳理并深入NVIDIA自家的技术体系，从最基础的API Remoting，到企业级的GRID vGPU，再到硬件级的MIG，理解其官方对不同虚拟化需求的层次化解答。</li><li>其他硬件厂商的GPU虚拟化调度方案：我们将简要介绍AMD和Intel等其他主流GPU厂商在虚拟化方面的思路和技术实现，如SR-IOV和GVT-g，以形成更广阔的行业视野。</li><li>云厂商与开源社区基于容器的GPU虚拟化调度方案：这是本章的重点。我们将深入剖析以阿里云cGPU、腾讯云qGPU、以及开源社区方案为代表的、在云原生环境下大放异彩的创新技术。这些方案大多基于容器，通过API劫持等技术，实现了对GPU算力和显存的超细粒度共享和池化，是当前提升GPU集群资源利用率的最前沿探索。</li></ol><p>通过本章的学习，你将掌握一幅完整的GPU虚拟化技术图谱，从硬件厂商的“正统”方案，到云厂商和开源社区的“奇技淫巧”，你将理解它们各自背后的核心原理、技术权衡以及最适合的应用场景。这将使你能够在构建下一代AI平台时，针对资源利用率、隔离性、性能和成本等核心指标，做出最高阶的架构决策。</p><h2 id=81-nvidia的gpu虚拟化调度方案>8.1 NVIDIA的GPU虚拟化调度方案</h2><p>NVIDIA作为GPU领域的绝对领导者，其自身的GPU虚拟化技术体系经过了多年的演进，形成了一套层次分明、覆盖不同场景的解决方案。理解这套“官方”体系，是理解其他所有虚拟化方案的基础和参照系。</p><h3 id=811-基础层api-remoting与vcuda>8.1.1 基础层：API Remoting与vCUDA</h3><p>在vGPU和MIG等重量级方案出现之前，学术界和早期工业界就已经在探索通过软件方式实现GPU共享。其最核心、最基础的思想就是API远程调用（API Remoting）。</p><p>核心原理：</p><ol><li>客户端（Client）：在一个没有物理GPU的客户机（可以是VM或容器）上，提供一个“伪造”的CUDA运行时库（shim library）。这个库拥有与原生<code>libcuda.so</code>完全相同的API接口。</li><li>服务器端（Server）：在一台拥有物理GPU的服务器上，运行一个服务守护进程。</li><li>API拦截与转发：当客户端的应用程序调用一个CUDA API时（例如<code>cudaMalloc</code>），这个调用并不会在本地执行，而是被“伪造”的库拦截（Intercept）。</li><li>拦截后，这个库会将API调用的名称、参数等信息进行序列化（Serialization），并通过网络（如TCP/IP）转发给远端的服务器守护进程。</li><li>远程执行与结果返回：服务器守护进程接收到请求后，反序列化出原始的API调用，然后在本地的物理GPU上真正地执行这个调用。</li><li>执行完毕后，如果API有返回值或需要通过指针返回数据，服务器守护进程会将结果再次序列化，通过网络发回给客户端的“伪造”库，最终返回给应用程序。</li></ol><p>vCUDA项目：vCUDA是早期一个著名的、基于API Remoting思想的开源学术项目。它完整地实现了上述流程，允许多个没有GPU的虚拟机，共享使用一台物理服务器上的GPU资源。</p><p>优点：</p><p>架构简单直观：概念清晰，易于理解。</p><p>打破物理边界：首次实现了GPU资源的跨节点、网络化共享，理论上可以将整个数据中心的GPU池化。</p><p>缺点：</p><p>性能瓶颈严重：</p><p>网络延迟：每一次API调用都需要经过一次网络来回，对于那些需要频繁调用大量小API的应用，其性能开销是灾难性的。</p><p>数据传输：当API调用涉及大量数据传输时（如<code>cudaMemcpy</code>），所有数据都需要通过CPU和网络进行拷贝，完全丧失了PCIe和NVLink的带宽优势。</p><p>内核启动开销大：启动一个计算内核本身也成了一次数百微秒甚至毫秒级的网络操作。</p><p>兼容性问题：需要为每一个新版本的CUDA驱动和API重写和适配“伪造”库，维护成本极高。</p><p>功能不完整：很难完整地实现所有复杂的CUDA功能，特别是涉及底层硬件交互和指针操作的功能。</p><p>现状与意义：纯粹的API Remoting方案由于其固有的性能缺陷，在今天已经很少被直接用于高性能的AI训练，但它的核心思想——API拦截与转发——却被后续许多更高级的虚拟化方案所借鉴和发扬光大。它是理解cGPU等技术的思想源头。</p><h3 id=812-企业级标准nvidia-grid-vgpu>8.1.2 企业级标准：NVIDIA GRID vGPU</h3><p>我们在第七章已经详细介绍了NVIDIA vGPU的原理。这里，我们将其置于NVIDIA的整体方案中，再次强调其定位。</p><p>定位：GRID vGPU是NVIDIA官方主推的、面向企业级虚拟化环境（主要是VMware vSphere, Citrix Hypervisor等）的商业GPU虚拟化解决方案。其产品品牌经过多次演变，现在主要整合在NVIDIA AI Enterprise (NVAIE)和NVIDIA RTX Virtual Workstation (vWS)等软件套件中。</p><p>核心技术回顾：</p><p>仲裁模型（Mediated Pass-through）：与API Remoting不同，vGPU的API转发发生在虚拟机与Hypervisor之间，通过一个高效的、专门的VMM通道，而不是通用的网络。</p><p>vGPU Manager：作为运行在Hypervisor中的“总管”，负责所有vGPU实例的创建、调度和隔离。</p><p>时间分片调度：在硬件层面，通过时间分片技术，让多个vGPU实例轮流使用物理GPU的计算引擎。</p><p>显存隔离：vGPU Manager为每个vGPU实例在物理显存中划分出一段固定的、受保护的区域，确保了显存的强隔离。</p><p>在NVIDIA体系中的角色：</p><p>虚拟化环境的“正统”方案：它是唯一被NVIDIA官方完整支持、能够在VMware等主流虚拟化平台上提供完整功能、性能和企业级服务的方案。</p><p>覆盖图形与计算：vGPU不仅支持CUDA计算，也完美支持OpenGL、DirectX等图形API，因此广泛应用于云桌面（VDI）和云游戏等场景。</p><p>软件定义的灵活性：通过vGPU Manager和不同的vGPU配置文件，管理员可以灵活地为不同的VM分配不同大小的显存，并应用不同的调度策略。</p><h3 id=813-硬件级虚拟化nvidia-mig-multi-instance-gpu>8.1.3 硬件级虚拟化：NVIDIA MIG (Multi-Instance GPU)</h3><p>MIG同样在第七章有所介绍。它代表了NVIDIA在硬件层面解决GPU共享问题的最新思路。</p><p>定位：MIG是面向裸金属和容器化环境的、提供强隔离的多租户共享方案。它尤其适用于需要在一个物理节点上运行多个互相不信任、且需要有性能保障的AI任务的场景，如公有云的PaaS/CaaS平台。</p><p>核心技术回顾：</p><p>空间分片（Spatial Slicing）：MIG在硬件层面，将GPU的物理资源（SM、L2缓存、显存控制器等）进行静态地、空间上地分割。</p><p>GPU实例（GPU Instance, GI）：每个分割出的单元就是一个GI。每个GI都拥有自己独立的、不受干扰的硬件资源通路。</p><p>计算实例（Compute Instance, CI）：在一个GI内部，还可以进一步创建多个CI。CI共享GI的显存，但拥有各自独立的计算引擎状态，适用于需要隔离执行上下文但可以共享数据的场景。</p><p>MIG与vGPU的对比：</p><p>隔离性：MIG的隔离性是硬件级的，比vGPU的软件仲裁隔离更强、更彻底。一个GI的故障不可能影响到另一个GI。</p><p>性能：MIG没有API转发的开销，每个GI的性能是可预测的、有保障的，因为它独占了一部分物理硬件。vGPU的性能则是共享的、有竞争的，一个vGPU的实际性能取决于当时有多少个其他vGPU在同时工作。</p><p>灵活性：vGPU的共享是动态的（时间分片），一个vGPU在空闲时，其他vGPU可以利用全部算力。MIG的划分是静态的，一个GI即使空闲，它的硬件资源也无法被其他GI使用。</p><p>显存：vGPU的显存分配更灵活（由配置文件决定）。MIG的显存分配是与硬件切片绑定的，规格固定。</p><p>适用环境：vGPU主要面向VM环境。MIG则更适合裸金属和容器环境，与Kubernetes的Device Plugin结合得非常好。</p><p>NVIDIA方案总结：</p><p>NVIDIA提供了一套从软件到硬件、从灵活共享到强隔离的“组合拳”：</p><p>MIG提供了最强的隔离和性能保障，但灵活性稍差，是多租户容器平台的理想选择。</p><p>vGPU提供了最平衡的灵活性、隔离性和功能完整性，是企业级虚拟化环境（VM）的首选商业方案。</p><p>MPS（Multi-Process Service）则是一种轻量级的时间分片方案，适用于在单个用户、可信环境下，让多个小任务（如推理服务）共享一张卡，以提升吞吐量。</p><p>API Remoting的思想则作为一种基础技术，被开源社区和云厂商发扬光大，衍生出了更多创新的虚拟化方案。</p><h2 id=82-其他硬件厂商的gpu虚拟化调度方案>8.2 其他硬件厂商的GPU虚拟化调度方案</h2><p>虽然NVIDIA在数据中心GPU市场占据主导，但了解AMD和Intel的方案有助于我们形成一个更全面的视角。它们的方案大多围绕着业界标准化的I/O虚拟化技术展开。</p><h3 id=821-amd的sr-iov方案>8.2.1 AMD的SR-IOV方案</h3><p>AMD是SR-IOV（Single Root I/O Virtualization）技术在GPU虚拟化领域的主要推动者。其MxGPU技术就是基于SR-IOV实现的。</p><p>核心原理：</p><p>AMD的数据中心GPU（如Instinct MI系列）在硬件上支持SR-IOV。</p><p>通过在Hypervisor中加载物理功能（PF）驱动，可以将一张物理GPU卡在硬件上划分为多个虚拟功能（VF）。例如，一张MI100可以划分为最多8个VF。</p><p>每个VF都拥有自己独立的调度队列、显存页表和中断，从硬件上看就是一个独立的GPU设备。</p><p>这些VF可以被直接“直通”给不同的虚拟机。</p><p>虚拟机内部加载AMD的VF驱动，即可像使用物理GPU一样使用这个VF。</p><p>与NVIDIA vGPU的对比：</p><p>实现方式：AMD SR-IOV是硬件直通模型，性能开销极小。NVIDIA vGPU是API转发模型，有软件仲裁开销。</p><p>隔离性：两者都提供硬件辅助的强隔离。</p><p>灵活性：SR-IOV的VF划分规格是固定的，不够灵活。vGPU的配置文件和时间分片调度提供了更高的灵活性。</p><p>生态系统：NVIDIA vGPU的生态系统更成熟，与VMware、Citrix等主流虚拟化平台的集成更深入，商业支持也更完善。</p><h3 id=822-intel的gvt-g方案>8.2.2 Intel的GVT-g方案</h3><p>Intel在推广其集成显卡和独立数据中心GPU（如Ponte Vecchio, Gaudi）时，也推出了自己的GPU虚拟化技术，其中GVT-g（Graphics Virtualization Technology -g）是其代表作。</p><p>核心原理：GVT-g采用的是一种类似于NVIDIA vGPU的API转发/仲裁模型，但它是完全开源的，并深度集成在Linux内核（KVMGT）和QEMU中。</p><p>无Guest驱动：与vGPU不同，GVT-g的一个巧妙之处在于，它不需要在虚拟机内部安装任何特殊的驱动。虚拟机会加载一个标准的、开源的Intel i915图形驱动，这个驱动完全不知道自己运行在虚拟环境中。</p><p>Hypervisor仲裁：当Guest驱动提交图形或计算命令时，Hypervisor中的KVMGT模块会截获这些提交到底层硬件寄存器的操作，然后代表该VM，安全地将这些命令提交给物理GPU执行。</p><p>显存虚拟化：GVT-g通过影子页表（Shadow Page Table）技术，为每个VM虚拟化了GPU的地址空间（GGTT），实现了显存的隔离。</p><p>优点：</p><p>开源与免费：GVT-g是完全开源的，无需任何商业许可证。</p><p>兼容性好：无需特殊的Guest驱动，简化了部署。</p><p>技术先进：其设计思想（如无Guest驱动、影子页表）非常优雅。</p><p>缺点：</p><p>生态与成熟度：GVT-g主要围绕Intel自家的GPU产品，其在数据中心大规模AI计算领域的生态和市场认可度，与NVIDIA相比还有较大差距。</p><p>性能：作为一种软件仲裁方案，同样存在一定的性能开销。</p><h2 id=83-云厂商与开源社区基于容器的gpu虚拟化调度方案>8.3 云厂商与开源社区基于容器的GPU虚拟化调度方案</h2><p>在云原生的浪潮下，如何围绕容器实现比MIG更灵活、比时间分片隔离性更好、成本比vGPU更低的GPU共享方案，成为了各大云厂商和开源社区竞相追逐的“圣杯”。这些方案的核心思想，大多可以追溯到我们之前提到的API Remoting，但它们在实现上进行了大量的创新和优化。</p><p>核心思想：本地化的API劫持与资源管控</p><p>这些方案的通用架构模式如下：</p><h3 id=cuda-api劫持hooking>CUDA API劫持（Hooking）</h3><p>它们提供一个自定义的<code>libcuda.so</code>动态链接库。当用户的容器启动时，通过<code>LD_PRELOAD</code>环境变量，强制应用程序加载这个自定义的库，而不是系统原生的CUDA库。</p><p>这个自定义库会“劫持”所有应用程序发出的CUDA API调用。</p><h3 id=资源管理与调度中心daemon>资源管理与调度中心（Daemon）</h3><p>在每个GPU节点上，运行一个常驻的管理守护进程。</p><p>这个守护进程负责管理本节点上所有物理GPU的真实资源（算力、显存）。</p><p>它维护着每个容器被分配的虚拟GPU资源的“账本”（例如，容器A分配了2GB显存和20%算力）。</p><h3 id=本地ipc通信>本地IPC通信</h3><p>被劫持的API调用，并不通过网络转发，而是通过高效的本地进程间通信（IPC），如Unix Domain Socket，发送给同机上的管理守护进程。这避免了API Remoting的巨大网络开销。</p><h3 id=按需模拟与资源限制>按需模拟与资源限制</h3><p>显存管理：当守护进程收到一个<code>cudaMalloc</code>请求时，它会检查该容器的显存配额是否足够。如果足够，它才会在真实的物理GPU上分配显存，并建立一个从“虚拟显存地址”到“真实显存地址”的映射。它会向容器“谎报”GPU的总显存为其被分配的配额大小。</p><p>算力管理：对于计算内核的启动，守护进程会根据容器的算力配额，来控制其内核在物理GPU上的执行机会。这可以通过多种方式实现，例如：</p><p>控制内核并发度：限制该容器在同一时间最多能有多少个线程块（Blocks）在GPU上运行。</p><p>动态调整SM频率或功耗限制（需要硬件支持）。</p><p>更精细的时间分片：结合NVIDIA MPS或自己实现的调度器，精确控制其执行时间。</p><h3 id=831-阿里云-cgpu>8.3.1 阿里云 cGPU</h3><p>cGPU（container GPU）是阿里云容器服务团队推出的GPU共享虚拟化方案。</p><p>核心特性：</p><p>算力与显存解耦：用户可以独立地申请算力和显存。例如，可以申请一个只有1GB显存但需要50%算力的容器，或者一个需要10GB显存但只需要10%算力的容器。</p><p>显存隔离：通过API劫持，cGPU为每个容器虚拟了一个独立的显存空间，一个容器无法访问到另一个容器的显存。</p><p>算力隔离：通过控制CUDA Kernel的执行（类似于时间分片），实现了算力的隔离和限制。</p><p>与Kubernetes深度集成：提供了cGPU的Device Plugin，用户可以在Pod YAML中直接以<code>aliyun.com/gpu-mem: 1024</code>（单位MB）和<code>aliyun.com/gpu-core: 50</code>（单位%）的形式声明资源，调度器会根据节点的cGPU资源余量进行调度。</p><h3 id=832-腾讯云-qgpu>8.3.2 腾讯云 qGPU</h3><p>qGPU（GPU TKE-Ving）是腾讯云容器服务（TKE）推出的GPU虚拟化方案，其思想与cGPU类似，但在实现细节和商业化程度上有所不同。</p><p>核心特性：</p><p>同样实现了算力和显存的细粒度切分和隔离。</p><p>强调QoS保障，能够为不同优先级的任务提供有差异的服务质量。</p><p>与腾讯云自身的监控、计费、运维体系深度集成。</p><h3 id=833-开源社区方案以tke-vcudagpu-manager为例>8.3.3 开源社区方案：以TKE vCUDA+GPU Manager为例</h3><p>除了闭源的商业方案，开源社区也涌现了许多类似的项目，例如腾讯云早期开源的TKE GPU-Manager。</p><p>TKE vCUDA: 这是一个实现了本地API劫持的<code>libcuda.so</code>库。</p><p>GPU Manager: 这是一个与Kubernetes集成的调度和管理组件。</p><p>它允许用户在Pod的Annotation中声明GPU需求（如<code>tke.cloud.tencent.com/gpu-core-percentage: 30</code>, <code>tke.cloud.tencent.com/gpu-mem-percentage: 30</code>）。</p><p>一个自定义的调度器扩展（Scheduler Extender）会根据这些Annotation和节点的GPU资源使用情况，来决定Pod应该被调度到哪个节点。</p><p>节点上的GPU Manager会根据分配结果，为启动的容器设置<code>LD_PRELOAD</code>等环境变量，启用vCUDA的API劫持。</p><h3 id=834-这些方案的共性优点与挑战>8.3.4 这些方案的共性、优点与挑战</h3><p>共性：它们都采用了“应用无感、本地劫持、集中管控”的核心架构模式。对于用户的AI应用代码来说，完全不知道自己运行在虚拟化的GPU环境中。</p><p>优点：</p><p>极致的灵活性和利用率：实现了GPU资源的超细粒度切分和超卖，可以将GPU利用率从普遍的10%-30%提升到60%-80%甚至更高，极大降低了AI应用的单位算力成本。</p><p>算力与显存解耦：满足了各种奇形怪状的资源需求，适配了从开发、调试、推理到小规模训练的全场景。</p><p>易于集成：与云原生生态（Kubernetes, Docker）无缝集成，提供了良好的用户体验。</p><p>挑战与权衡：</p><p>性能开销：API劫持和IPC通信虽然比网络转发快得多，但仍然会引入一定的性能开Guthaben，特别是对于那些大量、频繁调用小API的应用。对于需要极致性能的大规模分布式训练，其性能可能不如MIG或物理卡。</p><p>兼容性与维护成本：最大的挑战来自于对NVIDIA驱动和CUDA版本的追随。每当NVIDIA发布新的驱动，这些方案的<code>libcuda.so</code>劫持库可能都需要进行适配和严格的测试，以确保所有CUDA API都能被正确地模拟和处理。这是一个巨大的、持续的工程投入。</p><p>隔离性的强度：这种软件层面的隔离，其强度和安全性理论上不如MIG或vGPU这样的硬件/Hypervisor级隔离。虽然已经可以防止大部分常规错误，但在最严格的安全要求下，仍然存在潜在的风险。</p><p>功能完整性：某些非常底层或未公开的CUDA功能可能难以被完美模拟，导致一些特殊的应用无法运行。</p><h2 id=84-本章小结>8.4 本章小结</h2><p>在本章中，我们对GPU虚拟化调度方案进行了一次全景式的、深入的考察，将我们对GPU资源共享的理解，从第七章的“板卡级分配”提升到了“精细化虚拟化”的更高层次。</p><p>我们梳理了NVIDIA官方的虚拟化技术“全家桶”，明确了它们各自的定位：</p><p>MIG以其无与伦比的硬件强隔离和性能可预测性，成为云原生多租户场景下安全与性能的基石。</p><p>GRID vGPU凭借其在VM环境中的成熟生态、灵活的时间分片调度和完善的功能，稳坐企业级虚拟化市场的头把交椅。</p><p>API Remoting作为一种基础思想，为后来的许多创新方案奠定了理论基础。</p><p>我们还横向对比了其他硬件厂商的方案，如AMD的SR-IOV和Intel的GVT-g。我们看到，它们分别代表了硬件直通和开源仲裁这两条不同的技术路线，虽然在生态上不及NVIDIA，但其技术思路同样具有重要的参考价值。</p><p>本章的重中之重，是我们对以阿里云cGPU、腾讯云qGPU为代表的、兴起于云原生社区的容器GPU虚拟化方案的深入剖析。我们揭示了它们共同的、巧妙的核心架构：通过本地化的CUDA API劫持和集中的资源管理守护进程，它们成功地在保持应用无感的同时，实现了对GPU算力和显存的超细粒度切分、隔离与池化。这些方案以其极致的灵活性和资源利用率，完美地契合了云原生时代对弹性和成本效益的追求，代表了当前GPU虚拟化领域最活跃、最具创新性的发展方向，尽管它们也面临着性能开销、兼容性维护和隔离强度等方面的持续挑战。</p><p>最终，我们得出一个结论：不存在任何一种“银弹”式的GPU虚拟化方案。 这是一个充满了权衡的决策空间。架构师在选择方案时，必须像一位经验丰富的主厨，根据眼前的“食材”（业务场景）和需要达成的“口味”（核心诉求），来精心调配各种“调料”（技术方案）：</p><p>若安全隔离是第一要务，选择MIG或vGPU。</p><p>若极致的资源利用率和成本效益是首要目标，选择cGPU/qGPU这类方案。</p><p>若追求无损的裸金属性能，且可以接受独占，选择GPU直通或简单的独占式容器调度。</p><p>对这幅GPU虚拟化技术图谱的深刻理解，将使我们有能力为我们的AI平台构建一个既强大又经济、既稳定又灵活的算力底座，从而在激烈的AI军备竞赛中，获得关键的“成本优势”和“敏捷优势”。</p></div><div class=post_footer><div class=meta><div class=info><span class="field tags"><i class=ri-stack-line></i>
<a href=https://zhurongshuo.com/tags/%E4%B9%A6%E7%A8%BF/>书稿</a></span></div></div></div></div><div class=doc_comments></div></div></div></div><a id=back_to_top href=# class=back_to_top><i class=ri-arrow-up-s-line></i></a><footer class=footer><div class=powered_by><a href=https://varkai.com>Designed by VarKai, </a><a href=http://www.gohugo.io/>Proudly published with Hugo,</a></div><div class=footer_slogan><span>法不净空，觉无性也。</span></div><div class=powered_by style=margin-top:10px;font-size:14px><a href=https://zhurongshuo.com/>Copyright © 2010-2025 祝融说 zhurongshuo.com All Rights Reserved.</a></div></footer><script defer src=https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><link href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.css rel=stylesheet integrity="sha256-7qiTu3a8qjjWtcX9w+f2ulVUZSUdCZFEK62eRlmLmCE=" crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin=anonymous></script><script defer src=https://zhurongshuo.com/js/zozo.js></script><script type=text/javascript async src="https://cdn.jsdelivr.net/npm/mathjax@2.7.8/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["[[","]]"]],processEscapes:!0,processEnvironments:!0,skipTags:["script","noscript","style","textarea","pre"],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}}),MathJax.Hub.Queue(function(){var e,t=MathJax.Hub.getAllJax();for(e=0;e<t.length;e+=1)t[e].SourceElement().parentNode.className+=" has-jax"})</script><style>code.has-jax{font:inherit;font-size:100%;background:inherit;border:inherit;color:#515151}</style><script>window.GA_MEASUREMENT_ID="G-KKJ5ZEG1NB",window.GA_CONFIG={enableReadingTime:!0,enableScrollDepth:!0,enableOutboundLinks:!0,enableDownloads:!0,lazyLoadTimeout:3e3}</script><script defer src=https://zhurongshuo.com/js/ga-optimizer.js></script></body></html>