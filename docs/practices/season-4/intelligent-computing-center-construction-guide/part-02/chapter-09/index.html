<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><meta name=google-site-verification content="8_xpI-TS3tNV8UPug-Q6Ef3BhKTcy0WOG7dEdAcm2zk"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="祝融"><link rel=dns-prefetch href=//cdn.jsdelivr.net><link rel=preconnect href=https://cdn.jsdelivr.net crossorigin><link rel=dns-prefetch href=//www.googletagmanager.com><link rel=preconnect href=https://www.googletagmanager.com crossorigin><link rel=canonical href=https://zhurongshuo.com/practices/season-4/intelligent-computing-center-construction-guide/part-02/chapter-09/><title>祝融说。 第9章 GPU集群的网络虚拟化设计与实现</title><meta property="og:title" content="第9章 GPU集群的网络虚拟化设计与实现"><meta property="og:url" content="https://zhurongshuo.com/practices/season-4/intelligent-computing-center-construction-guide/part-02/chapter-09/"><meta property="og:site_name" content="祝融说。"><meta property="og:locale" content="zh_CN"><meta property="og:type" content="article"><meta property="article:published_time" content="2025-12-07T00:00:00+08:00"><meta property="article:modified_time" content="2025-12-07T00:00:00+08:00"><meta property="article:tag" content="书稿"><meta name=description content="在第六章，我们专注于构建GPU集群的物理网络，如同规划了一座拥有高速公路（计算网络）、主干道（存储网络）和市政道路（业务网络）的现代化城市。这个物理网络是所有数据流动的坚实基础。然而，在一个多用户、多任务、多租户的真实云环境中，仅仅拥有一个共享的物理网络是远远不够的。
"><meta property="og:description" content="在第六章，我们专注于构建GPU集群的物理网络，如同规划了一座拥有高速公路（计算网络）、主干道（存储网络）和市政道路（业务网络）的现代化城市。这个物理网络是所有数据流动的坚实基础。然而，在一个多用户、多任务、多租户的真实云环境中，仅仅拥有一个共享的物理网络是远远不够的。
"><meta property="og:image" content="https://zhurongshuo.com/images/favicon.ico"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="第9章 GPU集群的网络虚拟化设计与实现"><meta name=twitter:description content="在第六章，我们专注于构建GPU集群的物理网络，如同规划了一座拥有高速公路（计算网络）、主干道（存储网络）和市政道路（业务网络）的现代化城市。这个物理网络是所有数据流动的坚实基础。然而，在一个多用户、多任务、多租户的真实云环境中，仅仅拥有一个共享的物理网络是远远不够的。
"><meta name=twitter:image content="https://zhurongshuo.com/images/favicon.ico"><meta name=keywords content="智算中心建设指南：大模型算力的基础架构,第9章 GPU集群的网络虚拟化设计与实现"><link rel="shortcut icon" href=https://zhurongshuo.com/images/favicon.ico><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/normalize.css><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/animate-custom.css><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/zozo.css><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/remixicon-custom.css><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/highlight.css><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","headline":"第9章 GPU集群的网络虚拟化设计与实现","description":"在第六章，我们专注于构建GPU集群的物理网络，如同规划了一座拥有高速公路（计算网络）、主干道（存储网络）和市政道路（业务网络）的现代化城市。这个物理网络是所有数据流动的坚实基础。然而，在一个多用户、多任务、多租户的真实云环境中，仅仅拥有一个共享的物理网络是远远不够的。\n","datePublished":"2025-12-07T00:00:00\u002b08:00","dateModified":"2025-12-07T00:00:00\u002b08:00","author":{"@type":"Person","name":"祝融"},"publisher":{"@type":"Organization","name":"祝融说。","logo":{"@type":"ImageObject","url":"https:\/\/zhurongshuo.com\/images\/favicon.ico"}},"image":"https:\/\/zhurongshuo.com\/images\/favicon.ico","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/zhurongshuo.com\/practices\/season-4\/intelligent-computing-center-construction-guide\/part-02\/chapter-09\/"},"keywords":"书稿"}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"首页","item":"https:\/\/zhurongshuo.com\/"},{"@type":"ListItem","position":2,"name":"practices","item":"https:\/\/zhurongshuo.com\/practices/"},{"@type":"ListItem","position":3,"name":"第9章 GPU集群的网络虚拟化设计与实现","item":"https:\/\/zhurongshuo.com\/practices\/season-4\/intelligent-computing-center-construction-guide\/part-02\/chapter-09\/"}]}</script></head><body><div class="main animate__animated animate__fadeInDown"><div class="nav_container animated fadeInDown"><div class=site_nav id=site_nav><ul><li><a href=https://zhurongshuo.com/>首页</a></li><li><a href=https://zhurongshuo.com/start/>开始</a></li><li><a href=https://zhurongshuo.com/advanced/>进阶rc</a></li><li><a href=https://zhurongshuo.com/posts/>归档</a></li><li><a href=https://zhurongshuo.com/tags/>标签</a></li><li><a href=https://zhurongshuo.com/about/>关于</a></li></ul></div><div class=menu_icon><a id=menu_icon><i class=ri-menu-line></i></a></div></div><div class="header animated fadeInDown"><div class=site_title_container><div class=site_title><h1><a href=https://zhurongshuo.com/><span class=web-font>祝融说。</span></a></h1></div><div class=description><p class=sub_title>法不净空，觉无性也。</p><div class=my_socials><a href=https://zhurongshuo.com/books/ title=book-open><i class=ri-book-open-line></i></a>
<a href=https://zhurongshuo.com/practices/ title=trophy><i class=ri-trophy-line></i></a>
<a href=https://zhurongshuo.com/gallery/ title=gallery><i class=ri-gallery-line></i></a>
<a href=https://zhurongshuo.com/about/ title=game><i class=ri-game-line></i></a>
<a href type=application/rss+xml title=rss target=_blank><i class=ri-rss-fill></i></a></div></div></div></div><div class=content><div class=post_page><div class="post animate__animated animate__fadeInDown"><div class="post_title post_detail_title"><h2><a href=https://zhurongshuo.com/practices/season-4/intelligent-computing-center-construction-guide/part-02/chapter-09/>第9章 GPU集群的网络虚拟化设计与实现</a></h2><span class=date>2025.12.07</span></div><div class="post_content markdown"><p>在第六章，我们专注于构建GPU集群的物理网络，如同规划了一座拥有高速公路（计算网络）、主干道（存储网络）和市政道路（业务网络）的现代化城市。这个物理网络是所有数据流动的坚实基础。然而，在一个多用户、多任务、多租户的真实云环境中，仅仅拥有一个共享的物理网络是远远不够的。</p><p>想象一下，如果将所有用户的“车辆”（数据包）都放任在同一个巨大的、扁平的物理交通网络上行驶，会产生什么问题？</p><p>地址冲突：不同的用户或应用可能希望使用相同的IP地址段（例如，都喜欢用<code>192.168.1.0/24</code>），这将导致灾难性的地址冲突。</p><p>安全隔离缺失：任何一个用户都可以轻易地“看到”甚至“攻击”另一个用户的数据包，毫无隐私和安全可言。</p><p>广播风暴：一个用户的广播或多播流量会扩散到整个物理网络，影响所有用户。</p><p>路由策略僵化：物理网络的路由策略是统一的，无法为不同的用户或应用提供定制化的、灵活的路由规则。</p><p>为了解决这些问题，我们必须在物理网络之上，构建一个虚拟化的网络层。本章，我们将深入探讨GPU集群的网络虚拟化设计与实现。我们的核心目标是，学习如何将一个共享的物理网络基础设施，通过软件定义的技术，虚拟成成百上千个独立的、逻辑上完全隔离的私有网络。每个私有网络都如同一个租户专属的“城中城”，拥有自己独立的地址空间、路由表、安全策略和网络服务。</p><p>我们将从网络虚拟化的基石技术——基于SDN的VPC——入手，理解它是如何实现网络资源的池化与隔离的。接着，我们将聚焦于机器学习网络中不可或缺的关键组件——云负载均衡，探讨它如何为AI推理服务提供高可用和可扩展性。然后，我们会学习如何通过专线接入、对等连接与VPC网关等技术，打通虚拟网络与外部世界（如用户的数据中心、其他VPC、公网）的安全连接。最后，我们将深入到底层，剖析SDN NFV网关这一实现各种高级网络功能的核心引擎，是如何被部署和加速的。</p><p>通过本章的学习，你将掌握一套完整的云原生网络虚拟化架构知识。你将不再仅仅看到物理的交换机和路由器，而是能够以云架构师的视角，去理解和设计VPC、子网、安全组、负载均衡器、NAT网关等高级网络服务。这将使你能够为一个多租户的、安全的、功能丰富的GPU云平台，构建一个既灵活又强大的虚拟网络基础设施。</p><h2 id=91-基于sdn的vpc技术网络虚拟化技术的基石>9.1 基于SDN的VPC技术：网络虚拟化技术的基石</h2><p>VPC（Virtual Private Cloud，虚拟私有云）是现代云计算中网络虚拟化的核心概念和事实标准。它允许用户在一个共享的公有云基础设施中，构建一个逻辑上完全隔离的、由用户自己掌控的私有网络环境。而实现VPC的底层技术，正是SDN（Software-Defined Networking，软件定义网络）。</p><h3 id=911-sdn的核心思想控制与转发分离>9.1.1 SDN的核心思想：控制与转发分离</h3><p>在传统的网络设备（如交换机、路由器）中，控制平面（Control Plane）和数据平面（Data Plane）是紧密耦合在同一台设备中的。</p><p>控制平面：负责“思考”和“决策”。它运行着各种路由协议（如OSPF, BGP），学习网络拓扑，计算路由表，生成转发表。</p><p>数据平面：负责“执行”。它利用专门的ASIC芯片，根据控制平面生成的转发表，对流经的数据包进行高速的查询和转发。</p><p>SDN的核心思想就是将这两个平面进行解耦：</p><ol><li>数据平面（基础设施层）：底层的物理交换机被“简化”成只负责高速转发的“傻瓜”设备（有时被称为“白盒交换机”）。它们不再运行复杂的路由协议。</li><li>控制平面（SDN控制器）：一个集中的、运行在服务器上的SDN控制器（SDN Controller）成为了整个网络的“大脑”。它拥有全局的网络拓扑视图，负责所有的路由计算、策略制定和流量工程。</li><li>南向接口（Southbound Interface）：SDN控制器通过一个标准的协议（如OpenFlow, OVSDB）与底层的所有物理交换机进行通信。控制器将计算好的转发表规则（Flow Table）下发到交换机中。</li><li>北向接口（Northbound Interface）：SDN控制器向上层应用（如VPC管理平台、云编排系统）提供RESTful API等编程接口，使得网络可以被程序化地、自动化地管理和配置。</li></ol><h3 id=912-overlay网络构建虚拟世界的隧道>9.1.2 Overlay网络：构建虚拟世界的“隧道”</h3><p>仅仅分离控制和转发还不足以实现VPC的隔离。SDN通常会结合Overlay网络技术来构建虚拟网络。</p><p>Underlay网络：指我们底层的物理网络基础设施（Spine-Leaf架构的交换机和路由器）。Underlay网络的目标很简单：实现IP包在物理设备之间的可达性。</p><p>Overlay网络：在Underlay网络之上，通过隧道技术（Tunneling）构建的一个虚拟的、逻辑上的网络。</p><p>工作原理：当一个虚拟机（租户A）要发送一个数据包给同VPC但在不同物理机上的另一个虚拟机时，这个原始的数据包（内部包）首先被发送到宿主机上的一个虚拟交换机（vSwitch）。</p><p>vSwitch并不会直接将这个内部包发送到物理网络，而是将其作为“货物”，封装在一个新的IP包（外部包）的数据部分。这个外部包的源IP是源宿主机的物理IP，目的IP是目的宿主机的物理IP。这个封装的过程，就如同建立了一条“隧道”。</p><p>这个外部包在Underlay物理网络中被正常地路由和转发，直到到达目的宿主机。</p><p>目的宿主机的vSwitch接收到这个外部包后，将其“解封”，取出内部的原始数据包，再投递给目标虚拟机。</p><p>隧道协议：实现Overlay网络的隧道协议有很多种，最主流的是VXLAN（Virtual eXtensible LAN）和Geneve。</p><p>VXLAN：将原始的以太网帧封装在UDP包中。它引入了一个24位的VNI（VXLAN Network Identifier）字段，用来唯一标识一个虚拟网络。所有VNI相同的VXLAN包，都属于同一个VPC。24位的VNI理论上可以支持多达1600万个独立的虚拟网络，远远超过了传统VLAN（只有4096个）的限制。</p><h3 id=913-基于sdn的vpc实现流程>9.1.3 基于SDN的VPC实现流程</h3><p>现在，我们将SDN和VXLAN结合起来，看看一个典型的VPC是如何工作的：</p><ol><li>用户创建VPC：用户通过云平台的界面或API，创建一个VPC，并为其定义一个私有的IP地址段（CIDR），例如<code>10.0.0.0/16</code>。</li><li>控制器分配VNI：SDN控制器收到这个请求后，会为这个新的VPC分配一个全局唯一的VNI，例如<code>1001</code>。</li><li>用户创建子网和虚拟机：用户在VPC内部创建子网（如<code>10.0.1.0/24</code>），并在子网中启动一台虚拟机VM-A。</li><li>控制器下发规则：<ol><li>SDN控制器知道了VM-A的虚拟IP是<code>10.0.1.10</code>，MAC地址是<code>mac-a</code>，它所在的物理主机的IP是<code>1.1.1.1</code>。</li><li>控制器会将这些信息（<code>mac-a -> 1.1.1.1</code>的映射关系）下发给网络中的所有vSwitch。</li></ol></li><li>VPC内部通信：<ol><li>当同一个VPC内的VM-A (<code>10.0.1.10</code>) 要与另一台物理主机上的VM-B (<code>10.0.2.20</code>) 通信时，VM-A发出一个目标IP为<code>10.0.2.20</code>的包。</li><li>VM-A所在的vSwitch截获这个包。它查询SDN控制器下发的转发表，发现<code>10.0.2.20</code>对应的物理主机是<code>1.1.1.2</code>，并且它们同属于VNI <code>1001</code>。</li><li>vSwitch将VM-A的原始包用VNI <code>1001</code>进行VXLAN封装，然后封装在一个新的IP包中，源IP为<code>1.1.1.1</code>，目的IP为<code>1.1.1.2</code>。</li><li>这个封装后的包在物理网络中被转发到主机<code>1.1.1.2</code>。</li><li>主机<code>1.1.1.2</code>上的vSwitch解封，将原始包投递给VM-B。</li></ol></li><li>VPC隔离：如果VM-A试图访问一个不属于VNI <code>1001</code>的IP地址，vSwitch在查询转发表时会发现没有匹配的规则，或者规则指示该流量应被丢弃，从而实现了VPC之间的隔离。</li></ol><h3 id=914-vpc中的安全组security-group>9.1.4 VPC中的安全组（Security Group）</h3><p>概念：安全组是一种分布式的、有状态的虚拟防火墙，它作用于虚拟机（或容器）的网卡层面。它允许用户定义一组入向（Inbound）和出向（Outbound）的访问控制规则。</p><p>实现：安全组的规则同样是由SDN控制器统一下发，并在每个宿主机的vSwitch（或使用Linux内核的iptables/nftables/eBPF）上强制执行的。由于它是在数据包进入/离开虚拟机的第一跳就进行过滤，因此效率非常高，并且可以实现同一子网内不同虚拟机之间的隔离（而传统的网络ACL通常只能在子网边界进行控制）。</p><p>通过SDN和Overlay技术，我们成功地将一个共享的物理网络，虚拟化成了多个独立的、安全的、功能丰富的VPC。这是构建多租户GPU云平台的网络基础。</p><h2 id=92-云负载均衡机器学习网络的中流砥柱>9.2 云负载均衡：机器学习网络的中流砥柱</h2><p>在GPU集群中，除了用于训练的原始算力，一个非常重要的应用场景是提供AI推理服务（Inference）。例如，将一个训练好的图像识别模型或语言模型，部署成一个在线API服务，供外部应用调用。</p><p>这些推理服务通常需要满足高可用（High Availability）和高可扩展性（High Scalability）的要求。我们不可能只部署一个服务实例，因为它可能随时宕机，也无法应对高并发的请求。因此，我们通常会部署多个相同的服务实例，然后通过一个负载均衡器（Load Balancer, LB）来将外部的请求流量分发给这些后端的实例。</p><p>云负载均衡器是VPC网络中一个核心的网络服务组件。</p><h3 id=921-负载均衡器的角色和类型>9.2.1 负载均衡器的角色和类型</h3><p>核心功能：</p><ol><li>流量分发：接收外部请求，并根据一定的调度算法（如轮询、最少连接、源IP哈希），将请求转发给一个健康的后端服务实例。</li><li>健康检查（Health Check）：定期地、主动地向后端的每个服务实例发送“心跳”探测包（如一个HTTP GET请求或一个TCP SYN包）。如果某个实例在规定时间内没有正确响应，负载均衡器会将其标记为“不健康”，并暂时停止向其转发新的流量。</li><li>高可用：当某个后端实例故障时，健康检查机制能自动发现并将其从服务池中摘除，保证了整体服务的可用性。</li><li>水平扩展：当请求量增加时，我们只需向后端服务池中添加更多的服务实例，负载均衡器会自动将流量分发给它们，实现了服务的弹性伸缩。</li><li>提供单一入口：负载均衡器为整个后端服务集群提供一个统一的、固定的虚拟IP地址（VIP）作为服务入口，对客户端屏蔽了后端的复杂性。</li></ol><p>根据工作层级划分的类型：</p><ol><li>四层负载均衡（L4 LB）：</li></ol><p>工作在OSI模型的传输层（TCP/UDP）。</p><p>它根据数据包的源IP、源端口、目的IP、目的端口这四个元信息来进行转发决策。它不关心数据包的应用层内容（如HTTP头或URL）。</p><p>优点：性能极高，因为处理逻辑简单，可以由专门的硬件或高效的内核代码实现。</p><p>缺点：灵活性差，无法根据应用层信息进行精细的流量调度。</p><ol start=2><li>七层负载均衡（L7 LB）：</li></ol><p>工作在应用层（如HTTP/HTTPS）。</p><p>它能够解析应用层协议的内容。例如，一个HTTP负载均衡器可以根据请求的URL路径、域名、Cookie、HTTP头等信息，来决定将请求转发给哪个后端服务集群。例如，<code>api.example.com/images</code>的请求转发给图像处理服务，<code>api.example.com/text</code>的请求转发给文本处理服务。</p><p>优点：极其灵活，可以实现复杂的路由和业务逻辑。还可以提供SSL卸载、内容缓存、请求重写等高级功能。</p><p>缺点：性能开销较大，因为它需要对每个数据包进行深度解析和处理。</p><h3 id=922-四层负载均衡的实现技术lvsdr>9.2.2 四层负载均衡的实现技术：LVS/DR</h3><p>在Linux世界中，LVS（Linux Virtual Server）是实现高性能四层负载均衡的经典开源项目。其最常用、性能最高的模式是直接路由（Direct Routing, DR）模式。</p><p>LVS/DR的工作原理：</p><ol><li>统一的VIP：负载均衡器（Director）和所有的后端真实服务器（Real Server, RS）都配置了同一个虚拟IP地址（VIP），但只有Director将这个VIP宣告给外部网络。</li><li>入向流量（请求）：客户端向VIP发送一个请求包。Director收到这个包后，并不修改其IP头，而是仅仅修改其二层MAC地址，将其目标MAC地址改为选中的某个后端RS的MAC地址。然后，Director将这个“改了脸”的包直接扔到与RS所在的同一个二层网络中。</li><li>出向流量（响应）：后端RS收到这个包后，发现其目标IP是自己配置的VIP，于是处理这个请求。处理完毕后，RS不经过Director，而是直接将响应包发回给客户端。因为响应包的源IP是VIP，客户端可以正常接收。</li></ol><p>LVS/DR的优势：</p><p>极致的性能：Director只处理入向的请求流量，且只做了一次轻量级的MAC地址修改。出向的、通常数据量更大的响应流量直接由RS返回，完全不占用Director的资源。这使得LVS/DR的吞吐能力可以非常高。</p><p>在云环境中的实现：</p><p>云厂商的四层负载均衡（在AWS中叫NLB，在阿里云叫CLB）其底层大多借鉴了LVS/DR的思想，但通过与SDN控制器和vSwitch的深度结合，实现了更高级的功能。</p><p>vSwitch可以扮演LVS/DR中Director的角色，对入向的流量进行DNAT（修改目的IP为某个RS的私有IP），然后在RS响应时，再进行SNAT（修改源IP为VIP），从而实现类似的效果，但对后端RS的配置要求更低。</p><h3 id=923-七层负载均衡的实现技术>9.2.3 七层负载均衡的实现技术</h3><p>七层负载均衡本质上是一个反向代理（Reverse Proxy）。</p><p>工作原理：</p><ol><li>L7 LB与客户端建立一个完整的TCP连接和应用层会话（如HTTP）。</li><li>它接收并完整地解析客户端的请求。</li><li>根据其配置的路由规则，它再作为客户端，与选中的后端RS建立另一个全新的TCP连接和应用层会话。</li><li>它将原始请求（或经过修改的请求）发送给RS。</li><li>RS处理完毕，将响应返回给L7 LB。</li><li>L7 LB再将响应通过第一个连接，返回给原始客户端。</li></ol><p>常见的实现：Nginx, HAProxy, Envoy等都是非常优秀的开源七层负载均衡/反向代理软件。云厂商的七层负载均衡器（如AWS的ALB, 阿里云的ALB）通常也是基于这些开源项目或自研的类似技术构建的。</p><p>在Kubernetes中的集成：Ingress</p><p>Kubernetes通过Ingress资源对象和Ingress Controller来标准化七层负载均衡的用法。</p><p>用户创建一个Ingress对象，在其中定义基于主机名和URL路径的路由规则。</p><p>集群中运行的Ingress Controller（例如<code>nginx-ingress-controller</code>）会监听这些Ingress对象，并自动地将这些规则转换成其后端代理软件（如Nginx）的配置文件，然后动态地应用这些配置。</p><h3 id=924-ai推理服务的负载均衡选择>9.2.4 AI推理服务的负载均衡选择</h3><p>对于那些对性能要求极高、协议简单的内部服务间调用，可以考虑使用四层负载均衡。</p><p>对于需要对外提供HTTP/HTTPS API、需要根据URL进行灵活路由、或需要SSL卸载等高级功能的在线推理服务，七层负载均衡是更合适的选择。在Kubernetes环境中，使用Ingress是标准的做法。</p><h2 id=93-专线接入对等连接与vpc网关>9.3 专线接入、对等连接与VPC网关</h2><p>一个孤立的VPC是没有价值的。我们必须打通VPC与外部其他网络之间的连接，才能构建一个完整的混合云或多云架构。VPC网关是实现这些连接的关键组件。</p><h3 id=931-场景一连接用户本地数据中心idc>9.3.1 场景一：连接用户本地数据中心（IDC）</h3><p>企业通常希望将其在云上的GPU集群，与自己的本地数据中心（IDC）安全、稳定地连接起来，以实现：</p><p>在IDC和云上VPC之间迁移数据。</p><p>云上的应用访问IDC中的数据库或服务。</p><p>IDC中的员工安全地访问云上的开发环境。</p><p>有两种主要的连接方式：</p><p>VPN网关（VPN Gateway） + IPsec VPN：</p><p>原理：在VPC中创建一个VPN网关，在用户IDC的出口路由器或防火墙上配置VPN。两者之间通过公网建立一条或多条IPsec VPN隧道。所有在隧道中传输的数据都会被加密，保证了安全性。</p><p>优点：配置相对简单、快速，成本低廉（因为走的是公网）。</p><p>缺点：性能和稳定性受制于公网的质量，带宽有限、延迟较高且不稳定，不适合大规模、持续性的数据传输。</p><p>专线接入（Direct Connect / ExpressRoute）+ 专线网关（Direct Connect Gateway）：</p><p>原理：用户通过电信运营商，租用一条物理专线，一端连接到自己的IDC，另一端连接到云服务商指定的接入点（Access Point）。然后，在VPC中创建一个专线网关，通过这个接入点，将VPC与用户的物理专线连接起来。</p><p>优点：提供了私有的、独占的物理连接。带宽高（可达10Gbps甚至100Gbps）、延迟极低且稳定，安全性最高。</p><p>缺点：成本高昂，开通周期长。</p><p>选择：对于生产环境、需要稳定高性能混合云连接的企业级GPU集群，专线接入是必然选择。VPN可以作为一种临时的、或用于管理目的的备份链路。</p><h3 id=932-场景二连接同一个云上的其他vpc>9.3.2 场景二：连接同一个云上的其他VPC</h3><p>在一个大型组织中，不同的部门或项目可能会创建多个不同的VPC。有时，这些VPC之间需要相互通信。</p><p>VPC对等连接（VPC Peering Connection）：</p><p>原理：这是连接两个VPC最直接的方式。用户在两个VPC之间创建一个“对等连接”请求，待双方确认后，云平台的SDN控制器会自动在这两个VPC的路由器之间建立一条私有的、高带宽的连接。两个VPC可以像在同一个网络中一样，使用私有IP地址直接通信。</p><p>优点：性能好，延迟低，因为流量完全在云服务商的骨干网内部传输。</p><p>缺点：对等连接是点对点的，且不具有传递性。如果VPC A与VPC B对等，VPC B与VPC C对等，VPC A和VPC C之间并不能自动通信，需要再单独建立一个对等连接。当VPC数量很多时，会形成一个复杂的“网状”拓扑，管理困难。</p><p>云企业网（Transit Gateway / Cloud Enterprise Network）：</p><p>原理：为了解决对等连接的“网状”问题，云厂商推出了云企业网服务。你可以创建一个云企业网关（Transit Gateway），然后将你的所有VPC和专线网关都“连接”到这个云企业网关上。</p><p>云企业网关如同一个云上的路由器或集线器（Hub）。任何连接到它的网络实例，都可以与其他任何连接到它的实例进行通信，形成了一个星型（Hub-and-Spoke）拓扑。</p><p>优点：极大地简化了多VPC、多地域、混合云环境下的网络管理。扩展性好，路由策略可以集中配置。</p><h3 id=933-场景三连接公共互联网>9.3.3 场景三：连接公共互联网</h3><p>VPC中的虚拟机或容器需要访问互联网，或者对外提供服务。</p><p>NAT网关（NAT Gateway）：</p><p>功能：为VPC内没有公网IP的私有子网中的实例，提供访问互联网（出向）的能力。当这些实例访问公网时，其流量会被路由到NAT网关，NAT网关会将其源私有IP地址转换为自己的公网IP地址（SNAT），然后再发送出去。</p><p>它是一个高可用、高吞吐量的托管服务，解决了传统自建NAT主机的性能和单点故障问题。</p><p>互联网网关（Internet Gateway, IGW）：</p><p>功能：它是VPC与互联网之间的“大门”。将一个IGW附加到VPC上，并配置一条指向IGW的默认路由（<code>0.0.0.0/0</code>），就可以让拥有公网IP的子网中的实例与互联网进行双向通信。</p><p>负载均衡器、NAT网关等需要与公网交互的服务，都依赖于IGW的存在。</p><h2 id=94-sdn-nfv网关的实现与部署>9.4 SDN NFV网关的实现与部署</h2><p>我们在前面讨论的所有VPC网关（VPN网关、NAT网关、专线网关等），以及负载均衡器、防火墙等，在现代云环境中，都越来越多地通过NFV（Network Function Virtualization，网络功能虚拟化）技术，以虚拟化网络功能（Virtual Network Function, VNF）的形式来实现。</p><p>NFV的思想是将传统的、运行在专用硬件盒子里的网络功能（如路由、防火墙、负载均衡），用软件的形式实现，并让它们运行在通用的、标准化的x86服务器上。而当NFV与SDN结合时，这些VNF就成为了SDN网络中可被集中编排和管理的“软件插件”。</p><p>一个SDN NFV网关节点，就是一台或多台专门用于运行这些VNF的高性能服务器。</p><h3 id=941-基于virtio-netvhost的虚拟机部署nfv>9.4.1 基于virtio-net/vhost的虚拟机部署NFV</h3><p>这是传统NFV的一种实现方式，VNF被部署在虚拟机中。</p><p>virtio-net：这是为KVM虚拟机设计的一套半虚拟化（Paravirtualized）的网络设备标准。</p><p>前端驱动（Frontend）：运行在Guest OS（虚拟机）内部，是一个轻量级的网络驱动。</p><p>后端驱动（Backend）：运行在Host OS（宿主机）中，通常由QEMU来实现。</p><p>工作原理：前端驱动将数据包放入一个与后端共享的内存环形缓冲区（virtqueue）中，然后通知后端来取。相比模拟一个真实的物理网卡（如e1000），virtio避免了大量的VM-exit/VM-entry（虚拟机陷入/退出）开销，性能好得多。</p><p>vhost-net：进一步的优化。为了避免QEMU这个用户态进程成为瓶颈，vhost-net将virtio的后端驱动直接实现在了宿主机的内核中。这样，Guest OS可以直接与Host内核交换数据，路径更短，性能更高。</p><p>优点：提供了良好的隔离性（VM级别），技术成熟稳定。</p><p>缺点：</p><p>性能瓶颈：即使有vhost-net，数据包仍然需要在Guest内核和Host内核之间进行多次内存拷贝和上下文切换，难以满足100Gbps级别的高吞吐量需求。</p><p>CPU开销大：Host和Guest的CPU都需要深度参与数据包处理。</p><h3 id=942-基于sr-iov的虚拟机部署nfv>9.4.2 基于SR-IOV的虚拟机部署NFV</h3><p>为了突破virtio的性能瓶颈，可以将SR-IOV技术应用于NFV网关节点。</p><p>原理：</p><p>网关服务器使用支持SR-IOV的智能网卡。</p><p>网卡被配置为创建多个虚拟功能（VF）。</p><p>每个VF被直接“直通”给一个运行VNF的虚拟机。</p><p>优点：</p><p>接近裸金属的性能：VNF虚拟机可以直接、独占地访问网卡的硬件资源，完全绕过了宿主机的内核和vSwitch，延迟极低，吞吐量极高。</p><p>缺点：</p><p>灵活性差：失去了虚拟化的很多灵活性。例如，虚拟机的实时迁移变得困难。</p><p>功能受限：vSwitch提供的很多高级功能（如安全组、流量镜像）无法直接应用在这些直通的VF上。</p><h3 id=943-使用dpdk技术对nfv加速>9.4.3 使用DPDK技术对NFV加速</h3><p>无论是基于virtio还是SR-IOV，要榨干硬件的最后一滴性能，都需要DPDK（Data Plane Development Kit）的加持。</p><p>DPDK在NFV中的应用：在运行VNF的虚拟机或容器内部，使用DPDK来接管虚拟网卡（virtio-net VF或SR-IOV VF）或物理网卡。</p><p>核心技术：</p><p>内核旁路（Kernel Bypass）：DPDK应用直接在用户态通过轮询（Polling）模式读写网卡队列，完全绕过了内核网络栈的中断、系统调用和上下文切换开销。</p><p>大页内存（Huge Pages）：使用2MB或1GB的大页，减少TLB Miss，提升内存访问性能。</p><p>CPU亲和性：将DPDK的轮询线程绑定到特定的CPU核心上，避免线程在不同核心间迁移。</p><p>效果：使用DPDK，一个普通的x86服务器核心，其网络包转发能力可以提升一个数量级，从百万PPS（包每秒）级别提升到千万PPS级别，这使得用纯软件实现100Gbps线速的VNF成为可能。</p><p>SDN NFV网关的现代实现：</p><p>现代的云网络网关，通常是一个集大成者。它可能会运行在裸金属服务器上，使用DPDK来获得极致的数据平面性能。它会与SDN控制器紧密集成，接收控制器下发的转发表和策略。同时，它可能会利用DPU/智能网卡，将一部分最耗费CPU的转发任务（如VXLAN封装/解封、连接跟踪）硬件卸载掉，从而实现性能、灵活性和成本的最佳平衡。</p><h2 id=95-本章小结>9.5 本章小结</h2><p>在本章中，我们完成了从物理网络到虚拟网络的关键跃迁。我们认识到，网络虚拟化是构建一个现代、安全、多租户GPU云平台不可或缺的技术支柱。它将底层共享的、刚性的物理网络，抽象成了上层隔离的、弹性的、可编程的虚拟网络服务。</p><p>我们的探索始于网络虚拟化的基石——基于SDN的VPC技术。我们理解了SDN“控制与转发分离”的核心思想，以及它如何通过Overlay网络（特别是VXLAN），实现了对网络资源的池化和逻辑隔离，从而为每个租户构建起一个独立的私有网络空间。</p><p>接着，我们深入了VPC中几个最核心的网络服务组件：</p><p>云负载均衡器，作为机器学习网络，特别是AI推理服务的中流砥柱。我们对比了四层负载均衡（如LVS/DR）追求的极致性能和七层负载均衡（如Nginx/Ingress）追求的应用层灵活性，明确了它们在不同场景下的选型。</p><p>我们学习了如何通过VPC网关来打通VPC的内外连接。无论是通过VPN或专线连接本地IDC，还是通过对等连接或云企业网连接其他VPC，亦或是通过NAT网关和互联网网关连接公网，这些网关组件共同构成了VPC与世界对话的桥梁。</p><p>最后，我们深入到底层，剖析了实现这些高级网络功能的引擎——SDN NFV网关。我们看到了其实现方式的演进，从基于virtio/vhost的传统虚拟机部署，到追求极致性能的SR-IOV硬件直通，再到利用DPDK在通用服务器上榨干性能的软件加速方案。我们认识到，现代的NFV网关是一个软硬件协同、不断将功能下沉和卸载的复杂系统。</p><p>通过本章的学习，我们不再仅仅是物理网络的运维者，更成为了虚拟网络的“规划师”和“架构师”。我们掌握了一套完整的词汇和工具，去描述、设计和实现一个能够满足AI时代复杂应用需求的云原生网络。这个弹性的、安全的、可编程的虚拟网络层，将与我们在前几章学习的物理网络、GPU虚拟化等技术紧密结合，共同构筑起一个强大的、面向未来的大模型算力基础设施。</p></div><div class=post_footer><div class=meta><div class=info><span class="field tags"><i class=ri-stack-line></i>
<a href=https://zhurongshuo.com/tags/%E4%B9%A6%E7%A8%BF/>书稿</a></span></div></div></div></div><div class=doc_comments></div></div></div></div><a id=back_to_top href=# class=back_to_top><i class=ri-arrow-up-s-line></i></a><footer class=footer><div class=powered_by><a href=https://varkai.com>Designed by VarKai, </a><a href=http://www.gohugo.io/>Proudly published with Hugo,</a></div><div class=footer_slogan><span>法不净空，觉无性也。</span></div><div class=powered_by style=margin-top:10px;font-size:14px><a href=https://zhurongshuo.com/>Copyright © 2010-2025 祝融说 zhurongshuo.com All Rights Reserved.</a></div></footer><script defer src=https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><link href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.css rel=stylesheet integrity="sha256-7qiTu3a8qjjWtcX9w+f2ulVUZSUdCZFEK62eRlmLmCE=" crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin=anonymous></script><script defer src=https://zhurongshuo.com/js/zozo.js></script><script type=text/javascript async src="https://cdn.jsdelivr.net/npm/mathjax@2.7.8/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["[[","]]"]],processEscapes:!0,processEnvironments:!0,skipTags:["script","noscript","style","textarea","pre"],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}}),MathJax.Hub.Queue(function(){var e,t=MathJax.Hub.getAllJax();for(e=0;e<t.length;e+=1)t[e].SourceElement().parentNode.className+=" has-jax"})</script><style>code.has-jax{font:inherit;font-size:100%;background:inherit;border:inherit;color:#515151}</style><script>window.GA_MEASUREMENT_ID="G-KKJ5ZEG1NB",window.GA_CONFIG={enableReadingTime:!0,enableScrollDepth:!0,enableOutboundLinks:!0,enableDownloads:!0,lazyLoadTimeout:3e3}</script><script defer src=https://zhurongshuo.com/js/ga-optimizer.js></script></body></html>