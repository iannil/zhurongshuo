<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><meta name=google-site-verification content="8_xpI-TS3tNV8UPug-Q6Ef3BhKTcy0WOG7dEdAcm2zk"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="祝融"><link rel=dns-prefetch href=//cdn.jsdelivr.net><link rel=preconnect href=https://cdn.jsdelivr.net crossorigin><link rel=dns-prefetch href=//www.googletagmanager.com><link rel=preconnect href=https://www.googletagmanager.com crossorigin><link rel=canonical href=https://zhurongshuo.com/practices/season-4/intelligent-computing-center-construction-guide/part-04/chapter-13/><title>祝融说。 第13章 服务机器学习的GPU计算平台落地案例</title><meta property="og:title" content="第13章 服务机器学习的GPU计算平台落地案例"><meta property="og:url" content="https://zhurongshuo.com/practices/season-4/intelligent-computing-center-construction-guide/part-04/chapter-13/"><meta property="og:site_name" content="祝融说。"><meta property="og:locale" content="zh_CN"><meta property="og:type" content="article"><meta property="article:published_time" content="2025-12-07T00:00:00+08:00"><meta property="article:modified_time" content="2025-12-07T00:00:00+08:00"><meta property="article:tag" content="书稿"><meta name=description content="在本书的前三个部分，我们已经系统性地、由浅入深地完成了构建一个大规模GPU计算平台所需的全栈知识体系的学习。我们如同一个学徒，从认识最基础的“砖瓦”（GPU芯片），到学会建造“房屋”（GPU服务器），再到规划整个“城市”的交通（网络）、仓储（存储）、市政（应用平台）与管理（运维运营）。我们已经掌握了所有的理论知识、设计原则和关键技术。
"><meta property="og:description" content="在本书的前三个部分，我们已经系统性地、由浅入深地完成了构建一个大规模GPU计算平台所需的全栈知识体系的学习。我们如同一个学徒，从认识最基础的“砖瓦”（GPU芯片），到学会建造“房屋”（GPU服务器），再到规划整个“城市”的交通（网络）、仓储（存储）、市政（应用平台）与管理（运维运营）。我们已经掌握了所有的理论知识、设计原则和关键技术。
"><meta property="og:image" content="https://zhurongshuo.com/images/favicon.ico"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="第13章 服务机器学习的GPU计算平台落地案例"><meta name=twitter:description content="在本书的前三个部分，我们已经系统性地、由浅入深地完成了构建一个大规模GPU计算平台所需的全栈知识体系的学习。我们如同一个学徒，从认识最基础的“砖瓦”（GPU芯片），到学会建造“房屋”（GPU服务器），再到规划整个“城市”的交通（网络）、仓储（存储）、市政（应用平台）与管理（运维运营）。我们已经掌握了所有的理论知识、设计原则和关键技术。
"><meta name=twitter:image content="https://zhurongshuo.com/images/favicon.ico"><meta name=keywords content="智算中心建设指南：大模型算力的基础架构,第13章 服务机器学习的GPU计算平台落地案例"><link rel="shortcut icon" href=https://zhurongshuo.com/images/favicon.ico><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/normalize.css><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/animate-custom.css><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/zozo.css><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/remixicon-custom.css><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/highlight.css><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","headline":"第13章 服务机器学习的GPU计算平台落地案例","description":"在本书的前三个部分，我们已经系统性地、由浅入深地完成了构建一个大规模GPU计算平台所需的全栈知识体系的学习。我们如同一个学徒，从认识最基础的“砖瓦”（GPU芯片），到学会建造“房屋”（GPU服务器），再到规划整个“城市”的交通（网络）、仓储（存储）、市政（应用平台）与管理（运维运营）。我们已经掌握了所有的理论知识、设计原则和关键技术。\n","datePublished":"2025-12-07T00:00:00\u002b08:00","dateModified":"2025-12-07T00:00:00\u002b08:00","author":{"@type":"Person","name":"祝融"},"publisher":{"@type":"Organization","name":"祝融说。","logo":{"@type":"ImageObject","url":"https:\/\/zhurongshuo.com\/images\/favicon.ico"}},"image":"https:\/\/zhurongshuo.com\/images\/favicon.ico","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/zhurongshuo.com\/practices\/season-4\/intelligent-computing-center-construction-guide\/part-04\/chapter-13\/"},"keywords":"书稿"}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"首页","item":"https:\/\/zhurongshuo.com\/"},{"@type":"ListItem","position":2,"name":"practices","item":"https:\/\/zhurongshuo.com\/practices/"},{"@type":"ListItem","position":3,"name":"第13章 服务机器学习的GPU计算平台落地案例","item":"https:\/\/zhurongshuo.com\/practices\/season-4\/intelligent-computing-center-construction-guide\/part-04\/chapter-13\/"}]}</script></head><body><div class="main animate__animated animate__fadeInDown"><div class="nav_container animated fadeInDown"><div class=site_nav id=site_nav><ul><li><a href=https://zhurongshuo.com/>首页</a></li><li><a href=https://zhurongshuo.com/start/>开始</a></li><li><a href=https://zhurongshuo.com/advanced/>进阶rc</a></li><li><a href=https://zhurongshuo.com/posts/>归档</a></li><li><a href=https://zhurongshuo.com/tags/>标签</a></li><li><a href=https://zhurongshuo.com/about/>关于</a></li></ul></div><div class=menu_icon><a id=menu_icon><i class=ri-menu-line></i></a></div></div><div class="header animated fadeInDown"><div class=site_title_container><div class=site_title><h1><a href=https://zhurongshuo.com/><span class=web-font>祝融说。</span></a></h1></div><div class=description><p class=sub_title>法不净空，觉无性也。</p><div class=my_socials><a href=https://zhurongshuo.com/books/ title=book-open><i class=ri-book-open-line></i></a>
<a href=https://zhurongshuo.com/practices/ title=trophy><i class=ri-trophy-line></i></a>
<a href=https://zhurongshuo.com/gallery/ title=gallery><i class=ri-gallery-line></i></a>
<a href=https://zhurongshuo.com/about/ title=game><i class=ri-game-line></i></a>
<a href type=application/rss+xml title=rss target=_blank><i class=ri-rss-fill></i></a></div></div></div></div><div class=content><div class=post_page><div class="post animate__animated animate__fadeInDown"><div class="post_title post_detail_title"><h2><a href=https://zhurongshuo.com/practices/season-4/intelligent-computing-center-construction-guide/part-04/chapter-13/>第13章 服务机器学习的GPU计算平台落地案例</a></h2><span class=date>2025.12.07</span></div><div class="post_content markdown"><p>在本书的前三个部分，我们已经系统性地、由浅入深地完成了构建一个大规模GPU计算平台所需的全栈知识体系的学习。我们如同一个学徒，从认识最基础的“砖瓦”（GPU芯片），到学会建造“房屋”（GPU服务器），再到规划整个“城市”的交通（网络）、仓储（存储）、市政（应用平台）与管理（运维运营）。我们已经掌握了所有的理论知识、设计原则和关键技术。</p><p>现在，是时候将所有这些知识融会贯通，投入到一场“实战演习”中了。理论的价值最终体现在实践之中。本章——《服务机器学习的GPU计算平台落地案例》——将作为本书的收官之作，我们将扮演一个AI基础设施总架构师的角色，从零开始，为一个具体、真实、且极具挑战性的机器学习应用场景，设计和实现一个完整的、端到端的GPU计算平台。</p><p>我们将以当前AI领域最复杂、投入最大的应用之一——自动驾驶模型训练——作为我们的需求来源。自动驾驶对算力、数据、网络、存储的需求是全方位、立体化、且极其苛刻的，这使得它成为了检验我们基础设施设计能力的绝佳“试金石”。</p><p>我们将遵循一个完整的系统设计流程，从需求分析开始，到总体架构设计，再到计算、存储、网络等各个子系统的详细设计与实现。</p><ol><li>需求分析：我们将首先深入自动驾驶模型训练的工作流，理解其独特的数据特征、模型规模、训练范式以及对基础设施的极致要求。</li><li>总体设计：基于需求分析，我们将确立平台的核心设计理念——基于云原生的高性能计算（Cloud-Native HPC），旨在将HPC的极致性能与云的弹性、敏捷性完美结合。</li><li>分系统设计与实现：随后，我们将把前面章节学习到的所有知识都应用到这个案例中，逐一进行计算、存储、网络三大核心子系统的需求分析、技术选型和架构设计。<ol><li>计算：如何选择GPU和服务器？如何设计集群规模和混合并行策略？</li><li>存储：如何构建分层存储体系来管理PB级的海量数据？如何解决高并发的数据读取瓶颈？</li><li>网络：如何设计多平面网络来承载训练、存储和管理等不同流量？</li></ol></li><li>平台与运维串联：在设计三大子系统的同时，我们会将第十一、十二章学习到的应用平台和运维运营理念贯穿其中，确保我们构建的不仅仅是一个“裸”的HPC集群，而是一个易于使用、易于管理的现代化AI PaaS平台。</li></ol><p>通过这个从需求到落地的完整案例演练，我们将把之前所有分散的知识点串联成一个有机的整体，真正体验一次作为总架构师进行端到端系统设计的全过程。这将是对我们整个学习旅程的最终检验，也是将理论知识转化为实践能力的最后一跃。完成本章后，你将不仅仅是“知道”如何构建一个GPU平台，更是“懂得”如何根据真实世界的复杂需求，去思考、去权衡、去创造一个真正优秀的解决方案。</p><h2 id=131-需求来源自动驾驶模型训练>13.1 需求来源：自动驾驶模型训练</h2><p>自动驾驶，特别是L4/L5级别的高度自动驾驶，被誉为“人工智能皇冠上的明珠”。其背后，是一个由数据驱动的、规模空前庞大的机器学习闭环系统。要理解如何为它设计基础设施，我们必须首先深入其工作流的核心。</p><h3 id=1311-自动驾驶的数据闭环工作流>13.1.1 自动驾驶的“数据闭环”工作流</h3><p>自动驾驶系统的迭代，依赖于一个持续循环、不断优化的“数据闭环”：</p><h4 id=数据采集data-acquisition>数据采集（Data Acquisition）</h4><p>庞大的自动驾驶测试车队，每天24小时在全球各地路采。</p><p>每辆车都配备了多种传感器：高分辨率摄像头（前视、后视、侧视、环视）、高精度激光雷达（LiDAR）、毫米波雷达（RADAR）、IMU（惯性测量单元）、GPS等。</p><p>这些传感器每秒钟都会产生巨量的数据。一辆测试车每小时产生的数据量可达1-2 TB。一个拥有数百辆车的车队，每天产生的数据量可以轻松达到PB级别。</p><h4 id=数据回传与存储data-ingestion--storage>数据回传与存储（Data Ingestion & Storage）</h4><p>采集到的数据通过移动网络或在车辆回场后，通过高速有线网络，回传到数据中心。</p><p>这些原始的、非结构化的数据（我们称之为“Raw Data”），需要被可靠地、低成本地存储起来。其总体量将达到数十乃至数百PB，是典型的“数据湖”场景。</p><h4 id=数据处理与标注data-processing--labeling>数据处理与标注（Data Processing & Labeling）</h4><p>原始数据并不能直接用于训练。算法工程师需要从海量的“生肉”中，挖掘出有价值的“困难场景（Corner Cases）”，例如一次罕见的行人横穿、一次恶劣天气下的光照变化、一次复杂的无保护左转等。这个过程被称为数据挖掘（Data Mining）。</p><p>被挖掘出的有价值数据片段，需要被送去进行人工或自动化的数据标注。例如，在图像中框出所有的车辆、行人、交通标志，并为它们打上标签；在LiDAR点云中，对每个点进行语义分割。标注后的数据（我们称之为“Ground Truth”），是模型学习的“标准答案”。</p><h4 id=模型训练model-training>模型训练（Model Training）</h4><p>这是对算力需求最集中的环节。算法工程师使用标注好的数据集，来训练各种类型的深度学习模型：</p><p>感知（Perception）模型：如基于图像的2D/3D目标检测（YOLO, CenterPoint）、基于LiDAR点云的语义分割和目标检测、多传感器融合（BEV-Fusion）等。</p><p>预测（Prediction）模型：预测道路上其他交通参与者（车辆、行人）在未来几秒内的轨迹。</p><p>规划与控制（Planning & Control）模型：基于对环境的理解和对未来的预测，决策出本车最优的行驶路径和速度。</p><p>这些模型，特别是端到端的大模型和BEV（鸟瞰图视角）模型，其参数量巨大，训练过程需要在大规模的GPU集群上进行，采用复杂的混合并行策略。</p><h4 id=仿真与评估simulation--evaluation>仿真与评估（Simulation & Evaluation）</h4><p>训练好的新模型，并不能直接上路测试。它首先需要在一个大规模的、逼真的仿真平台上进行测试。</p><p>仿真平台会回放各种真实的或虚构的交通场景，来检验新模型在各种Corner Case下的表现。一次完整的回归测试，可能需要在数千个CPU或GPU节点上并行运行数百万个仿真案例。</p><h4 id=模型部署与在环验证deployment--hil>模型部署与在环验证（Deployment & HIL）</h4><p>通过仿真验证的模型，会被部署到硬件在环（Hardware-in-the-Loop, HIL）测试台架上，甚至真实的测试车辆上，进行小范围的实路测试。</p><p>在实路测试中，系统会持续收集新模型表现不佳的场景，这些新的“困难场景”数据又会被回传到数据中心，进入下一个“数据闭环”的迭代。</p><h3 id=1312-自动驾驶对基础设施的极致需求>13.1.2 自动驾驶对基础设施的极致需求</h3><p>通过上述工作流分析，我们可以提炼出自动驾驶对基础设施的几个核心需求：</p><h4 id=海量的分层的存储需求>海量的、分层的存储需求</h4><p>PB到EB级的“数据湖”：需要一个成本极低、扩展性近乎无限的存储系统，来归档海量的原始路采数据。—— 这指向了分布式对象存储。</p><p>高性能的“训练数据仓库”：标注好的、用于训练的数据集，需要被一个能够支持数千个节点高并发、低延迟、高吞吐量读取的存储系统来承载。—— 这指向了分布式并行文件系统。</p><p>快速的检查点（Checkpoint）存储：训练过程中产生的模型检查点可能高达数百GB，需要被快速地写入，以减少因抢占或故障导致的训练中断时间。</p><h4 id=超大规模的异构计算需求>超大规模的异构计算需求</h4><p>大规模GPU训练集群：模型训练是核心负载。需要一个拥有数千张顶级GPU卡、通过高速网络互联的计算集群，来支持长达数周甚至数月的、大规模的分布式训练任务。</p><p>大规模CPU/GPU仿真集群：仿真测试对计算的需求量同样巨大，且通常是大量可以并行的、相对独立的短任务，对CPU和GPU都有需求。</p><p>数据处理与分析集群：数据挖掘、数据清洗等ETL（提取、转换、加载）任务，通常运行在基于Spark或Flink的大数据集群上。</p><h4 id=极致性能的网络需求>极致性能的网络需求</h4><p>高带宽、无损的计算网络：用于支持大规模分布式训练中的梯度和参数交换。网络的性能直接决定了集群的扩展效率。</p><p>高吞吐的存储网络：必须能够支撑起数千个节点同时从存储系统中高速拉取数据，避免“数据喂养”成为瓶颈。</p><p>高带宽的数据入口：需要有从外部（路采车队、标注中心）到数据中心的高速数据上传通道。</p><h4 id=对弹性和敏捷性的要求>对弹性和敏捷性的要求</h4><p>多种工作负载并存：集群需要同时支持长周期的训练大任务、海量的仿真短任务、交互式的数据分析任务等。</p><p>资源抢占与调度：不同任务之间存在资源竞争。平台需要有能力支持任务的抢占式调度（例如，一个高优先级的模型发布前回归测试，可以抢占一个普通的探索性训练任务）和断点续训。</p><p>快速迭代：算法工程师需要能够快速地搭建实验环境、提交训练任务、获取结果，整个平台的易用性和敏捷性至关重要。</p><p>这些复杂而苛刻的需求，告诉我们不能简单地照搬传统HPC或互联网的架构。我们需要一种新的、融合两者优点的架构。</p><h2 id=132-总体设计基于云原生的高性能计算>13.2 总体设计——基于云原生的高性能计算</h2><p>面对自动驾驶模型训练这一复杂场景，我们的总体设计理念是构建一个“基于云原生的高性能计算（Cloud-Native HPC）”平台。</p><p>高性能计算（HPC）：我们将借鉴传统超算中心的设计思想，来构建平台的核心“引擎”。这意味着：</p><p>采用专门的、高密度的GPU计算节点。</p><p>采用专门的、无损的、低延迟的高性能计算网络（如InfiniBand或RoCE）来连接计算节点。</p><p>采用专门的、高性能的并行文件系统来支撑训练数据的读取。</p><p>目标是为大规模、紧耦合的分布式训练任务，提供极致的、可预测的裸金属性能。</p><p>云原生（Cloud-Native）：我们将全面拥抱以Kubernetes为核心的云原生技术栈，来构建平台的“操作系统”和“用户界面”。这意味着：
一切皆容器：所有的应用，无论是训练任务、仿真任务、还是数据处理服务，都将被封装在容器中运行。</p><p>统一的资源编排：使用Kubernetes作为唯一的、统一的资源调度和编排平台，来管理集群中所有的计算、存储和网络资源。</p><p>声明式API与自动化：通过Kubernetes的声明式API，实现基础设施的自动化部署、应用的弹性伸缩和自愈。</p><p>敏捷与弹性：利用云原生的生态，为用户提供灵活、按需、自助式的资源申请和环境管理能力，提升研发效率。</p><p>Cloud-Native HPC的本质，就是试图在同一个平台上，既能跑好像分布式AI训练这样对极致性能和通信延迟要求极高的“HPC类”应用，又能跑好像数据分析、仿真、在线服务这样需要弹性、敏捷和隔离性的“云类”应用。它试图将HPC的“肌肉”和Cloud的“大脑”完美地结合起来。</p><p>基于这一总体设计理念，我们将开始进行各个子系统的详细设计。</p><h2 id=133-计算需求分析与设计实现>13.3 计算需求分析与设计实现</h2><p>计算子系统是整个平台的核心，其设计直接决定了平台能够支持的模型规模和训练效率。</p><h3 id=1331-需求分析>13.3.1 需求分析</h3><p>GPU选型：自动驾驶模型，特别是BEV等多模态融合模型，参数量巨大，且大量使用Transformer结构。这要求GPU具备：</p><p>巨大的显存容量：以容纳巨大的模型、激活值和优化器状态。</p><p>极高的混合精度计算能力：特别是针对Transformer优化的能力。</p><p>极高的GPU间互联带宽：以支持高效的张量并行和流水线并行。</p><p>服务器选型：需要选择为大规模AI训练优化的、高密度的GPU服务器。</p><p>集群规模：这是一个关键的业务和成本决策。假设根据算法团队的规划，为了在合理的时间内（例如2周）完成一个SOTA（State-of-the-Art）模型的训练，他们估算出需要约1024张顶级GPU卡的持续算力。</p><p>调度需求：需要支持大规模分布式训练任务的批量提交、排队、抢占，并需要与容器平台（Kubernetes）集成。同时，为了提升利用率，需要支持GPU的共享调度。</p><h3 id=1332-设计与实现>13.3.2 设计与实现</h3><h4 id=gpu与服务器选型>GPU与服务器选型</h4><p>GPU：毫无疑问，选择当前旗舰级的NVIDIA H100 80GB SXM5模块。其80GB HBM3显存、第四代Tensor Core对FP8的支持、以及Transformer引擎，完美契合了自动驾驶大模型的需求。</p><p>服务器：选择NVIDIA DGX H100或其认证的OEM厂商（如Supermicro, Dell, Inspur）生产的、搭载8张H100 SXM5的HGX H100 8-GPU服务器。这些服务器通过NVSwitch实现了8卡之间900 GB/s的全连接NVLink，是进行高效张量并行的理想平台。</p><h4 id=集群规模与物理布局>集群规模与物理布局</h4><p>总规模：为了满足1024卡的算力需求，我们需要部署<code>1024 / 8 = 128</code>台HGX H100 8-GPU服务器。</p><p>物理单元：我们将这128台服务器组织成一个可扩展单元（Scalable Unit, SU）。这个SU是一个独立的、拥有完整计算、网络和存储资源的“巨型计算机”。未来需要扩容时，可以再复制一个或多个SU。</p><p>机柜布局：每台HGX H100服务器功耗极高（约10.2KW），对机柜的供电和散热能力提出严峻挑战。需要采用高密度的、支持液冷或先进风冷的机柜。假设一个机柜可以容纳4台HGX服务器，那么128台服务器需要32个机柜。</p><h4 id=训练策略与并行模式>训练策略与并行模式</h4><p>对于一个将在1024卡集群上运行的超大模型，我们将采用3D混合并行策略：</p><p>张量并行（TP）：在单台HGX服务器内部的8个GPU之间，进行张量并行（例如，TP size = 8）。这得益于900 GB/s的NVLink带宽。</p><p>流水线并行（PP）：将模型的不同层，切分到不同的服务器上。例如，使用4台服务器（32个GPU）作为一个流水线并行组（PP size = 4，每个stage由一个8-GPU的TP组构成）。</p><p>数据并行（DP）：在多个这样的流水线并行组之间，进行数据并行。在我们的1024卡集群中，总共可以有<code>1024 / 32 = 32</code>个数据并行副本（DP size = 32）。</p><p>同时，我们将采用ZeRO-1/2来优化数据并行中的优化器状态和梯度存储，进一步节省显存。</p><h4 id=调度系统设计>调度系统设计</h4><p>底层编排：使用Kubernetes作为统一的资源编排层。</p><p>批量调度器：仅靠K8s默认调度器无法满足HPC类任务的需求。我们需要引入一个批量调度器（Batch Scheduler），如Volcano或Kube-Batch。</p><p>Gang Scheduling：批量调度器支持“成组调度”。一个1024卡的分布式训练任务，必须保证所有1024个Pod都同时被调度成功，才会开始运行。只要有一个Pod资源不足，整个任务就会处于等待状态，而不会启动部分Pod，造成资源浪费。</p><p>队列与优先级：支持创建不同的任务队列（如高优队列、普通队列），并支持任务的抢占。</p><p>GPU共享：</p><p>对于大规模训练任务，我们将通过K8s Device Plugin，为每个Pod分配独占的物理GPU。</p><p>对于开发、调试、小规模推理等任务，我们将启用NVIDIA MIG功能。在部分服务器上，将H100 GPU划分为多个GI实例，并通过MIG的Device Plugin上报给K8s，供用户按需申请。这极大地提升了GPU的利用率。</p><h2 id=134-存储需求分析与设计实现>13.4 存储需求分析与设计实现</h2><p>存储是支撑整个数据闭环的关键，其设计必须满足分层、高性能、高并发的需求。</p><h3 id=1341-需求分析>13.4.1 需求分析</h3><p>数据湖：需要一个EB级的、成本极低的存储池，用于归档每日产生的PB级原始路采数据（Raw Data）。数据的写入是持续的，但读取频率相对较低。</p><p>训练数据仓库：需要一个高性能的存储系统，来存放经过清洗和标注的、用于模型训练的数据集（约PB级）。这个系统必须能够承受1024个GPU节点、数万个data loader进程的高并发读取压力，提供TB/s级别的聚合读取带宽。必须支持POSIX接口和GPUDirect Storage。</p><p>模型检查点：训练过程中需要频繁地、快速地写入数百GB的模型检查点。</p><p>系统与应用存储：需要为所有服务器的操作系统、Kubernetes集群的etcd、以及各种中间件（如数据库、Kafka）提供高可用的持久化存储。</p><h3 id=1342-分层存储架构设计与实现>13.4.2 分层存储架构设计与实现</h3><p>我们将设计一个经典的三层存储架构：</p><h4 id=冷数据层数据湖分布式对象存储>冷数据层（数据湖）：分布式对象存储</h4><p>技术选型：采用基于大容量HDD硬盘的Ceph集群，并启用其RGW（RADOS Gateway）功能，提供S3兼容的对象存储接口。</p><p>设计要点：</p><p>规模：部署数百个存储节点，每个节点挂载多块大容量（如18TB/20TB）的SATA HDD，构建一个EB级的存储池。</p><p>数据保护：采用纠删码（Erasure Coding, EC）代替多副本。例如，<code>k=8, m=3</code>的EC方案，可以将数据分成8个数据块和3个校验块，存放在11个不同的OSD上。它只需要1.375倍的存储开销，就能容忍任意3个OSD的故障，相比3副本（3倍开销）极大地节省了成本。</p><p>接入：数据采集和预处理集群通过S3 API，将数据写入这个数据湖。</p><h4 id=热数据层训练数据仓库分布式并行文件系统>热数据层（训练数据仓库）：分布式并行文件系统</h4><p>技术选型：采用Lustre文件系统，其后端存储全部使用高性能的NVMe SSD。</p><p>设计要点：</p><p>架构：部署一个高可用的MDS（元数据服务器）集群，以及一个由数十台OSS（对象存储服务器）组成的大规模数据服务器集群。每个OSS都配备多块高性能的NVMe SSD。</p><p>网络：所有Lustre的服务器（MDS, OSS）和计算节点（客户端）都必须连接到同一个高性能的存储网络上（我们将在下一节讨论）。</p><p>GPUDirect Storage支持：Lustre客户端需要配置为支持GPUDirect Storage。这将允许GPU在读取训练数据时，绕过CPU内存，实现从Lustre OSS直达GPU显存的数据路径。</p><p>容量与性能：整个Lustre集群的容量设计在PB级别，足以容纳当前和未来一段时间内的所有训练数据集。其聚合带宽通过增加OSS节点的数量，可以线性扩展到TB/s级别，满足1024卡集群的“喂养”需求。</p><p>工作流：训练任务开始前，一个数据准备作业会从冷数据层的Ceph对象存储中，将本次训练所需的数据子集，预取（Prefetch）到热数据层的Lustre文件系统中。</p><h4 id=平台与应用存储层分布式块存储>平台与应用存储层：分布式块存储</h4><p>技术选型：我们可以复用对象存储的Ceph集群，但在其中划分出一个专门由SSD硬盘组成的存储池（Pool），并在此池上启用RBD（RADOS Block Device）服务。</p><p>设计要点：</p><p>这个SSD池专门为需要低延迟和高IOPS的应用服务。</p><p>通过Kubernetes的Ceph CSI插件，我们可以动态地为Pod创建和挂载RBD类型的持久化卷（PV）。</p><p>应用：</p><p>为所有计算和管理节点的操作系统提供RBD启动盘。</p><p>为Kubernetes的etcd集群、Prometheus、数据库等有状态服务提供高可用的持久化存储。</p><p>作为模型检查点的临时写入位置，利用SSD的高写入性能。</p><p>通过这个分层存储设计，我们用最合适的成本，满足了不同场景的苛刻需求，构建了一个既“深不可测”又“快如闪电”的数据基座。</p><h2 id=135-网络需求分析与设计实现>13.5 网络需求分析与设计实现</h2><p>网络是连接计算和存储的命脉。对于我们这个1024卡规模的集群，网络设计是成败的关键。</p><h3 id=1351-需求分析>13.5.1 需求分析</h3><p>计算网络：</p><p>必须支持1024个GPU节点之间的大规模、高并发RDMA通信。</p><p>必须是无损的、低延迟的。</p><p>聚合带宽必须能够支撑起3D并行训练中的密集通信。</p><p>必须具备高可用性和可扩展性。</p><p>存储网络：</p><p>必须能够连接所有1024个计算节点和所有的Lustre、Ceph存储节点。</p><p>必须提供TB/s级别的聚合带宽，以满足并发数据加载的需求。</p><p>带外管理网络：</p><p>必须连接所有设备（服务器BMC、交换机、PDU等）的管理端口。</p><p>必须与所有数据网络物理隔离，确保最高级别的安全和可靠性。</p><h3 id=1352-多平面网络架构设计与实现>13.5.2 多平面网络架构设计与实现</h3><p>我们将采用物理上分离的多平面网络架构。</p><h4 id=计算网络compute-fabric>计算网络（Compute Fabric）</h4><p>技术选型：InfiniBand (IB)。对于1024卡这种超大规模的、对通信极度敏感的HPC类负载，IB以其原生的无损特性、硬件级的拥塞管理和极致的低延迟，是比RoCE更稳妥、性能更可预测的选择。我们将采用NVIDIA Quantum-2 400Gb/s InfiniBand平台。</p><p>拓扑架构：采用一个两层的Fat-Tree (Clos)架构。</p><p>Leaf层：部署若干台Leaf交换机。每台HGX H100服务器（拥有8张ConnectX-7 400G IB网卡）会以冗余的方式，将其中的4张网卡连接到一个Leaf交换机，另外4张连接到另一个Leaf交换机。</p><p>Spine层：部署一个由多台大型Director Switch（如NVIDIA QM9700）组成的Spine层。每个Leaf交换机都全连接到所有的Spine交换机上。</p><p>无阻塞设计：精心设计Leaf-Spine之间的上行链路数量，实现一个1:1无阻塞的网络，确保任意两个GPU节点之间都能获得全速率的400Gbps带宽。</p><p>SHARP技术：NVIDIA Quantum-2平台支持SHARP（Scalable Hierarchical Aggregation and Reduction Protocol）技术，可以将<code>All-Reduce</code>等集体通信操作，直接在网络交换机中进行硬件计算和聚合，而不是在终端GPU上完成。这可以极大地降低<code>All-Reduce</code>的延迟，进一步提升分布式训练的扩展效率。</p><h4 id=存储与业务网络storage--general-purpose-network>存储与业务网络（Storage & General Purpose Network）</h4><p>为了节约成本和简化布线，我们将存储和业务流量承载在一个统一的、物理分离的以太网网络上。</p><p>技术选型：采用基于NVIDIA Spectrum-X或同级别厂商的、支持RoCE的400Gb/s以太网交换机。</p><p>拓扑架构：同样采用一个独立的Spine-Leaf Clos架构。</p><p>连接：所有计算节点、Lustre存储节点、Ceph存储节点以及管理/API服务器，都通过其以太网卡连接到这个网络。</p><p>逻辑隔离：</p><p>通过VLAN或VRF技术，在逻辑上将存储流量和业务流量进行隔离。</p><p>对于连接Lustre的存储流量，如果需要使用RDMA（Lustre over RDMA），则需要在这个网络的相关部分，配置无损以太网（PFC/ECN）。</p><p>对于普通的业务流量和连接Ceph的TCP/IP流量，则运行在标准的、有损的以太网VLAN上。</p><h4 id=带外管理网络out-of-band-management-network>带外管理网络（Out-of-Band Management Network）</h4><p>技术选型：采用一对冗余的、简单的10GbE以太网交换机。</p><p>拓扑：简单的星型或树形拓扑。</p><p>连接：所有服务器的BMC端口、所有IB和以太网交换机的管理端口、PDU等，全部连接到这个完全独立的管理网络上。</p><p>通过这套物理上三平面分离的网络设计，我们为不同类型的流量提供了最优的、互不干扰的承载通道：用最昂贵、性能最好的InfiniBand网络来伺服最宝贵的计算流量；用一个高性能、可划分的以太网来承载存储和通用业务；用一个简单可靠的管理网络来保障整个集群的“生命线”。</p><h2 id=136-本章小结>13.6 本章小结</h2><p>在本章中，我们进行了一次激动人心的、从理论到实践的“毕业设计”。我们以极具挑战性的自动驾驶模型训练为靶心，将本书前面所有章节学习到的知识体系，进行了一次系统性的、端到端的综合应用，完整地设计了一个世界级的、基于云原生HPC理念的GPU计算平台。</p><p>我们的设计之旅始于对需求来源的深刻洞察。通过剖析自动驾驶复杂的“数据闭环”工作流，我们提炼出了其对基础设施在存储、计算、网络、弹性等多个维度上的极致、多样的需求。这为我们后续所有的架构决策提供了清晰的指引。</p><p>在总体设计中，我们确立了“Cloud-Native HPC”这一核心指导思想，旨在鱼与熊掌兼得——既要HPC的极致性能，又要Cloud的敏捷与弹性。这决定了我们后续的技术选型将是“两条腿走路”：用最硬核的HPC技术构建性能引擎，用最先进的云原生技术构建管理和应用平台。</p><p>在具体的分系统设计与实现中，我们做出了如下关键决策：</p><p>在计算层面，我们选择了HGX H100 8-GPU服务器作为标准计算单元，构建了1024卡的集群规模，并设计了TP+PP+DP的3D混合并行训练策略。在调度上，我们采用Kubernetes + Volcano的组合来支持大规模批任务的成组调度，并通过MIG技术实现了GPU的细粒度共享，兼顾了大规模训练和日常研发的需求。</p><p>在存储层面，我们构建了一个分层异构的存储体系：用低成本的Ceph对象存储构建EB级的“数据湖”；用高性能的、全NVMe的Lustre并行文件系统构建TB/s级吞吐的“热数据仓库”，并通过GPUDirect Storage喂养GPU；用高IOPS的Ceph块存储为系统和应用提供高可用的持久化卷。</p><p>在网络层面，我们设计了物理三平面的分离式网络架构：用极致性能的400G InfiniBand网络和Fat-Tree拓扑来承载核心的计算流量，并利用SHARP等技术进行硬件加速；用一个独立的、高性能的以太网来承载存储和通用业务流量；用一个完全隔离的带外管理网来保障整个集群的生命线。</p><p>至此，我们不仅完成了一次纸面上的设计，更重要的是，我们建立了一套完整的设计思维框架。我们学会了如何从一个复杂的业务需求出发，进行层层分解，然后在每一个技术点上，基于我们深厚的知识储备，进行合理的权衡与选型，并最终将所有这些选择，有机地整合成一个协调、高效、优雅的整体架构。</p><p>这正是本书希望传递给每一位读者的核心价值——不仅仅是知识的集合，更是一种架构的智慧。希望通过这次旅程，你已经准备好，去迎接大模型时代的挑战，去设计和构建属于你自己的、强大的AI基础设施。前路浩荡，未来可期。</p></div><div class=post_footer><div class=meta><div class=info><span class="field tags"><i class=ri-stack-line></i>
<a href=https://zhurongshuo.com/tags/%E4%B9%A6%E7%A8%BF/>书稿</a></span></div></div></div></div><div class=doc_comments></div></div></div></div><a id=back_to_top href=# class=back_to_top><i class=ri-arrow-up-s-line></i></a><footer class=footer><div class=powered_by><a href=https://varkai.com>Designed by VarKai, </a><a href=http://www.gohugo.io/>Proudly published with Hugo,</a></div><div class=footer_slogan><span>法不净空，觉无性也。</span></div><div class=powered_by style=margin-top:10px;font-size:14px><a href=https://zhurongshuo.com/>Copyright © 2010-2025 祝融说 zhurongshuo.com All Rights Reserved.</a></div></footer><script defer src=https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><link href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.css rel=stylesheet integrity="sha256-7qiTu3a8qjjWtcX9w+f2ulVUZSUdCZFEK62eRlmLmCE=" crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin=anonymous></script><script defer src=https://zhurongshuo.com/js/zozo.js></script><script type=text/javascript async src="https://cdn.jsdelivr.net/npm/mathjax@2.7.8/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["[[","]]"]],processEscapes:!0,processEnvironments:!0,skipTags:["script","noscript","style","textarea","pre"],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}}),MathJax.Hub.Queue(function(){var e,t=MathJax.Hub.getAllJax();for(e=0;e<t.length;e+=1)t[e].SourceElement().parentNode.className+=" has-jax"})</script><style>code.has-jax{font:inherit;font-size:100%;background:inherit;border:inherit;color:#515151}</style><script>window.GA_MEASUREMENT_ID="G-KKJ5ZEG1NB",window.GA_CONFIG={enableReadingTime:!0,enableScrollDepth:!0,enableOutboundLinks:!0,enableDownloads:!0,lazyLoadTimeout:3e3}</script><script defer src=https://zhurongshuo.com/js/ga-optimizer.js></script></body></html>