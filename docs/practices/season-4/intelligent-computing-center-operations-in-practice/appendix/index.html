<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><meta name=google-site-verification content="8_xpI-TS3tNV8UPug-Q6Ef3BhKTcy0WOG7dEdAcm2zk"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="祝融"><link rel=dns-prefetch href=//cdn.jsdelivr.net><link rel=preconnect href=https://cdn.jsdelivr.net crossorigin><link rel=dns-prefetch href=//www.googletagmanager.com><link rel=preconnect href=https://www.googletagmanager.com crossorigin><link rel=canonical href=https://zhurongshuo.com/practices/season-4/intelligent-computing-center-operations-in-practice/appendix/><title>祝融说。 附录</title><meta property="og:title" content="附录"><meta property="og:url" content="https://zhurongshuo.com/practices/season-4/intelligent-computing-center-operations-in-practice/appendix/"><meta property="og:site_name" content="祝融说。"><meta property="og:locale" content="zh_CN"><meta property="og:type" content="article"><meta property="article:published_time" content="2025-11-29T00:00:00+08:00"><meta property="article:modified_time" content="2025-11-29T00:00:00+08:00"><meta property="article:tag" content="书稿"><meta name=description content="附录A：实操手册 理论的光辉，终需实践的印证。本附录旨在为你提供一套端到端的、可动手操作的实验指南，将我们在正文中探讨的核心概念——从底层的驱动安装，到上层的调度、推理和监控——转化为你本地环境中一行行可执行的命令和一段段可运行的代码。
我们深知，在复杂的AI基础设施领域，环境的差异性是最大的挑战。因此，本手册在设计时，力求简化依赖、明确前提，并对关键步骤提供详尽的注解。我们鼓励你不仅要“复制-粘贴”地完成实验，更要在过程中，回顾和思考每个步骤背后的原理，将其与正文中的知识点一一对应。
"><meta property="og:description" content="附录A：实操手册 理论的光辉，终需实践的印证。本附录旨在为你提供一套端到端的、可动手操作的实验指南，将我们在正文中探讨的核心概念——从底层的驱动安装，到上层的调度、推理和监控——转化为你本地环境中一行行可执行的命令和一段段可运行的代码。
我们深知，在复杂的AI基础设施领域，环境的差异性是最大的挑战。因此，本手册在设计时，力求简化依赖、明确前提，并对关键步骤提供详尽的注解。我们鼓励你不仅要“复制-粘贴”地完成实验，更要在过程中，回顾和思考每个步骤背后的原理，将其与正文中的知识点一一对应。
"><meta property="og:image" content="https://zhurongshuo.com/images/favicon.ico"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="附录"><meta name=twitter:description content="附录A：实操手册 理论的光辉，终需实践的印证。本附录旨在为你提供一套端到端的、可动手操作的实验指南，将我们在正文中探讨的核心概念——从底层的驱动安装，到上层的调度、推理和监控——转化为你本地环境中一行行可执行的命令和一段段可运行的代码。
我们深知，在复杂的AI基础设施领域，环境的差异性是最大的挑战。因此，本手册在设计时，力求简化依赖、明确前提，并对关键步骤提供详尽的注解。我们鼓励你不仅要“复制-粘贴”地完成实验，更要在过程中，回顾和思考每个步骤背后的原理，将其与正文中的知识点一一对应。
"><meta name=twitter:image content="https://zhurongshuo.com/images/favicon.ico"><meta name=keywords content="智算中心运营实战：从基础设施到大模型全栈优化,附录"><link rel="shortcut icon" href=https://zhurongshuo.com/images/favicon.ico><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/normalize.css><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/animate-custom.css><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/zozo.css><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/remixicon-custom.css><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/highlight.css><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","headline":"附录","description":"附录A：实操手册 理论的光辉，终需实践的印证。本附录旨在为你提供一套端到端的、可动手操作的实验指南，将我们在正文中探讨的核心概念——从底层的驱动安装，到上层的调度、推理和监控——转化为你本地环境中一行行可执行的命令和一段段可运行的代码。\n我们深知，在复杂的AI基础设施领域，环境的差异性是最大的挑战。因此，本手册在设计时，力求简化依赖、明确前提，并对关键步骤提供详尽的注解。我们鼓励你不仅要“复制-粘贴”地完成实验，更要在过程中，回顾和思考每个步骤背后的原理，将其与正文中的知识点一一对应。\n","datePublished":"2025-11-29T00:00:00\u002b08:00","dateModified":"2025-11-29T00:00:00\u002b08:00","author":{"@type":"Person","name":"祝融"},"publisher":{"@type":"Organization","name":"祝融说。","logo":{"@type":"ImageObject","url":"https:\/\/zhurongshuo.com\/images\/favicon.ico"}},"image":"https:\/\/zhurongshuo.com\/images\/favicon.ico","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/zhurongshuo.com\/practices\/season-4\/intelligent-computing-center-operations-in-practice\/appendix\/"},"keywords":"书稿"}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"首页","item":"https:\/\/zhurongshuo.com\/"},{"@type":"ListItem","position":2,"name":"practices","item":"https:\/\/zhurongshuo.com\/practices/"},{"@type":"ListItem","position":3,"name":"附录","item":"https:\/\/zhurongshuo.com\/practices\/season-4\/intelligent-computing-center-operations-in-practice\/appendix\/"}]}</script></head><body><div class="main animate__animated animate__fadeInDown"><div class="nav_container animated fadeInDown"><div class=site_nav id=site_nav><ul><li><a href=https://zhurongshuo.com/>首页</a></li><li><a href=https://zhurongshuo.com/start/>开始</a></li><li><a href=https://zhurongshuo.com/advanced/>进阶rc</a></li><li><a href=https://zhurongshuo.com/posts/>归档</a></li><li><a href=https://zhurongshuo.com/tags/>标签</a></li><li><a href=https://zhurongshuo.com/about/>关于</a></li></ul></div><div class=menu_icon><a id=menu_icon><i class=ri-menu-line></i></a></div></div><div class="header animated fadeInDown"><div class=site_title_container><div class=site_title><h1><a href=https://zhurongshuo.com/><span class=web-font>祝融说。</span></a></h1></div><div class=description><p class=sub_title>法不净空，觉无性也。</p><div class=my_socials><a href=https://zhurongshuo.com/books/ title=book-open><i class=ri-book-open-line></i></a>
<a href=https://zhurongshuo.com/gallery/ title=gallery><i class=ri-gallery-line></i></a>
<a href=https://zhurongshuo.com/about/ title=game><i class=ri-game-line></i></a>
<a href=https://zhurongshuo.com/practices/ title=trophy><i class=ri-trophy-line></i></a>
<a href type=application/rss+xml title=rss target=_blank><i class=ri-rss-fill></i></a></div></div></div></div><div class=content><div class=post_page><div class="post animate__animated animate__fadeInDown"><div class="post_title post_detail_title"><h2><a href=https://zhurongshuo.com/practices/season-4/intelligent-computing-center-operations-in-practice/appendix/>附录</a></h2><span class=date>2025.11.29</span></div><div class="post_content markdown"><h2 id=附录a实操手册>附录A：实操手册</h2><p>理论的光辉，终需实践的印证。本附录旨在为你提供一套端到端的、可动手操作的实验指南，将我们在正文中探讨的核心概念——从底层的驱动安装，到上层的调度、推理和监控——转化为你本地环境中一行行可执行的命令和一段段可运行的代码。</p><p>我们深知，在复杂的AI基础设施领域，环境的差异性是最大的挑战。因此，本手册在设计时，力求简化依赖、明确前提，并对关键步骤提供详尽的注解。我们鼓励你不仅要“复制-粘贴”地完成实验，更要在过程中，回顾和思考每个步骤背后的原理，将其与正文中的知识点一一对应。</p><p>请准备好你的终端，让我们开始这场“理论联系实际”的终极演练。</p><h3 id=环境准备脚本terraformansible快速拉起实验环境>环境准备脚本：Terraform/Ansible快速拉起实验环境</h3><p>在真实的生产环境中，我们会使用Terraform来自动化云资源的创建（如VPC、虚拟机、负载均衡器），然后用Ansible来对这些虚拟机进行精细化的配置（如安装驱动、配置软件）。这是一个复杂但强大的Infrastructure as Code (IaC)流程。</p><p>由于完整的IaC脚本与特定的云厂商（AWS, Azure, GCP, 阿里云等）和你的账号配置强相关，在此提供一个通用的、可在任何云上执行的脚本是不现实的。因此，本节将提供一个概念性、模板化的Ansible Playbook，旨在向你展示自动化配置的核心逻辑。你可以将其作为起点，根据你自己的环境进行适配。</p><p>对于本地实验，我们推荐使用一台已安装好Linux（如Ubuntu 22.04）并配有NVIDIA GPU或可访问昇腾卡的物理机或虚拟机。</p><h4 id=ansible-playbook模板-用于配置gpu节点>Ansible Playbook模板 (用于配置GPU节点)</h4><p>这个Playbook展示了在一个或多个全新的Ubuntu节点上，自动化安装Docker、NVIDIA驱动、NVIDIA Container Toolkit和轻量级K8s (k3s)的流程。</p><p>前提：</p><ol><li>你有一台控制机，已安装Ansible (<code>pip install ansible</code>)。</li><li>你有一个或多个目标GPU节点，控制机可以通过SSH免密登录到这些节点。</li><li>在控制机上配置好Ansible的<code>inventory</code>文件（例如<code>/etc/ansible/hosts</code>），定义你的GPU节点组。</li></ol><div class=highlight><pre tabindex=0 class=chroma><code class=language-ini data-lang=ini><span class=line><span class=cl><span class=k>[gpu_nodes]</span>
</span></span><span class=line><span class=cl><span class=na>192.168.1.101</span>
</span></span><span class=line><span class=cl><span class=na>192.168.1.102</span>
</span></span></code></pre></div><p>Playbook文件 (<code>setup_gpu_node.yml</code>):</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nn>---</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span>- <span class=nt>hosts</span><span class=p>:</span><span class=w> </span><span class=l>gpu_nodes</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>become</span><span class=p>:</span><span class=w> </span><span class=kc>yes</span><span class=w> </span><span class=c># 以root权限执行</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>vars</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>nvidia_driver_version</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;535&#34;</span><span class=w> </span><span class=c># 指定你想安装的驱动版本</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>tasks</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=m>1</span><span class=l>. Update APT cache and install prerequisite</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>apt</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>update_cache</span><span class=p>:</span><span class=w> </span><span class=kc>yes</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=s1>&#39;apt-transport-https&#39;</span><span class=p>,</span><span class=w> </span><span class=s1>&#39;ca-certificates&#39;</span><span class=p>,</span><span class=w> </span><span class=s1>&#39;curl&#39;</span><span class=p>,</span><span class=w> </span><span class=s1>&#39;gnupg-agent&#39;</span><span class=p>,</span><span class=w> </span><span class=s1>&#39;software-properties-common&#39;</span><span class=p>]</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>state</span><span class=p>:</span><span class=w> </span><span class=l>present</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=m>2</span><span class=l>. Add Docker&#39;s official GPG key</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>apt_key</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>url</span><span class=p>:</span><span class=w> </span><span class=l>https://download.docker.com/linux/ubuntu/gpg</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>state</span><span class=p>:</span><span class=w> </span><span class=l>present</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=m>3</span><span class=l>. Add Docker repository</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>apt_repository</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>repo</span><span class=p>:</span><span class=w> </span><span class=l>deb [arch=amd64] https://download.docker.com/linux/ubuntu {{ ansible_distribution_release }} stable</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>state</span><span class=p>:</span><span class=w> </span><span class=l>present</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=m>4</span><span class=l>. Install Docker Engine</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>apt</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=s1>&#39;docker-ce&#39;</span><span class=p>,</span><span class=w> </span><span class=s1>&#39;docker-ce-cli&#39;</span><span class=p>,</span><span class=w> </span><span class=s1>&#39;containerd.io&#39;</span><span class=p>]</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>state</span><span class=p>:</span><span class=w> </span><span class=l>present</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=m>5</span><span class=l>. Add NVIDIA driver repository</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>apt_repository</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>repo</span><span class=p>:</span><span class=w> </span><span class=l>ppa:graphics-drivers/ppa</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>state</span><span class=p>:</span><span class=w> </span><span class=l>present</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=m>6</span><span class=l>. Install NVIDIA Driver</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>apt</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;nvidia-driver-{{ nvidia_driver_version }}&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>state</span><span class=p>:</span><span class=w> </span><span class=l>present</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>register</span><span class=p>:</span><span class=w> </span><span class=l>driver_install</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>notify</span><span class=p>:</span><span class=w> </span><span class=l>Reboot node</span><span class=w> </span><span class=c># 驱动安装后通常需要重启</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=m>7</span><span class=l>. Add NVIDIA Container Toolkit repository</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>shell</span><span class=p>:</span><span class=w> </span><span class=p>|</span><span class=sd>
</span></span></span><span class=line><span class=cl><span class=sd>        curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
</span></span></span><span class=line><span class=cl><span class=sd>        curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
</span></span></span><span class=line><span class=cl><span class=sd>        sed &#39;s#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g&#39; | \
</span></span></span><span class=line><span class=cl><span class=sd>        tee /etc/apt/sources.list.d/nvidia-container-toolkit.list</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=m>8</span><span class=l>. Install NVIDIA Container Toolkit</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>apt</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>update_cache</span><span class=p>:</span><span class=w> </span><span class=kc>yes</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>nvidia-container-toolkit</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>state</span><span class=p>:</span><span class=w> </span><span class=l>present</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=m>9</span><span class=l>. Configure Docker to use NVIDIA runtime and restart</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>shell</span><span class=p>:</span><span class=w> </span><span class=p>|</span><span class=sd>
</span></span></span><span class=line><span class=cl><span class=sd>        nvidia-ctk runtime configure --runtime=docker
</span></span></span><span class=line><span class=cl><span class=sd>        systemctl restart docker</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=m>10</span><span class=l>. Install k3s (Lightweight Kubernetes)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>shell</span><span class=p>:</span><span class=w> </span><span class=p>|</span><span class=sd>
</span></span></span><span class=line><span class=cl><span class=sd>        curl -sfL https://get.k3s.io | sh -
</span></span></span><span class=line><span class=cl><span class=sd>        mkdir -p $HOME/.kube
</span></span></span><span class=line><span class=cl><span class=sd>        cp /etc/rancher/k3s/k3s.yaml $HOME/.kube/config
</span></span></span><span class=line><span class=cl><span class=sd>        chown $(id -u):$(id -g) $HOME/.kube/config</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>args</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>creates</span><span class=p>:</span><span class=w> </span><span class=l>/usr/local/bin/k3s</span><span class=w> </span><span class=c># 避免重复安装</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>handlers</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Reboot node</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>reboot</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>msg</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;Rebooting node after NVIDIA driver installation&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>connect_timeout</span><span class=p>:</span><span class=w> </span><span class=m>5</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>reboot_timeout</span><span class=p>:</span><span class=w> </span><span class=m>300</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>pre_reboot_delay</span><span class=p>:</span><span class=w> </span><span class=m>0</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>post_reboot_delay</span><span class=p>:</span><span class=w> </span><span class=m>30</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>test_command</span><span class=p>:</span><span class=w> </span><span class=l>whoami</span><span class=w>
</span></span></span></code></pre></div><p>如何运行：</p><p>在你的控制机上执行：<code>ansible-playbook -i /etc/ansible/hosts setup_gpu_node.yml</code></p><p>这个Playbook会自动完成所有节点的标准化配置，为后续的K8s实验打下坚实的基础。</p><h3 id=lab-1---基础环境手把手教你安装昇腾驱动与cann软件栈>Lab 1 - 基础环境：手把手教你安装昇腾驱动与CANN软件栈</h3><p>本实验将指导你在一个配置有华为昇腾AI处理器的服务器上，完成最基础、也最关键的软件栈安装。</p><p>前提：</p><ul><li>服务器已安装了受支持的操作系统（如EulerOS或指定的Ubuntu版本）。</li><li>你拥有服务器的root或sudo权限。</li><li>你已经从华为官网或镜像源下载了对应硬件型号和OS版本的<code>AIA-Ascend-Driver-*.run</code>和<code>AIA-Ascend-Toolkit-*.run</code>两个文件。</li></ul><p>步骤：</p><p>Step 1: 检查硬件与环境</p><p>在安装前，确认系统能识别到昇腾设备。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>lspci <span class=p>|</span> grep -i ascend
</span></span></code></pre></div><p>你应该能看到类似<code>Processing accelerators: Huawei Technologies Co., Ltd. Ascend 910 AI Processor</code>的输出。</p><p>Step 2: 安装驱动</p><p>驱动是连接操作系统内核与NPU硬件的桥梁。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># 赋予执行权限</span>
</span></span><span class=line><span class=cl>chmod +x AIA-Ascend-Driver-*.run
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 以root权限执行安装</span>
</span></span><span class=line><span class=cl>sudo ./AIA-Ascend-Driver-*.run --install
</span></span></code></pre></div><p>安装过程中，请仔细阅读提示。安装完成后，通常需要重启服务器以加载新的内核模块。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>sudo reboot
</span></span></code></pre></div><p>Step 3: 安装CANN工具包</p><p>CANN (Compute Architecture for Neural Networks) 是昇腾的应用使能软件栈，包含了编译器、加速库、工具链等。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># 赋予执行权限</span>
</span></span><span class=line><span class=cl>chmod +x AIA-Ascend-Toolkit-*.run
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 以root权限执行安装</span>
</span></span><span class=line><span class=cl><span class=c1># --install-path指定安装路径，--install-for-all表示为所有用户安装</span>
</span></span><span class=line><span class=cl>sudo ./AIA-Ascend-Toolkit-*.run --install --install-path<span class=o>=</span>/usr/local/ascend --install-for-all
</span></span></code></pre></div><p>Step 4: 配置环境变量</p><p>为了让系统能够找到CANN的命令和库，需要将相关路径添加到环境变量中。CANN的安装包非常贴心地提供了一个脚本来做这件事。
将以下行添加到你的<code>~/.bashrc</code>或系统的<code>/etc/profile</code>中：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># 编辑.bashrc文件</span>
</span></span><span class=line><span class=cl>vim ~/.bashrc
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 在文件末尾添加以下行 (路径请根据你的实际安装路径修改)</span>
</span></span><span class=line><span class=cl><span class=nb>source</span> /usr/local/ascend/ascend-toolkit/set_env.sh
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 使配置立即生效</span>
</span></span><span class=line><span class=cl><span class=nb>source</span> ~/.bashrc
</span></span></code></pre></div><p>Step 5: 验证安装</p><p>这是最关键的一步，验证我们的安装是否成功。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># 执行npu-smi命令</span>
</span></span><span class=line><span class=cl>npu-smi info
</span></span></code></pre></div><p>如果安装成功，你将看到类似以下的输出，详细列出了服务器上所有NPU卡的信息，包括型号、ID、温度、功耗、HBM使用率等。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>+-------------------------------------------------------------------------------------------+
</span></span><span class=line><span class=cl>| npu-smi 21.0.2                  Version: 21.0.2                                           |
</span></span><span class=line><span class=cl>+-------------------------------+-----------------+-----------------------------------------+
</span></span><span class=line><span class=cl>| NPU     Name                  | Health          | Power(W)          Temp(C)               |
</span></span><span class=line><span class=cl>| Chip    Device-Chip-Id        | Bus-Id          | AICore(%)         HBM(MB)               |
</span></span><span class=line><span class=cl>+===============================+=================+=========================================+
</span></span><span class=line><span class=cl>| 0       Ascend 910B           | OK              | 110.0             45                    |
</span></span><span class=line><span class=cl>| 0       0-0                   | 0000:C1:00.0    | 0                 0 / 32768             |
</span></span><span class=line><span class=cl>+-------------------------------+-----------------+-----------------------------------------+
</span></span><span class=line><span class=cl>| 1       Ascend 910B           | OK              | 108.0             44                    |
</span></span><span class=line><span class=cl>| 0       1-0                   | 0000:C2:00.0    | 0                 0 / 32768             |
</span></span><span class=line><span class=cl>+-------------------------------+-----------------+-----------------------------------------+
</span></span><span class=line><span class=cl>... (其他NPU卡)
</span></span></code></pre></div><p>看到这个界面，恭喜你，你已经为昇腾AI处理器构建了最基础的软件运行环境！</p><h3 id=lab-2---调度实战在k8s中配置volcano并提交一个分布式pytorch-job>Lab 2 - 调度实战：在K8s中配置Volcano，并提交一个分布式PyTorch Job</h3><p>本实验将带你实践第5章的核心内容：使用Volcano调度器来解决原生K8s无法处理的分布式训练死锁问题。</p><p>前提：</p><ul><li>你有一个可用的K8s集群，并且集群中的节点已经配置好了GPU/NPU支持（即Device Plugin已安装）。</li><li>你已经安装了Helm客户端。</li></ul><p>Step 1: 安装Volcano</p><p>使用Helm可以一键安装Volcano。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>helm repo add volcano-sh https://volcano-sh.github.io/charts
</span></span><span class=line><span class=cl>helm repo update
</span></span><span class=line><span class=cl>helm install volcano volcano-sh/volcano -n volcano-system --create-namespace
</span></span></code></pre></div><p>验证安装：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>kubectl get pods -n volcano-system
</span></span></code></pre></div><p>你应该能看到<code>volcano-scheduler</code>, <code>volcano-controller</code>等Pod处于<code>Running</code>状态。</p><p>Step 2: 准备PyTorch分布式训练应用</p><p>我们将使用一个经典的PyTorch MNIST分布式训练脚本。</p><p>文件 (<code>mnist_distributed.py</code>):</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.distributed</span> <span class=k>as</span> <span class=nn>dist</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.optim</span> <span class=k>as</span> <span class=nn>optim</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torch.nn.parallel</span> <span class=kn>import</span> <span class=n>DistributedDataParallel</span> <span class=k>as</span> <span class=n>DDP</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torchvision</span> <span class=kn>import</span> <span class=n>datasets</span><span class=p>,</span> <span class=n>transforms</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>os</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>setup</span><span class=p>(</span><span class=n>rank</span><span class=p>,</span> <span class=n>world_size</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s1>&#39;MASTER_ADDR&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s1>&#39;MASTER_ADDR&#39;</span><span class=p>,</span> <span class=s1>&#39;localhost&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s1>&#39;MASTER_PORT&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s1>&#39;MASTER_PORT&#39;</span><span class=p>,</span> <span class=s1>&#39;12355&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>dist</span><span class=o>.</span><span class=n>init_process_group</span><span class=p>(</span><span class=s2>&#34;nccl&#34;</span><span class=p>,</span> <span class=n>rank</span><span class=o>=</span><span class=n>rank</span><span class=p>,</span> <span class=n>world_size</span><span class=o>=</span><span class=n>world_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>cleanup</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=n>dist</span><span class=o>.</span><span class=n>destroy_process_group</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>Net</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># ... (一个简单的CNN模型定义)</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>Net</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>conv1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>32</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>conv2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>32</span><span class=p>,</span> <span class=mi>64</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>fc1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>9216</span><span class=p>,</span> <span class=mi>128</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>fc2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>128</span><span class=p>,</span> <span class=mi>10</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>conv1</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>conv2</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>max_pool2d</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>flatten</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc1</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc2</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>torch</span><span class=o>.</span><span class=n>log_softmax</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>train</span><span class=p>(</span><span class=n>rank</span><span class=p>,</span> <span class=n>world_size</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>setup</span><span class=p>(</span><span class=n>rank</span><span class=p>,</span> <span class=n>world_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>    <span class=n>device</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>device</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;cuda:</span><span class=si>{</span><span class=n>rank</span> <span class=o>%</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>device_count</span><span class=p>()</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>    <span class=n>transform</span> <span class=o>=</span> <span class=n>transforms</span><span class=o>.</span><span class=n>Compose</span><span class=p>([</span>
</span></span><span class=line><span class=cl>        <span class=n>transforms</span><span class=o>.</span><span class=n>ToTensor</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>        <span class=n>transforms</span><span class=o>.</span><span class=n>Normalize</span><span class=p>((</span><span class=mf>0.1307</span><span class=p>,),</span> <span class=p>(</span><span class=mf>0.3081</span><span class=p>,))</span>
</span></span><span class=line><span class=cl>    <span class=p>])</span>
</span></span><span class=line><span class=cl>    <span class=n>dataset</span> <span class=o>=</span> <span class=n>datasets</span><span class=o>.</span><span class=n>MNIST</span><span class=p>(</span><span class=s1>&#39;../data&#39;</span><span class=p>,</span> <span class=n>train</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>download</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>transform</span><span class=o>=</span><span class=n>transform</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>train_sampler</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>distributed</span><span class=o>.</span><span class=n>DistributedSampler</span><span class=p>(</span><span class=n>dataset</span><span class=p>,</span> <span class=n>num_replicas</span><span class=o>=</span><span class=n>world_size</span><span class=p>,</span> <span class=n>rank</span><span class=o>=</span><span class=n>rank</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>train_loader</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>DataLoader</span><span class=p>(</span><span class=n>dataset</span><span class=p>,</span> <span class=n>batch_size</span><span class=o>=</span><span class=mi>64</span><span class=p>,</span> <span class=n>sampler</span><span class=o>=</span><span class=n>train_sampler</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>model</span> <span class=o>=</span> <span class=n>Net</span><span class=p>()</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>ddp_model</span> <span class=o>=</span> <span class=n>DDP</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>device_ids</span><span class=o>=</span><span class=p>[</span><span class=n>device</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>optimizer</span> <span class=o>=</span> <span class=n>optim</span><span class=o>.</span><span class=n>SGD</span><span class=p>(</span><span class=n>ddp_model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.01</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>3</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>train_sampler</span><span class=o>.</span><span class=n>set_epoch</span><span class=p>(</span><span class=n>epoch</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>batch_idx</span><span class=p>,</span> <span class=p>(</span><span class=n>data</span><span class=p>,</span> <span class=n>target</span><span class=p>)</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>train_loader</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>data</span><span class=p>,</span> <span class=n>target</span> <span class=o>=</span> <span class=n>data</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>),</span> <span class=n>target</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
</span></span><span class=line><span class=cl>            <span class=n>output</span> <span class=o>=</span> <span class=n>ddp_model</span><span class=p>(</span><span class=n>data</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>loss</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>functional</span><span class=o>.</span><span class=n>nll_loss</span><span class=p>(</span><span class=n>output</span><span class=p>,</span> <span class=n>target</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>            <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=n>batch_idx</span> <span class=o>%</span> <span class=mi>10</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Rank </span><span class=si>{</span><span class=n>rank</span><span class=si>}</span><span class=s2>, Epoch </span><span class=si>{</span><span class=n>epoch</span><span class=si>}</span><span class=s2>, Batch </span><span class=si>{</span><span class=n>batch_idx</span><span class=si>}</span><span class=s2>, Loss: </span><span class=si>{</span><span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>()</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>    <span class=n>cleanup</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=vm>__name__</span> <span class=o>==</span> <span class=s1>&#39;__main__&#39;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=c1># Volcano会注入VC_TASK_INDEX和VC_WORKER_NUM环境变量</span>
</span></span><span class=line><span class=cl>    <span class=n>rank</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;VC_TASK_INDEX&#34;</span><span class=p>,</span> <span class=s2>&#34;0&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>world_size</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;VC_WORKER_NUM&#34;</span><span class=p>,</span> <span class=s2>&#34;1&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Starting training on Rank </span><span class=si>{</span><span class=n>rank</span><span class=si>}</span><span class=s2> of </span><span class=si>{</span><span class=n>world_size</span><span class=si>}</span><span class=s2>...&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>train</span><span class=p>(</span><span class=n>rank</span><span class=p>,</span> <span class=n>world_size</span><span class=p>)</span>
</span></span></code></pre></div><p>Step 3: 构建Docker镜像</p><p>文件 (<code>Dockerfile</code>):</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-dockerfile data-lang=dockerfile><span class=line><span class=cl><span class=k>FROM</span><span class=w> </span><span class=s>pytorch/pytorch:2.1.0-cuda12.1-cudnn8-runtime</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>WORKDIR</span><span class=w> </span><span class=s>/app</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>COPY</span> mnist_distributed.py .<span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># 下载数据集，避免在运行时下载</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>RUN</span> python -c <span class=s2>&#34;from torchvision import datasets; datasets.MNIST(&#39;../data&#39;, download=True)&#34;</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>CMD</span> <span class=p>[</span><span class=s2>&#34;python&#34;</span><span class=p>,</span> <span class=s2>&#34;mnist_distributed.py&#34;</span><span class=p>]</span><span class=err>
</span></span></span></code></pre></div><p>构建并推送到你的镜像仓库：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>docker build -t your-registry/pytorch-dist-mnist:v1 .
</span></span><span class=line><span class=cl>docker push your-registry/pytorch-dist-mnist:v1
</span></span></code></pre></div><p>Step 4: 编写并提交VolcanoJob</p><p>这是核心步骤，我们定义一个需要2个Pod（每个Pod 1张GPU）的分布式作业。</p><p>文件 (<code>volcano_pytorch_job.yml</code>):</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>batch.volcano.sh/v1alpha1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Job</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>pytorch-mnist-dist-job</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>schedulerName</span><span class=p>:</span><span class=w> </span><span class=l>volcano</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>minAvailable</span><span class=p>:</span><span class=w> </span><span class=m>2</span><span class=w> </span><span class=c># 核心：Gang Scheduling，必须凑齐2个Pod才能开始</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>queue</span><span class=p>:</span><span class=w> </span><span class=l>default</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>tasks</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>worker</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>replicas</span><span class=p>:</span><span class=w> </span><span class=m>2</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>template</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>containers</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>pytorch-worker</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>your-registry/pytorch-dist-mnist:v1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>              </span><span class=nt>resources</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>                </span><span class=nt>limits</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>                  </span><span class=nt>nvidia.com/gpu</span><span class=p>:</span><span class=w> </span><span class=m>1</span><span class=w> </span><span class=c># 每个Pod申请1张GPU</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>restartPolicy</span><span class=p>:</span><span class=w> </span><span class=l>OnFailure</span><span class=w>
</span></span></span></code></pre></div><p>提交作业：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>kubectl apply -f volcano_pytorch_job.yml
</span></span></code></pre></div><p>Step 5: 观察与验证</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># 查看PodGroup状态，Volcano调度的核心对象</span>
</span></span><span class=line><span class=cl>kubectl get podgroup
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 查看Pod状态，你会看到两个Pod几乎是同时被创建并进入Running状态</span>
</span></span><span class=line><span class=cl>kubectl get pods -l volcanosh.dev/job-name<span class=o>=</span>pytorch-mnist-dist-job
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 查看其中一个Pod的日志</span>
</span></span><span class=line><span class=cl>kubectl logs pytorch-mnist-dist-job-worker-0
</span></span></code></pre></div><p>在日志中，你将看到来自不同Rank（Rank 0和Rank 1）的训练日志交错打印，这表明分布式训练已成功建立并运行！</p><h3 id=lab-3---压测实战部署vllm服务使用脚本模拟100并发生成性能报告>Lab 3 - 压测实战：部署vLLM服务，使用脚本模拟100并发，生成性能报告</h3><p>本实验将带你体验第8章的推理服务部署与压测。</p><p>前提：</p><ul><li>一台或多台配备NVIDIA GPU的机器，已安装好驱动和Docker。</li><li>Python环境已安装。</li></ul><p>Step 1: 部署vLLM服务</p><p>我们将以最简单的方式，使用vLLM官方提供的Docker镜像来部署一个Llama 3 8B Instruct模型的服务。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># 拉取vLLM镜像</span>
</span></span><span class=line><span class=cl>docker pull vllm/vllm-openai:latest
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 启动服务</span>
</span></span><span class=line><span class=cl><span class=c1># 注意：这需要较好的网络来下载模型，模型会被缓存到~/.cache/huggingface</span>
</span></span><span class=line><span class=cl>docker run --gpus all -d <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -p 8000:8000 <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -v ~/.cache/huggingface:/root/.cache/huggingface <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --name vllm_server <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  vllm/vllm-openai:latest <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --model meta-llama/Meta-Llama-3-8B-Instruct
</span></span></code></pre></div><p>注意： Llama 3模型需要Hugging Face的访问授权。请确保你已登录HF并接受了其使用协议。</p><p>Step 2: 验证服务</p><p>使用<code>curl</code>测试服务是否正常工作。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>curl http://localhost:8000/v1/chat/completions <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    -H <span class=s2>&#34;Content-Type: application/json&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    -d <span class=s1>&#39;{
</span></span></span><span class=line><span class=cl><span class=s1>        &#34;model&#34;: &#34;meta-llama/Meta-Llama-3-8B-Instruct&#34;,
</span></span></span><span class=line><span class=cl><span class=s1>        &#34;messages&#34;: [
</span></span></span><span class=line><span class=cl><span class=s1>            {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Hello! What is your name?&#34;}
</span></span></span><span class=line><span class=cl><span class=s1>        ]
</span></span></span><span class=line><span class=cl><span class=s1>    }&#39;</span>
</span></span></code></pre></div><p>如果返回了模型的回答，则服务部署成功。</p><p>Step 3: 准备Locust压测脚本</p><p>安装Locust：<code>pip install locust</code></p><p>文件 (<code>locustfile.py</code>): (参考8.3节提供的脚本，这里提供一个非流式的简化版，便于快速上手)</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>locust</span> <span class=kn>import</span> <span class=n>task</span><span class=p>,</span> <span class=n>HttpUser</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>random</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>LLMUser</span><span class=p>(</span><span class=n>HttpUser</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=nd>@task</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>generate</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>prompts</span> <span class=o>=</span> <span class=p>[</span><span class=s2>&#34;What is the capital of France?&#34;</span><span class=p>,</span> <span class=s2>&#34;Write a short poem about the sea.&#34;</span><span class=p>,</span> <span class=s2>&#34;Explain black holes to a 5-year-old.&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>payload</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;model&#34;</span><span class=p>:</span> <span class=s2>&#34;meta-llama/Meta-Llama-3-8B-Instruct&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;messages&#34;</span><span class=p>:</span> <span class=p>[{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;user&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=n>random</span><span class=o>.</span><span class=n>choice</span><span class=p>(</span><span class=n>prompts</span><span class=p>)}],</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;max_tokens&#34;</span><span class=p>:</span> <span class=mi>100</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>client</span><span class=o>.</span><span class=n>post</span><span class=p>(</span><span class=s2>&#34;/v1/chat/completions&#34;</span><span class=p>,</span> <span class=n>json</span><span class=o>=</span><span class=n>payload</span><span class=p>,</span> <span class=n>name</span><span class=o>=</span><span class=s2>&#34;/v1/chat/completions&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p>Step 4: 启动压测</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>locust -f locustfile.py --host http://localhost:8000
</span></span></code></pre></div><p>Step 5: 分析性能报告</p><ol><li>打开浏览器访问 <code>http://localhost:8089</code>。</li><li>输入并发用户数（Total users）为 <code>100</code>，增长速率（Spawn rate）为 <code>10</code>。</li><li>点击 "Start swarming"。</li><li>观察 "Charts" 标签页：<ul><li>Total Requests per Second (RPS): 你的服务每秒能完成多少次请求。</li><li>Response Time (ms): 响应时间的分布，重点关注95%和99%百分位值。这代表了绝大多数用户的体验。</li></ul></li><li>生成报告： 在 "Download Data" 标签页，你可以下载到详细的CSV格式报告，用于离线分析和存档。</li></ol><p>通过调整并发用户数，你可以找到服务的“性能拐点”——即在哪个并发水平上，响应时间开始急剧恶化。这为你进行容量规划提供了关键数据。</p><h3 id=lab-4---监控告警配置prometheus规则当gpu温度80或显存使用率95时触发告警>Lab 4 - 监控告警：配置Prometheus规则，当GPU温度>80℃或显存使用率>95%时触发告警</h3><p>本实验将带你实践第9章的可观测性内容，为你的GPU集群配置核心的硬件告警。</p><p>前提：</p><ul><li>一个K8s集群，已部署<a href=https://prometheus-operator.dev/>Prometheus Operator</a>（通常通过kube-prometheus-stack Helm chart安装）。</li><li>集群的GPU节点上已部署了DCGM-Exporter。</li></ul><p>Step 1: 理解PrometheusRule CRD</p><p>在Prometheus Operator生态中，告警规则是通过一个名为<code>PrometheusRule</code>的Kubernetes自定义资源来定义的。我们将创建一个YAML文件来定义我们的规则。</p><p>Step 2: 编写告警规则YAML文件</p><p>文件 (<code>gpu-alerts.yml</code>):</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>monitoring.coreos.com/v1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>PrometheusRule</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>gpu-alerts</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>labels</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=c># 这个label需要与你的Prometheus实例的ruleSelector匹配</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>prometheus</span><span class=p>:</span><span class=w> </span><span class=l>kube-prometheus </span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>role</span><span class=p>:</span><span class=w> </span><span class=l>alert-rules</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>groups</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>gpu.rules</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>rules</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span>- <span class=nt>alert</span><span class=p>:</span><span class=w> </span><span class=l>GPUTemperatureHigh</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>expr</span><span class=p>:</span><span class=w> </span><span class=l>DCGM_FI_DEV_GPU_TEMP &gt; 80</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>for</span><span class=p>:</span><span class=w> </span><span class=l>5m</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>labels</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=nt>severity</span><span class=p>:</span><span class=w> </span><span class=l>warning</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>annotations</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=nt>summary</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;GPU high temperature on {{ $labels.nodename }}&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=nt>description</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;GPU {{ $labels.gpu }} on node {{ $labels.nodename }} has been over 80°C for 5 minutes. Current value is {{ $value }}°C.&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span>- <span class=nt>alert</span><span class=p>:</span><span class=w> </span><span class=l>GPUMemoryHigh</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>expr</span><span class=p>:</span><span class=w> </span><span class=l>(DCGM_FI_DEV_FB_USED / DCGM_FI_DEV_FB_TOTAL) * 100 &gt; 95</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>for</span><span class=p>:</span><span class=w> </span><span class=l>10m</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>labels</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=nt>severity</span><span class=p>:</span><span class=w> </span><span class=l>critical</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>annotations</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=nt>summary</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;GPU high memory usage on {{ $labels.nodename }}&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=nt>description</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;GPU {{ $labels.gpu }} on node {{ $labels.nodename }} memory usage has been over 95% for 10 minutes. Current usage is {{ $value | printf `%.2f` }}%.&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span>- <span class=nt>alert</span><span class=p>:</span><span class=w> </span><span class=l>GPUXidErrorDetected</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>expr</span><span class=p>:</span><span class=w> </span><span class=l>rate(DCGM_FI_DEV_XID_ERRORS[5m]) &gt; 0</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>for</span><span class=p>:</span><span class=w> </span><span class=l>1m</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>labels</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=nt>severity</span><span class=p>:</span><span class=w> </span><span class=l>critical</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>annotations</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=nt>summary</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;GPU XID Error Detected on {{ $labels.nodename }}&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=nt>description</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;GPU {{ $labels.gpu }} on node {{ $labels.nodename }} is reporting XID errors. This may indicate a software or hardware issue. Please investigate dmesg logs.&#34;</span><span class=w>
</span></span></span></code></pre></div><p>注解：</p><ul><li><code>expr</code>: PromQL查询表达式，是告警的触发条件。</li><li><code>for</code>: 条件需要持续为真的时间，防止瞬时抖动引发的误报。</li><li><code>labels.severity</code>: 定义告警级别，便于在Alertmanager中进行不同的路由。</li><li><code>annotations</code>: 定义告警的详细信息，<code>{{ $labels... }}</code>和<code>{{ $value }}</code>是模板变量，会在告警通知中被替换为实际值。</li></ul><p>Step 3: 应用规则并验证</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># 应用规则到你的K8s集群 (通常在monitoring命名空间)</span>
</span></span><span class=line><span class=cl>kubectl apply -f gpu-alerts.yml -n monitoring
</span></span></code></pre></div><p>几分钟后，Prometheus会加载这些新规则。</p><ol><li>打开Prometheus UI（通过<code>kubectl port-forward</code>）。</li><li>导航到 "Alerts" 页面，你应该能看到新添加的<code>GPUTemperatureHigh</code>, <code>GPUMemoryHigh</code>, <code>GPUXidErrorDetected</code>三条规则，状态为<code>Inactive</code>。</li></ol><p>Step 4: 模拟触发告警（可选但推荐）</p><p>为了验证告警通路是通的，我们需要人为地触发条件。</p><ul><li>触发温度告警： 在一个GPU节点上，运行一个高强度的GPU压力测试工具，如<code>gpu-burn</code>。</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># 在GPU节点上运行</span>
</span></span><span class=line><span class=cl>docker run --rm --gpus all -it wshuyi/gpu-burn -t <span class=m>600</span> <span class=c1># 运行10分钟</span>
</span></span></code></pre></div><p>同时在另一个终端观察<code>nvidia-smi</code>，当温度超过80℃并持续5分钟后，Prometheus中的告警状态会变为<code>Pending</code>，然后是<code>Firing</code>。</p><ul><li>触发显存告警： 编写一个简单的PyTorch脚本，分配一个巨大的Tensor。</li></ul><p>Step 5: 查看告警</p><p>当告警<code>Firing</code>时，如果你配置了Alertmanager，你将在配置好的通知渠道（如Slack、钉钉）中收到一条格式化好的告警信息，其中包含了你在<code>annotations</code>中定义的详细内容。</p><p>至此，你已经成功地为你的AI基础设施安装了最基础的“自动报警器”！</p><p>好的，我们来完成最后一个附录——效率工具箱。这个附录将提供一些立即可用的、高度实用的工具，它们是前面章节中复杂公式和排查逻辑的浓缩与自动化。这些工具旨在将你从重复性的计算和检查中解放出来，让你能更专注于架构设计和战略规划。</p><h2 id=附录b效率工具箱>附录B：效率工具箱</h2><p>在AI基础设施的宏大工程中，我们不仅需要有体系化的知识和深刻的洞察力，还需要一套能将这些智慧快速转化为行动的“利器”。正如优秀的程序员不会重复造轮子，资深的AI Infra工程师也应该善于将重复性的工作自动化、工具化。</p><p>本附录为你提供了一个“效率工具箱”，其中包含了两件我们精心打造的工具。第一件是《智算中心算力资源规划计算器》，它将第七章中复杂的数学公式封装成一个简单易用的Excel表格，让你能在几秒钟内完成一个价值数亿元的集群规划估算。第二件是Python运维脚本库，它提供了一系列即插即用的脚本，能帮你一键完成集群的健康巡检、算力需求的快速计算等日常任务。</p><p>这些工具是你从“理论家”变为“实干家”的加速器。请将它们收藏到你的“武器库”中，并在日常工作中不断地打磨和扩展它们，使其成为你个人知识体系中最锋利、最得心应手的一部分。</p><h3 id=智算中心算力资源规划计算器xlsx>《智算中心算力资源规划计算器.xlsx》</h3><p>这个Excel计算器旨在将第七章中关于显存占用和训练时间的估算模型，转化为一个交互式的、可视化的规划工具。你只需要在“输入”区域填入模型的关键参数和你的业务目标，它就能自动在“输出”区域为你计算出所需的资源规模和相关的性能指标。</p><h4 id=计算器结构设计>计算器结构设计</h4><p>我们将这个Excel文件设计为包含两个主要的工作表（Sheet）：<code>Training_Estimator</code>（训练资源估算器）和<code>Inference_Memory_Estimator</code>（推理显存估算器）。</p><h4 id=sheet-1-training_estimator-训练资源估算器>Sheet 1: Training_Estimator (训练资源估算器)</h4><p>这个工作表是整个计算器的核心，用于预训练或全量微调的资源规划。</p><p>【输入区 (Input Area)】 - 黄色背景单元格</p><p>A. 模型参数 (Model Parameters)</p><table><thead><tr><th style=text-align:left>参数名</th><th style=text-align:left>符号</th><th style=text-align:left>示例值</th><th style=text-align:left>单位</th><th style=text-align:left>备注</th></tr></thead><tbody><tr><td style=text-align:left>模型参数量</td><td style=text-align:left>P</td><td style=text-align:left>70</td><td style=text-align:left>B (十亿)</td><td style=text-align:left>如Llama 2 70B，填70</td></tr><tr><td style=text-align:left>模型层数</td><td style=text-align:left>L</td><td style=text-align:left>80</td><td style=text-align:left></td><td style=text-align:left></td></tr><tr><td style=text-align:left>隐藏层维度</td><td style=text-align:left>h</td><td style=text-align:left>8192</td><td style=text-align:left></td><td style=text-align:left></td></tr><tr><td style=text-align:left>注意力头数</td><td style=text-align:left>a</td><td style=text-align:left>64</td><td style=text-align:left></td><td style=text-align:left></td></tr><tr><td style=text-align:left>序列长度</td><td style=text-align:left>s</td><td style=text-align:left>4096</td><td style=text-align:left>Tokens</td><td style=text-align:left></td></tr></tbody></table><p>B. 数据与目标 (Data & Goals)</p><table><thead><tr><th style=text-align:left>参数名</th><th style=text-align:left>符号</th><th style=text-align:left>示例值</th><th style=text-align:left>单位</th><th style=text-align:left>备注</th></tr></thead><tbody><tr><td style=text-align:left>训练数据量</td><td style=text-align:left>D</td><td style=text-align:left>2</td><td style=text-align:left>T (万亿)</td><td style=text-align:left>Tokens</td></tr><tr><td style=text-align:left>目标训练天数</td><td style=text-align:left>Days</td><td style=text-align:left>90</td><td style=text-align:left>天</td><td style=text-align:left></td></tr></tbody></table><p>C. 硬件与并行策略 (Hardware & Parallelism)</p><table><thead><tr><th style=text-align:left>参数名</th><th style=text-align:left>符号</th><th style=text-align:left>示例值</th><th style=text-align:left>单位</th><th style=text-align:left>备注</th></tr></thead><tbody><tr><td style=text-align:left>GPU型号</td><td style=text-align:left>-</td><td style=text-align:left>A100-80G</td><td style=text-align:left></td><td style=text-align:left>(下拉菜单选择)</td></tr><tr><td style=text-align:left>单卡理论算力</td><td style=text-align:left>Peak</td><td style=text-align:left>312</td><td style=text-align:left>TFLOPS (FP16)</td><td style=text-align:left>(根据GPU型号自动填充)</td></tr><tr><td style=text-align:left>单卡显存容量</td><td style=text-align:left>Mem_GPU</td><td style=text-align:left>80</td><td style=text-align:left>GB</td><td style=text-align:left>(根据GPU型号自动填充)</td></tr><tr><td style=text-align:left>数据并行路数</td><td style=text-align:left>DP</td><td style=text-align:left>64</td><td style=text-align:left></td><td style=text-align:left></td></tr><tr><td style=text-align:left>张量并行路数</td><td style=text-align:left>TP</td><td style=text-align:left>8</td><td style=text-align:left></td><td style=text-align:left></td></tr><tr><td style=text-align:left>流水线并行路数</td><td style=text-align:left>PP</td><td style=text-align:left>8</td><td style=text-align:left></td><td style=text-align:left></td></tr><tr><td style=text-align:left>全局批次大小</td><td style=text-align:left>GBS</td><td style=text-align:left>1024</td><td style=text-align:left></td><td style=text-align:left>全局Batch Size</td></tr><tr><td style=text-align:left>MFU (算力利用率)</td><td style=text-align:left>MFU</td><td style=text-align:left>40%</td><td style=text-align:left>%</td><td style=text-align:left>最关键的经验值</td></tr></tbody></table><p>【输出区 (Output Area)】 - 绿色背景单元格</p><p>A. 训练时间与算力需求 (Time & Compute Requirements)</p><table><thead><tr><th style=text-align:left>指标名</th><th style=text-align:left>公式 (Excel表达式)</th><th style=text-align:left>结果</th><th style=text-align:left>单位</th></tr></thead><tbody><tr><td style=text-align:left>总计算量 (FLOPs)</td><td style=text-align:left><code>=6 * P * 10^9 * D * 10^12</code></td><td style=text-align:left>8.40E+23</td><td style=text-align:left>FLOPs</td></tr><tr><td style=text-align:left>所需总有效算力</td><td style=text-align:left><code>=A2 / (Days * 24 * 3600)</code></td><td style=text-align:left>1.08E+17</td><td style=text-align:left>FLOPS</td></tr><tr><td style=text-align:left></td><td style=text-align:left><code>=B2 / 10^12</code></td><td style=text-align:left>107,527</td><td style=text-align:left>TFLOPS (Effective)</td></tr><tr><td style=text-align:left>所需总GPU卡数</td><td style=text-align:left><code>=B3 / (Peak * MFU)</code></td><td style=text-align:left>862</td><td style=text-align:left>张</td></tr><tr><td style=text-align:left>建议集群规模</td><td style=text-align:left><code>=ROUNDUP(B4, -2)</code></td><td style=text-align:left>900</td><td style=text-align:left>张</td></tr><tr><td style=text-align:left>实际预估训练天数</td><td style=text-align:left><code>=(A2 / (B5 * Peak * MFU * 10^12)) / (24*3600)</code></td><td style=text-align:left>86.4</td><td style=text-align:left>天</td></tr></tbody></table><p>B. 显存占用分析 (Memory Analysis) - <em>基于ZeRO-1 + TP + PP</em></p><table><thead><tr><th style=text-align:left>指标名</th><th style=text-align:left>公式 (Excel表达式)</th><th style=text-align:left>结果</th><th style=text-align:left>单位</th></tr></thead><tbody><tr><td style=text-align:left>模型参数占用 (单卡)</td><td style=text-align:left><code>=2 * P / TP</code></td><td style=text-align:left>17.5</td><td style=text-align:left>GB</td></tr><tr><td style=text-align:left>优化器状态 (单卡)</td><td style=text-align:left><code>=12 * P / (DP * TP)</code></td><td style=text-align:left>1.64</td><td style=text-align:left>GB</td></tr><tr><td style=text-align:left>梯度占用 (单卡)</td><td style=text-align:left><code>=4 * P / (DP * TP)</code></td><td style=text-align:left>0.55</td><td style=text-align:left>GB</td></tr><tr><td style=text-align:left>激活值占用 (单卡) <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></td><td style=text-align:left><code>=(34 * s * (GBS/DP) * h * L) / (TP * 10^9)</code></td><td style=text-align:left>19.3</td><td style=text-align:left>GB</td></tr><tr><td style=text-align:left>预估单卡显存峰值</td><td style=text-align:left><code>=SUM(B8:B11)</code></td><td style=text-align:left>39.0</td><td style=text-align:left>GB</td></tr><tr><td style=text-align:left>显存是否满足？</td><td style=text-align:left><code>=IF(B12 &lt;= Mem_GPU, "✅ 满足", "❌ 超出！")</code></td><td style=text-align:left>✅ 满足</td><td style=text-align:left></td></tr></tbody></table><p>C. 网络带宽需求 (Network Bandwidth Requirement)</p><table><thead><tr><th style=text-align:left>指标名</th><th style=text-align:left>公式 (Excel表达式)</th><th style=text-align:left>结果</th><th style=text-align:left>单位</th></tr></thead><tbody><tr><td style=text-align:left>All-Reduce带宽 (DP) <sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup></td><td style=text-align:left><code>= (2 * P * 10^9 * 2) / (TP * (DP-1)/DP)</code></td><td style=text-align:left>8.75</td><td style=text-align:left>GB</td></tr><tr><td style=text-align:left>All-Gather/Reduce-Scatter带宽 (TP)</td><td style=text-align:left><code>= (2 * P * 10^9 * 2 * (TP-1)/TP) / (TP * PP)</code></td><td style=text-align:left>0.05</td><td style=text-align:left>GB</td></tr><tr><td style=text-align:left>P2P带宽 (PP)</td><td style=text-align:left><code>=(s * (GBS/DP) * h * 2 * 2) / (TP * PP)</code></td><td style=text-align:left>0.01</td><td style=text-align:left>GB</td></tr><tr><td style=text-align:left>单卡聚合带宽需求 <sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup></td><td style=text-align:left><code>= (B14+B15+B16) * 8 / 10^9 * (1/ (A2/(B5*Peak*MFU*10^12)) )</code></td><td style=text-align:left>~21</td><td style=text-align:left>Gbps</td></tr></tbody></table><p>使用说明：</p><ol><li>首先在【输入区】填入你的模型、数据和目标参数。</li><li>调整【硬件与并行策略】中的<code>DP</code>, <code>TP</code>, <code>PP</code>参数。</li><li>观察【输出区】的“所需总GPU卡数”和“显存是否满足？”。</li><li>你的目标是：在“显存满足”的前提下，找到一组<code>DP, TP, PP</code>组合，使得“建议集群规模”在你的预算范围内，且“实际预估训练天数”满足你的时间要求。</li><li>不断调整<code>DP</code>, <code>TP</code>, <code>PP</code>和<code>MFU</code>（如果你对你的集群优化有信心，可以调高MFU），进行“What-If”分析，找到最佳的资源配置方案。</li></ol><h3 id=python运维脚本库>Python运维脚本库</h3><p>这个脚本库旨在提供一系列开箱即用的运维工具，帮你自动化日常的巡检和计算任务。</p><h4 id=check_cluster_healthpy一键集群健康巡检><code>check_cluster_health.py</code>：一键集群健康巡检</h4><p>这个脚本通过调用Kubernetes Python客户端，获取所有节点，然后SSH到每个节点上执行<code>nvidia-smi</code>或<code>npu-smi</code>命令，并解析其输出，最后生成一份简洁的健康报告。</p><p>前提：</p><ul><li>安装Python库：<code>pip install kubernetes paramiko pandas</code></li><li>你的机器上配置好了<code>~/.kube/config</code>，可以访问目标K8s集群。</li><li>你的机器可以SSH免密登录到集群的所有节点。</li></ul><p>脚本 (<code>check_cluster_health.py</code>):</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>argparse</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>paramiko</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>pandas</span> <span class=k>as</span> <span class=nn>pd</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>kubernetes</span> <span class=kn>import</span> <span class=n>client</span><span class=p>,</span> <span class=n>config</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>datetime</span> <span class=kn>import</span> <span class=n>datetime</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>ssh_run_command</span><span class=p>(</span><span class=n>hostname</span><span class=p>,</span> <span class=n>command</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;通过SSH在远程节点上执行命令并返回输出。&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>try</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>ssh</span> <span class=o>=</span> <span class=n>paramiko</span><span class=o>.</span><span class=n>SSHClient</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>ssh</span><span class=o>.</span><span class=n>set_missing_host_key_policy</span><span class=p>(</span><span class=n>paramiko</span><span class=o>.</span><span class=n>AutoAddPolicy</span><span class=p>())</span>
</span></span><span class=line><span class=cl>        <span class=n>ssh</span><span class=o>.</span><span class=n>connect</span><span class=p>(</span><span class=n>hostname</span><span class=p>,</span> <span class=n>username</span><span class=o>=</span><span class=s1>&#39;root&#39;</span><span class=p>,</span> <span class=n>timeout</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span> <span class=c1># 请根据实际情况修改用户名</span>
</span></span><span class=line><span class=cl>        <span class=n>stdin</span><span class=p>,</span> <span class=n>stdout</span><span class=p>,</span> <span class=n>stderr</span> <span class=o>=</span> <span class=n>ssh</span><span class=o>.</span><span class=n>exec_command</span><span class=p>(</span><span class=n>command</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>output</span> <span class=o>=</span> <span class=n>stdout</span><span class=o>.</span><span class=n>read</span><span class=p>()</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=s1>&#39;utf-8&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>error</span> <span class=o>=</span> <span class=n>stderr</span><span class=o>.</span><span class=n>read</span><span class=p>()</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=s1>&#39;utf-8&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>ssh</span><span class=o>.</span><span class=n>close</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>error</span> <span class=ow>and</span> <span class=s2>&#34;command not found&#34;</span> <span class=ow>not</span> <span class=ow>in</span> <span class=n>error</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span> <span class=sa>f</span><span class=s2>&#34;ERROR: </span><span class=si>{</span><span class=n>error</span><span class=si>}</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>output</span>
</span></span><span class=line><span class=cl>    <span class=k>except</span> <span class=ne>Exception</span> <span class=k>as</span> <span class=n>e</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=sa>f</span><span class=s2>&#34;SSH_ERROR: </span><span class=si>{</span><span class=nb>str</span><span class=p>(</span><span class=n>e</span><span class=p>)</span><span class=si>}</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>parse_nvidia_smi</span><span class=p>(</span><span class=n>output</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;解析nvidia-smi的输出，提取关键健康指标。&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=s2>&#34;NVIDIA-SMI has failed&#34;</span> <span class=ow>in</span> <span class=n>output</span> <span class=ow>or</span> <span class=s2>&#34;ERROR&#34;</span> <span class=ow>in</span> <span class=n>output</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=p>[{</span><span class=s2>&#34;gpu_id&#34;</span><span class=p>:</span> <span class=s2>&#34;ALL&#34;</span><span class=p>,</span> <span class=s2>&#34;health&#34;</span><span class=p>:</span> <span class=s2>&#34;FAIL&#34;</span><span class=p>,</span> <span class=s2>&#34;message&#34;</span><span class=p>:</span> <span class=n>output</span><span class=p>}]</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>    <span class=n>devices</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>    <span class=n>lines</span> <span class=o>=</span> <span class=n>output</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=s1>&#39;</span><span class=se>\n</span><span class=s1>&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=c1># 非常简化的解析，实际生产中建议使用nvidia-smi --query-gpu=... --format=csv</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>line</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>lines</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=s2>&#34;MiB&#34;</span> <span class=ow>in</span> <span class=n>line</span> <span class=ow>and</span> <span class=s2>&#34;%&#34;</span> <span class=ow>in</span> <span class=n>line</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>parts</span> <span class=o>=</span> <span class=n>line</span><span class=o>.</span><span class=n>split</span><span class=p>()</span>
</span></span><span class=line><span class=cl>            <span class=n>gpu_id</span> <span class=o>=</span> <span class=n>parts</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=n>temp</span> <span class=o>=</span> <span class=n>parts</span><span class=p>[</span><span class=mi>3</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=n>power</span> <span class=o>=</span> <span class=n>parts</span><span class=p>[</span><span class=mi>5</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=n>mem_used</span> <span class=o>=</span> <span class=n>parts</span><span class=p>[</span><span class=mi>9</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=n>mem_total</span> <span class=o>=</span> <span class=n>parts</span><span class=p>[</span><span class=mi>11</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=n>gpu_util</span> <span class=o>=</span> <span class=n>parts</span><span class=p>[</span><span class=mi>13</span><span class=p>]</span>
</span></span><span class=line><span class=cl>          
</span></span><span class=line><span class=cl>            <span class=n>health</span> <span class=o>=</span> <span class=s2>&#34;OK&#34;</span>
</span></span><span class=line><span class=cl>            <span class=n>message</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=nb>int</span><span class=p>(</span><span class=n>temp</span><span class=o>.</span><span class=n>replace</span><span class=p>(</span><span class=s1>&#39;C&#39;</span><span class=p>,</span><span class=s1>&#39;&#39;</span><span class=p>))</span> <span class=o>&gt;</span> <span class=mi>85</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>health</span> <span class=o>=</span> <span class=s2>&#34;WARN&#34;</span>
</span></span><span class=line><span class=cl>                <span class=n>message</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Temp&gt;85C(</span><span class=si>{</span><span class=n>temp</span><span class=si>}</span><span class=s2>)&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=nb>int</span><span class=p>(</span><span class=n>power</span><span class=o>.</span><span class=n>replace</span><span class=p>(</span><span class=s1>&#39;W&#39;</span><span class=p>,</span><span class=s1>&#39;&#39;</span><span class=p>))</span> <span class=o>&gt;</span> <span class=mi>350</span><span class=p>:</span> <span class=c1># 假设功耗阈值为350W</span>
</span></span><span class=line><span class=cl>                <span class=n>health</span> <span class=o>=</span> <span class=s2>&#34;WARN&#34;</span>
</span></span><span class=line><span class=cl>                <span class=n>message</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Power&gt;350W(</span><span class=si>{</span><span class=n>power</span><span class=si>}</span><span class=s2>)&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=n>devices</span><span class=o>.</span><span class=n>append</span><span class=p>({</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;gpu_id&#34;</span><span class=p>:</span> <span class=n>gpu_id</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;health&#34;</span><span class=p>:</span> <span class=n>health</span> <span class=k>if</span> <span class=n>message</span> <span class=k>else</span> <span class=s2>&#34;OK&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;message&#34;</span><span class=p>:</span> <span class=s2>&#34;, &#34;</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>message</span><span class=p>)</span> <span class=k>if</span> <span class=n>message</span> <span class=k>else</span> <span class=s2>&#34;N/A&#34;</span>
</span></span><span class=line><span class=cl>            <span class=p>})</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>devices</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>get_k8s_nodes</span><span class=p>(</span><span class=n>label_selector</span><span class=o>=</span><span class=s2>&#34;&#34;</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;获取K8s集群中符合标签选择器的节点列表。&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>config</span><span class=o>.</span><span class=n>load_kube_config</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>v1</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>CoreV1Api</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>nodes</span> <span class=o>=</span> <span class=n>v1</span><span class=o>.</span><span class=n>list_node</span><span class=p>(</span><span class=n>label_selector</span><span class=o>=</span><span class=n>label_selector</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=p>[</span><span class=n>node</span><span class=o>.</span><span class=n>status</span><span class=o>.</span><span class=n>addresses</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>address</span> <span class=k>for</span> <span class=n>node</span> <span class=ow>in</span> <span class=n>nodes</span><span class=o>.</span><span class=n>items</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>main</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=n>parser</span> <span class=o>=</span> <span class=n>argparse</span><span class=o>.</span><span class=n>ArgumentParser</span><span class=p>(</span><span class=n>description</span><span class=o>=</span><span class=s2>&#34;Cluster Health Checker&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>parser</span><span class=o>.</span><span class=n>add_argument</span><span class=p>(</span><span class=s2>&#34;--vendor&#34;</span><span class=p>,</span> <span class=nb>type</span><span class=o>=</span><span class=nb>str</span><span class=p>,</span> <span class=n>default</span><span class=o>=</span><span class=s2>&#34;nvidia&#34;</span><span class=p>,</span> <span class=n>choices</span><span class=o>=</span><span class=p>[</span><span class=s2>&#34;nvidia&#34;</span><span class=p>,</span> <span class=s2>&#34;ascend&#34;</span><span class=p>],</span> <span class=n>help</span><span class=o>=</span><span class=s2>&#34;GPU/NPU vendor&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>parser</span><span class=o>.</span><span class=n>add_argument</span><span class=p>(</span><span class=s2>&#34;--nodes&#34;</span><span class=p>,</span> <span class=nb>type</span><span class=o>=</span><span class=nb>str</span><span class=p>,</span> <span class=n>help</span><span class=o>=</span><span class=s2>&#34;Comma-separated list of node IPs. Overrides k8s discovery.&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>parser</span><span class=o>.</span><span class=n>add_argument</span><span class=p>(</span><span class=s2>&#34;--label&#34;</span><span class=p>,</span> <span class=nb>type</span><span class=o>=</span><span class=nb>str</span><span class=p>,</span> <span class=n>default</span><span class=o>=</span><span class=s2>&#34;nvidia.com/gpu=true&#34;</span><span class=p>,</span> <span class=n>help</span><span class=o>=</span><span class=s2>&#34;K8s node label selector for discovery.&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>args</span> <span class=o>=</span> <span class=n>parser</span><span class=o>.</span><span class=n>parse_args</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>args</span><span class=o>.</span><span class=n>nodes</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>nodes_to_check</span> <span class=o>=</span> <span class=n>args</span><span class=o>.</span><span class=n>nodes</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=s1>&#39;,&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Discovering nodes with label &#39;</span><span class=si>{</span><span class=n>args</span><span class=o>.</span><span class=n>label</span><span class=si>}</span><span class=s2>&#39; from Kubernetes...&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>nodes_to_check</span> <span class=o>=</span> <span class=n>get_k8s_nodes</span><span class=p>(</span><span class=n>args</span><span class=o>.</span><span class=n>label</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Found </span><span class=si>{</span><span class=nb>len</span><span class=p>(</span><span class=n>nodes_to_check</span><span class=p>)</span><span class=si>}</span><span class=s2> nodes: </span><span class=si>{</span><span class=n>nodes_to_check</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>args</span><span class=o>.</span><span class=n>vendor</span> <span class=o>==</span> <span class=s2>&#34;nvidia&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>command</span> <span class=o>=</span> <span class=s2>&#34;nvidia-smi&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>parser_func</span> <span class=o>=</span> <span class=n>parse_nvidia_smi</span>
</span></span><span class=line><span class=cl>    <span class=k>elif</span> <span class=n>args</span><span class=o>.</span><span class=n>vendor</span> <span class=o>==</span> <span class=s2>&#34;ascend&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>command</span> <span class=o>=</span> <span class=s2>&#34;npu-smi info&#34;</span>
</span></span><span class=line><span class=cl>        <span class=c1># 你需要为npu-smi编写一个类似的解析函数</span>
</span></span><span class=line><span class=cl>        <span class=c1># parser_func = parse_npu_smi </span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Ascend NPU parser not implemented in this example.&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>results</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>node</span> <span class=ow>in</span> <span class=n>nodes_to_check</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Checking node: </span><span class=si>{</span><span class=n>node</span><span class=si>}</span><span class=s2> ...&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>output</span> <span class=o>=</span> <span class=n>ssh_run_command</span><span class=p>(</span><span class=n>node</span><span class=p>,</span> <span class=n>command</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>health_info</span> <span class=o>=</span> <span class=n>parser_func</span><span class=p>(</span><span class=n>output</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>device</span> <span class=ow>in</span> <span class=n>health_info</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>results</span><span class=o>.</span><span class=n>append</span><span class=p>({</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;node&#34;</span><span class=p>:</span> <span class=n>node</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;device_type&#34;</span><span class=p>:</span> <span class=n>args</span><span class=o>.</span><span class=n>vendor</span><span class=o>.</span><span class=n>upper</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;device_id&#34;</span><span class=p>:</span> <span class=n>device</span><span class=p>[</span><span class=s2>&#34;gpu_id&#34;</span><span class=p>],</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;health&#34;</span><span class=p>:</span> <span class=n>device</span><span class=p>[</span><span class=s2>&#34;health&#34;</span><span class=p>],</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;message&#34;</span><span class=p>:</span> <span class=n>device</span><span class=p>[</span><span class=s2>&#34;message&#34;</span><span class=p>],</span>
</span></span><span class=line><span class=cl>            <span class=p>})</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>    <span class=n>report_df</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>(</span><span class=n>results</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>&#34;</span> <span class=o>+</span> <span class=s2>&#34;=&#34;</span><span class=o>*</span><span class=mi>50</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  Cluster Health Report - </span><span class=si>{</span><span class=n>datetime</span><span class=o>.</span><span class=n>now</span><span class=p>()</span><span class=o>.</span><span class=n>strftime</span><span class=p>(</span><span class=s1>&#39;%Y-%m-</span><span class=si>%d</span><span class=s1> %H:%M:%S&#39;</span><span class=p>)</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;=&#34;</span><span class=o>*</span><span class=mi>50</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=n>report_df</span><span class=o>.</span><span class=n>to_string</span><span class=p>())</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 打印问题摘要</span>
</span></span><span class=line><span class=cl>    <span class=n>failed_nodes</span> <span class=o>=</span> <span class=n>report_df</span><span class=p>[</span><span class=n>report_df</span><span class=p>[</span><span class=s1>&#39;health&#39;</span><span class=p>]</span> <span class=o>!=</span> <span class=s1>&#39;OK&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=ow>not</span> <span class=n>failed_nodes</span><span class=o>.</span><span class=n>empty</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>&#34;</span> <span class=o>+</span> <span class=s2>&#34;!&#34;</span><span class=o>*</span><span class=mi>20</span> <span class=o>+</span> <span class=s2>&#34;  PROBLEM SUMMARY  &#34;</span> <span class=o>+</span> <span class=s2>&#34;!&#34;</span><span class=o>*</span><span class=mi>20</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=n>failed_nodes</span><span class=o>.</span><span class=n>to_string</span><span class=p>())</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;!&#34;</span><span class=o>*</span><span class=mi>59</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>&#34;</span> <span class=o>+</span> <span class=s2>&#34;✅&#34;</span><span class=o>*</span><span class=mi>10</span> <span class=o>+</span> <span class=s2>&#34;  All checks passed! Cluster is healthy. &#34;</span> <span class=o>+</span> <span class=s2>&#34;✅&#34;</span><span class=o>*</span><span class=mi>10</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=vm>__name__</span> <span class=o>==</span> <span class=s2>&#34;__main__&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>main</span><span class=p>()</span>
</span></span></code></pre></div><p>如何使用：</p><ol><li>巡检所有带<code>nvidia.com/gpu=true</code>标签的K8s节点：<code>python check_cluster_health.py</code></li><li>指定节点进行巡检：<code>python check_cluster_health.py --nodes 192.168.1.101,192.168.1.102</code></li></ol><p>这个脚本会输出一个清晰的表格，告诉你每个节点的每张卡是否健康，如果不健康，原因可能是什么（如高温）。这对于日常巡检和故障初步排查非常有用。</p><h4 id=calc_model_flopspy快速计算模型理论算力需求><code>calc_model_flops.py</code>：快速计算模型理论算力需求</h4><p>这个脚本将第七章的<code>6PD</code>公式封装成一个简单的命令行工具，帮你快速估算训练一个模型所需的总计算量和在特定集群上所需的时间。</p><p>脚本 (<code>calc_model_flops.py</code>):</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>argparse</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>sizeof_fmt</span><span class=p>(</span><span class=n>num</span><span class=p>,</span> <span class=n>suffix</span><span class=o>=</span><span class=s2>&#34;&#34;</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>unit</span> <span class=ow>in</span> <span class=p>[</span><span class=s2>&#34;&#34;</span><span class=p>,</span> <span class=s2>&#34;K&#34;</span><span class=p>,</span> <span class=s2>&#34;M&#34;</span><span class=p>,</span> <span class=s2>&#34;G&#34;</span><span class=p>,</span> <span class=s2>&#34;T&#34;</span><span class=p>,</span> <span class=s2>&#34;P&#34;</span><span class=p>,</span> <span class=s2>&#34;E&#34;</span><span class=p>,</span> <span class=s2>&#34;Z&#34;</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=nb>abs</span><span class=p>(</span><span class=n>num</span><span class=p>)</span> <span class=o>&lt;</span> <span class=mf>1000.0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span> <span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>num</span><span class=si>:</span><span class=s2>3.1f</span><span class=si>}{</span><span class=n>unit</span><span class=si>}{</span><span class=n>suffix</span><span class=si>}</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>num</span> <span class=o>/=</span> <span class=mf>1000.0</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>num</span><span class=si>:</span><span class=s2>.1f</span><span class=si>}</span><span class=s2>Y</span><span class=si>{</span><span class=n>suffix</span><span class=si>}</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>main</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=n>parser</span> <span class=o>=</span> <span class=n>argparse</span><span class=o>.</span><span class=n>ArgumentParser</span><span class=p>(</span><span class=n>description</span><span class=o>=</span><span class=s2>&#34;Estimate LLM Training FLOPs and Time&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>    <span class=c1># Model and Data parameters</span>
</span></span><span class=line><span class=cl>    <span class=n>parser</span><span class=o>.</span><span class=n>add_argument</span><span class=p>(</span><span class=s2>&#34;-p&#34;</span><span class=p>,</span> <span class=s2>&#34;--params&#34;</span><span class=p>,</span> <span class=nb>type</span><span class=o>=</span><span class=nb>float</span><span class=p>,</span> <span class=n>required</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>help</span><span class=o>=</span><span class=s2>&#34;Model parameters in billions (e.g., 70 for 70B)&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>parser</span><span class=o>.</span><span class=n>add_argument</span><span class=p>(</span><span class=s2>&#34;-d&#34;</span><span class=p>,</span> <span class=s2>&#34;--data_tokens&#34;</span><span class=p>,</span> <span class=nb>type</span><span class=o>=</span><span class=nb>float</span><span class=p>,</span> <span class=n>required</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>help</span><span class=o>=</span><span class=s2>&#34;Training data size in trillions of tokens (e.g., 2 for 2T)&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>    <span class=c1># Hardware and Performance parameters</span>
</span></span><span class=line><span class=cl>    <span class=n>parser</span><span class=o>.</span><span class=n>add_argument</span><span class=p>(</span><span class=s2>&#34;-n&#34;</span><span class=p>,</span> <span class=s2>&#34;--num_gpus&#34;</span><span class=p>,</span> <span class=nb>type</span><span class=o>=</span><span class=nb>int</span><span class=p>,</span> <span class=n>required</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>help</span><span class=o>=</span><span class=s2>&#34;Number of GPUs in the cluster&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>parser</span><span class=o>.</span><span class=n>add_argument</span><span class=p>(</span><span class=s2>&#34;--peak_tflops&#34;</span><span class=p>,</span> <span class=nb>type</span><span class=o>=</span><span class=nb>float</span><span class=p>,</span> <span class=n>required</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>help</span><span class=o>=</span><span class=s2>&#34;Peak TFLOPS of a single GPU at target precision (e.g., 312 for A100 FP16)&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>parser</span><span class=o>.</span><span class=n>add_argument</span><span class=p>(</span><span class=s2>&#34;--mfu&#34;</span><span class=p>,</span> <span class=nb>type</span><span class=o>=</span><span class=nb>float</span><span class=p>,</span> <span class=n>default</span><span class=o>=</span><span class=mf>0.4</span><span class=p>,</span> <span class=n>help</span><span class=o>=</span><span class=s2>&#34;Model FLOPs Utilization (MFU), a value between 0 and 1 (default: 0.4)&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>    <span class=n>args</span> <span class=o>=</span> <span class=n>parser</span><span class=o>.</span><span class=n>parse_args</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># --- Calculations ---</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>    <span class=c1># 1. Total FLOPs</span>
</span></span><span class=line><span class=cl>    <span class=n>total_flops</span> <span class=o>=</span> <span class=mi>6</span> <span class=o>*</span> <span class=p>(</span><span class=n>args</span><span class=o>.</span><span class=n>params</span> <span class=o>*</span> <span class=mf>1e9</span><span class=p>)</span> <span class=o>*</span> <span class=p>(</span><span class=n>args</span><span class=o>.</span><span class=n>data_tokens</span> <span class=o>*</span> <span class=mf>1e12</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>    <span class=c1># 2. Cluster effective throughput</span>
</span></span><span class=line><span class=cl>    <span class=n>single_gpu_effective_tflops</span> <span class=o>=</span> <span class=n>args</span><span class=o>.</span><span class=n>peak_tflops</span> <span class=o>*</span> <span class=n>args</span><span class=o>.</span><span class=n>mfu</span>
</span></span><span class=line><span class=cl>    <span class=n>cluster_effective_tflops</span> <span class=o>=</span> <span class=n>single_gpu_effective_tflops</span> <span class=o>*</span> <span class=n>args</span><span class=o>.</span><span class=n>num_gpus</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>    <span class=c1># 3. Estimated training time</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>cluster_effective_tflops</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>estimated_seconds</span> <span class=o>=</span> <span class=nb>float</span><span class=p>(</span><span class=s1>&#39;inf&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>estimated_seconds</span> <span class=o>=</span> <span class=n>total_flops</span> <span class=o>/</span> <span class=p>(</span><span class=n>cluster_effective_tflops</span> <span class=o>*</span> <span class=mf>1e12</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>    <span class=n>estimated_days</span> <span class=o>=</span> <span class=n>estimated_seconds</span> <span class=o>/</span> <span class=p>(</span><span class=mi>24</span> <span class=o>*</span> <span class=mi>3600</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># --- Print Report ---</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>&#34;</span> <span class=o>+</span> <span class=s2>&#34;=&#34;</span><span class=o>*</span><span class=mi>50</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;  LLM Training Estimation Report&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;=&#34;</span><span class=o>*</span><span class=mi>50</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>[Input Parameters]&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  - Model Size: </span><span class=si>{</span><span class=n>args</span><span class=o>.</span><span class=n>params</span><span class=si>}</span><span class=s2>B parameters&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  - Dataset Size: </span><span class=si>{</span><span class=n>args</span><span class=o>.</span><span class=n>data_tokens</span><span class=si>}</span><span class=s2>T tokens&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  - Cluster Size: </span><span class=si>{</span><span class=n>args</span><span class=o>.</span><span class=n>num_gpus</span><span class=si>}</span><span class=s2> GPUs&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  - Single GPU Peak Performance: </span><span class=si>{</span><span class=n>args</span><span class=o>.</span><span class=n>peak_tflops</span><span class=si>}</span><span class=s2> TFLOPS&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  - Assumed MFU: </span><span class=si>{</span><span class=n>args</span><span class=o>.</span><span class=n>mfu</span><span class=si>:</span><span class=s2>.2%</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>[Estimated Requirements]&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  - Total Training FLOPs: </span><span class=si>{</span><span class=n>sizeof_fmt</span><span class=p>(</span><span class=n>total_flops</span><span class=p>,</span> <span class=s1>&#39;FLOPs&#39;</span><span class=p>)</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  - Cluster Effective Throughput: </span><span class=si>{</span><span class=n>cluster_effective_tflops</span><span class=si>:</span><span class=s2>,.2f</span><span class=si>}</span><span class=s2> TFLOPS&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>[Final Estimation]&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;  - Estimated Training Time: </span><span class=si>{</span><span class=n>estimated_days</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2> days&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;=&#34;</span><span class=o>*</span><span class=mi>50</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>Note: This is a theoretical estimation. Actual time may vary based on network, storage, and software stack efficiency.&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=vm>__name__</span> <span class=o>==</span> <span class=s2>&#34;__main__&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>main</span><span class=p>()</span>
</span></span></code></pre></div><p>如何使用：</p><p>假设你想估算用一个1024张A100 (FP16峰值312 TFLOPS)的集群，训练一个70B模型、2T Tokens数据，预估MFU为40%所需的时间：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>python calc_model_flops.py <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --params <span class=m>70</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --data_tokens <span class=m>2</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --num_gpus <span class=m>1024</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --peak_tflops <span class=m>312</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --mfu 0.4
</span></span></code></pre></div><p>输出：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>==================================================
</span></span><span class=line><span class=cl>  LLM Training Estimation Report
</span></span><span class=line><span class=cl>==================================================
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>[Input Parameters]
</span></span><span class=line><span class=cl>  - Model Size: 70.0B parameters
</span></span><span class=line><span class=cl>  - Dataset Size: 2.0T tokens
</span></span><span class=line><span class=cl>  - Cluster Size: 1024 GPUs
</span></span><span class=line><span class=cl>  - Single GPU Peak Performance: 312.0 TFLOPS
</span></span><span class=line><span class=cl>  - Assumed MFU: 40.00%
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>[Estimated Requirements]
</span></span><span class=line><span class=cl>  - Total Training FLOPs: 840.0EFLOPs
</span></span><span class=line><span class=cl>  - Cluster Effective Throughput: 127,795.20 TFLOPS
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>[Final Estimation]
</span></span><span class=line><span class=cl>  - Estimated Training Time: 76.08 days
</span></span><span class=line><span class=cl>==================================================
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Note: This is a theoretical estimation. Actual time may vary based on network, storage, and software stack efficiency.
</span></span></code></pre></div><p>这个脚本为你提供了一个快速、便捷的方式，来验证你在Excel计算器中规划的方案，或者在日常讨论中快速给出算力需求的数量级估算。</p><h3 id=结语>结语</h3><p>这个效率工具箱，是你将理论知识转化为生产力的催化剂。它们并不完美，但它们提供了一个坚实的起点。真正的价值，在于你根据自己团队的特定需求，不断地去完善、扩展和定制这些工具。让它们和你一起成长，成为你作为一名顶尖AI Infra工程师，不可或-或缺的“左膀右臂”。</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>激活值公式为简化估算，实际与激活重计算等技术强相关。&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>网络带宽估算非常复杂，这里提供的是一个基于单次迭代通信量的粗略模型，实际需求受通信计算重叠度影响巨大。&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>表示单次迭代总通信量除以单次迭代时间，估算所需的平均带宽。&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><div class=post_footer><div class=meta><div class=info><span class="field tags"><i class=ri-stack-line></i>
<a href=https://zhurongshuo.com/tags/%E4%B9%A6%E7%A8%BF/>书稿</a></span></div></div></div></div><div class=doc_comments></div></div></div></div><a id=back_to_top href=# class=back_to_top><i class=ri-arrow-up-s-line></i></a><footer class=footer><div class=powered_by><a href=https://varkai.com>Designed by VarKai, </a><a href=http://www.gohugo.io/>Proudly published with Hugo,</a></div><div class=footer_slogan><span>法不净空，觉无性也。</span></div><div class=powered_by style=margin-top:10px;font-size:14px><a href=https://zhurongshuo.com/>Copyright © 2010-2025 祝融说 zhurongshuo.com All Rights Reserved.</a></div></footer><script defer src=https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><link href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.css rel=stylesheet integrity="sha256-7qiTu3a8qjjWtcX9w+f2ulVUZSUdCZFEK62eRlmLmCE=" crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin=anonymous></script><script defer src=https://zhurongshuo.com/js/zozo.js></script><script type=text/javascript async src="https://cdn.jsdelivr.net/npm/mathjax@2.7.8/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["[[","]]"]],processEscapes:!0,processEnvironments:!0,skipTags:["script","noscript","style","textarea","pre"],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}}),MathJax.Hub.Queue(function(){var e,t=MathJax.Hub.getAllJax();for(e=0;e<t.length;e+=1)t[e].SourceElement().parentNode.className+=" has-jax"})</script><style>code.has-jax{font:inherit;font-size:100%;background:inherit;border:inherit;color:#515151}</style><script async src="https://www.googletagmanager.com/gtag/js?id=G-KKJ5ZEG1NB"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KKJ5ZEG1NB")</script></body></html>