<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>智算中心运营实战：从基础设施到大模型全栈优化 on 祝融说。</title><link>https://zhurongshuo.com/practices/season-4/intelligent-computing-center-operations-in-practice/</link><description>Recent content in 智算中心运营实战：从基础设施到大模型全栈优化 on 祝融说。</description><generator>Hugo</generator><language>zh-cn</language><lastBuildDate>Sat, 29 Nov 2025 00:00:00 +0800</lastBuildDate><atom:link href="https://zhurongshuo.com/practices/season-4/intelligent-computing-center-operations-in-practice/index.xml" rel="self" type="application/rss+xml"/><item><title>第10章：常见故障排查与SRE实践</title><link>https://zhurongshuo.com/practices/season-4/intelligent-computing-center-operations-in-practice/part-04/chapter-10/</link><pubDate>Sat, 29 Nov 2025 00:00:00 +0800</pubDate><guid>https://zhurongshuo.com/practices/season-4/intelligent-computing-center-operations-in-practice/part-04/chapter-10/</guid><description>&lt;p&gt;在第九章中，我们为智算中心安装了强大的“天眼”系统——一个覆盖全链路的可观测性平台。我们现在能够看到硬件的每一次心跳，追踪业务的每一次请求。然而，监控本身并不能解决问题，它只是吹响了战斗的号角。当告警声响起，真正的考验才刚刚开始。&lt;/p&gt;
&lt;p&gt;一个价值数亿元的千卡集群，每宕机一分钟，都意味着数千甚至数万元的算力成本被白白烧掉。作为智算中心的守护者，我们的核心价值不仅在于保障系统“不出事”，更在于出事后能以最快的速度恢复服务（MTTR - Mean Time To Recovery）。这要求我们不仅要熟悉工具，更要具备像急诊医生一样的临床诊断思维：快速分类、定位病灶、对症下药、事后复盘。&lt;/p&gt;</description></item><item><title>第11章：两级智算运营体系设计</title><link>https://zhurongshuo.com/practices/season-4/intelligent-computing-center-operations-in-practice/part-05/chapter-11/</link><pubDate>Sat, 29 Nov 2025 00:00:00 +0800</pubDate><guid>https://zhurongshuo.com/practices/season-4/intelligent-computing-center-operations-in-practice/part-05/chapter-11/</guid><description>&lt;p&gt;在前面的篇章中，我们投入了巨大的精力，构建了一座功能强大的智算“巨舰”。我们精通了它的引擎（GPU/NPU），熟悉了它的航道（网络与存储），掌握了它的驾驶术（调度与并行），甚至还演练了各种紧急情况下的损管流程。现在，是时候为这艘巨舰任命一位“舰长”，并为它制定一套远航的“作战条令”了。&lt;/p&gt;</description></item><item><title>第12章：LLMOps与AI Infra的未来</title><link>https://zhurongshuo.com/practices/season-4/intelligent-computing-center-operations-in-practice/part-05/chapter-12/</link><pubDate>Sat, 29 Nov 2025 00:00:00 +0800</pubDate><guid>https://zhurongshuo.com/practices/season-4/intelligent-computing-center-operations-in-practice/part-05/chapter-12/</guid><description>&lt;p&gt;当我们合上这本书的前十一章时，我们已经共同完成了一段非凡的旅程。我们从最基础的硅芯片出发，穿越了网络、存储、容器、调度的丛林，攀登了训练、推理、监控的险峰，甚至还为我们庞大的智算帝国设计了协同运作的法律（运营体系）和货币（计费模型）。我们已经不仅仅是“会用AI”的人，而是“能支撑起AI”的人。&lt;/p&gt;</description></item><item><title>第1章：智算中心（AIDC）的新范式</title><link>https://zhurongshuo.com/practices/season-4/intelligent-computing-center-operations-in-practice/part-01/chapter-01/</link><pubDate>Sat, 29 Nov 2025 00:00:00 +0800</pubDate><guid>https://zhurongshuo.com/practices/season-4/intelligent-computing-center-operations-in-practice/part-01/chapter-01/</guid><description>&lt;p&gt;欢迎来到智能计算的时代。&lt;/p&gt;
&lt;p&gt;2022年末，随着ChatGPT的横空出世，一场由大型语言模型（LLM）驱动的技术革命席卷全球。它不仅颠覆了我们与信息交互的方式，更深刻地重塑了计算产业的底层逻辑。支撑这场革命的，不再是我们熟悉的、以通用计算为核心的传统数据中心（IDC），而是一种全新的、为海量智能计算量身打造的新型基础设施——智算中心（Artificial Intelligence Data Center, AIDC）。&lt;/p&gt;</description></item><item><title>第2章：双核心生态：NVIDIA与华为昇腾</title><link>https://zhurongshuo.com/practices/season-4/intelligent-computing-center-operations-in-practice/part-01/chapter-02/</link><pubDate>Sat, 29 Nov 2025 00:00:00 +0800</pubDate><guid>https://zhurongshuo.com/practices/season-4/intelligent-computing-center-operations-in-practice/part-01/chapter-02/</guid><description>&lt;p&gt;在第一章中，我们明确了智能算力（以GPU/NPU为代表）与通用算力（以CPU为代表）的本质区别。现在，我们将把显微镜对准智能算力的心脏——AI加速芯片。在这个领域，呈现出“一超多强”的格局。NVIDIA凭借其先发优势和强大的CUDA生态，构建了事实上的行业标准，是那个“一超”。与此同时，以华为昇腾为代表的国产力量正在迅速崛起，因其在自主可控和供应链安全方面的战略价值，以及在特定场景下展现出的优异能效比，成为了智算中心建设中不可或缺的“另一极”。&lt;/p&gt;</description></item><item><title>第3章：大模型的高速公路——高性能网络与存储</title><link>https://zhurongshuo.com/practices/season-4/intelligent-computing-center-operations-in-practice/part-01/chapter-03/</link><pubDate>Sat, 29 Nov 2025 00:00:00 +0800</pubDate><guid>https://zhurongshuo.com/practices/season-4/intelligent-computing-center-operations-in-practice/part-01/chapter-03/</guid><description>&lt;p&gt;在上一章中，我们已经深入剖析了NVIDIA和华为昇腾这两大AI算力引擎的内部构造。然而，一个残酷的现实是：即便你拥有了世界上最强大的GPU集群，如果无法高效地为其“喂饱”数据，并让它们之间进行无缝的“对话”，这些昂贵的硅片也只是一堆高功耗的取暖器。大模型训练，尤其是动辄上千卡的分布式训练，早已不是单点计算能力的竞赛，而是整个系统工程——特别是网络与存储——综合能力的体现。&lt;/p&gt;</description></item><item><title>第4章：AI容器化技术</title><link>https://zhurongshuo.com/practices/season-4/intelligent-computing-center-operations-in-practice/part-02/chapter-04/</link><pubDate>Sat, 29 Nov 2025 00:00:00 +0800</pubDate><guid>https://zhurongshuo.com/practices/season-4/intelligent-computing-center-operations-in-practice/part-02/chapter-04/</guid><description>&lt;p&gt;在第一篇中，我们完成了智算中心“地基”的建设。我们深入了AI芯片的内核，铺设了RDMA的高速网络，并构建了并行的存储系统。现在，我们拥有了强大的、但却原始的裸金属算力。这就像我们拥有了一座装备精良的工厂，但里面还没有标准化的生产线。直接在裸金属服务器上部署AI应用，会让我们迅速陷入“依赖地狱”——不同项目的Python版本冲突、系统库不兼容、环境难以迁移和复现。这在追求快速迭代的AI时代是不可接受的。&lt;/p&gt;</description></item><item><title>第5章：AI任务调度与资源管理</title><link>https://zhurongshuo.com/practices/season-4/intelligent-computing-center-operations-in-practice/part-02/chapter-05/</link><pubDate>Sat, 29 Nov 2025 00:00:00 +0800</pubDate><guid>https://zhurongshuo.com/practices/season-4/intelligent-computing-center-operations-in-practice/part-02/chapter-05/</guid><description>&lt;p&gt;在上一章中，我们已经成功地将AI算力（GPU/NPU）通过Device Plugin机制接入了Kubernetes，并学会了如何构建标准化的容器镜像。至此，我们的K8s集群已经具备了运行AI任务的基本能力。然而，一场新的、更为严峻的挑战正悄然降临，它发生在K8s集群的“中央大脑”——调度器（Scheduler）之中。&lt;/p&gt;</description></item><item><title>第6章：大模型训练全流程运营</title><link>https://zhurongshuo.com/practices/season-4/intelligent-computing-center-operations-in-practice/part-03/chapter-06/</link><pubDate>Sat, 29 Nov 2025 00:00:00 +0800</pubDate><guid>https://zhurongshuo.com/practices/season-4/intelligent-computing-center-operations-in-practice/part-03/chapter-06/</guid><description>&lt;p&gt;在前两篇中，我们已经成功地搭建了一套坚实的、云原生的AI基础设施。我们拥有了由顶尖GPU/NPU构成的算力池，铺设了RDMA高速网络，构建了高性能存储，并部署了能够智能调度AI任务的Volcano/Yunikorn平台。我们已经拥有了一座“硬件精良、软件智能”的现代化工厂。&lt;/p&gt;</description></item><item><title>第7章：算力需求精准核算</title><link>https://zhurongshuo.com/practices/season-4/intelligent-computing-center-operations-in-practice/part-03/chapter-07/</link><pubDate>Sat, 29 Nov 2025 00:00:00 +0800</pubDate><guid>https://zhurongshuo.com/practices/season-4/intelligent-computing-center-operations-in-practice/part-03/chapter-07/</guid><description>&lt;p&gt;在前面的章节中，我们已经深入了解了大模型训练的各种模式与并行策略。现在，一个所有AI Infra工程师都必须面对的终极问题摆在了面前：“我需要多少资源？”&lt;/p&gt;
&lt;p&gt;这个问题会以各种形式出现：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;算法工程师：“我想微调一个70B的模型，需要多少张A800？”&lt;/li&gt;
&lt;li&gt;你的老板：“我们计划从零预训练一个千亿模型，需要采购多少服务器？预算大概是多少？要训多久？”&lt;/li&gt;
&lt;li&gt;你自己：“这个训练任务为什么OOM（Out of Memory）了？到底是哪里爆了显存？”&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;回答这些问题，不能再依靠经验和感觉。你需要的是一套科学、严谨的量化分析方法。本章，我们将一起化身为“算力精算师”，从第一性原理出发，推导和建立大模型训练的两大核心数学模型：显存占用模型和训练吞吐量模型。我们将用公式和计算器，将模糊的“资源需求”转化为精确的数字。最后，我们将通过一个真实的选型实战，将这些理论应用到集群规划的决策中。&lt;/p&gt;</description></item><item><title>第8章：推理加速与服务化</title><link>https://zhurongshuo.com/practices/season-4/intelligent-computing-center-operations-in-practice/part-03/chapter-08/</link><pubDate>Sat, 29 Nov 2025 00:00:00 +0800</pubDate><guid>https://zhurongshuo.com/practices/season-4/intelligent-computing-center-operations-in-practice/part-03/chapter-08/</guid><description>&lt;p&gt;经过预训练、微调和严格的算力核算，我们终于得到了一个性能优异的大模型。然而，一个躺在硬盘里的模型文件本身并不产生价值。真正的挑战现在才开始：如何将这个庞大的模型部署成一个能够同时服务成千上万用户、响应速度快如闪电、并且成本可控的在线服务？&lt;/p&gt;
&lt;p&gt;这就是大模型推理（Inference）所要解决的问题。与训练不同，推理场景对延迟（Latency）和吞吐量（Throughput）的要求极为苛刻。用户无法忍受与聊天机器人对话时，每回复一句话都要等待几十秒。同时，对于企业而言，每一块用于推理的GPU都必须尽可能地服务更多用户，以摊薄昂贵的硬件和电力成本。&lt;/p&gt;</description></item><item><title>第9章：构建全链路监控体系</title><link>https://zhurongshuo.com/practices/season-4/intelligent-computing-center-operations-in-practice/part-04/chapter-09/</link><pubDate>Sat, 29 Nov 2025 00:00:00 +0800</pubDate><guid>https://zhurongshuo.com/practices/season-4/intelligent-computing-center-operations-in-practice/part-04/chapter-09/</guid><description>&lt;p&gt;至此，我们已经走过了从硬件选型、平台搭建到模型训练与推理服务化的完整旅程。我们构建的智算中心，如同一座精密的、高速运转的巨型工厂，每一秒都在进行着海量的计算。然而，复杂性与脆弱性是一对孪生兄弟。在这座工厂里，任何一个微小的环节——一块过热的GPU、一个拥堵的网络端口、一个配置不当的推理引擎——都可能引发连锁反应，导致昂贵的训练任务失败或在线服务中断。&lt;/p&gt;</description></item><item><title>附录</title><link>https://zhurongshuo.com/practices/season-4/intelligent-computing-center-operations-in-practice/appendix/</link><pubDate>Sat, 29 Nov 2025 00:00:00 +0800</pubDate><guid>https://zhurongshuo.com/practices/season-4/intelligent-computing-center-operations-in-practice/appendix/</guid><description/></item><item><title>前言</title><link>https://zhurongshuo.com/practices/season-4/intelligent-computing-center-operations-in-practice/introduction/</link><pubDate>Sat, 29 Nov 2025 00:00:00 +0800</pubDate><guid>https://zhurongshuo.com/practices/season-4/intelligent-computing-center-operations-in-practice/introduction/</guid><description>&lt;p&gt;我们正处在一个波澜壮阔的大时代门口。&lt;/p&gt;
&lt;p&gt;以大型语言模型（LLM）为代表的生成式人工智能（AIGC）技术，正以前所未有的力量，重塑着从软件开发、内容创作到科学研究的每一个角落。这不仅仅是一次技术的迭代，更是一场深刻的范式革命。无数的企业和开发者，怀揣着对智能未来的憧憬，投身于这场激动人心的“AI淘金热”之中。&lt;/p&gt;</description></item></channel></rss>