<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><meta name=google-site-verification content="8_xpI-TS3tNV8UPug-Q6Ef3BhKTcy0WOG7dEdAcm2zk"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="祝融"><link rel=dns-prefetch href=//cdn.jsdelivr.net><link rel=preconnect href=https://cdn.jsdelivr.net crossorigin><link rel=dns-prefetch href=//www.googletagmanager.com><link rel=preconnect href=https://www.googletagmanager.com crossorigin><link rel=canonical href=https://zhurongshuo.com/practices/season-4/intelligent-computing-center-operations-in-practice/part-05/chapter-12/><title>祝融说。 第12章：LLMOps与AI Infra的未来</title><meta property="og:title" content="第12章：LLMOps与AI Infra的未来"><meta property="og:url" content="https://zhurongshuo.com/practices/season-4/intelligent-computing-center-operations-in-practice/part-05/chapter-12/"><meta property="og:site_name" content="祝融说。"><meta property="og:locale" content="zh_CN"><meta property="og:type" content="article"><meta property="article:published_time" content="2025-11-29T00:00:00+08:00"><meta property="article:modified_time" content="2025-11-29T00:00:00+08:00"><meta property="article:tag" content="书稿"><meta name=description content="当我们合上这本书的前十一章时，我们已经共同完成了一段非凡的旅程。我们从最基础的硅芯片出发，穿越了网络、存储、容器、调度的丛林，攀登了训练、推理、监控的险峰，甚至还为我们庞大的智算帝国设计了协同运作的法律（运营体系）和货币（计费模型）。我们已经不仅仅是“会用AI”的人，而是“能支撑起AI”的人。
"><meta property="og:description" content="当我们合上这本书的前十一章时，我们已经共同完成了一段非凡的旅程。我们从最基础的硅芯片出发，穿越了网络、存储、容器、调度的丛林，攀登了训练、推理、监控的险峰，甚至还为我们庞大的智算帝国设计了协同运作的法律（运营体系）和货币（计费模型）。我们已经不仅仅是“会用AI”的人，而是“能支撑起AI”的人。
"><meta property="og:image" content="https://zhurongshuo.com/images/favicon.ico"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="第12章：LLMOps与AI Infra的未来"><meta name=twitter:description content="当我们合上这本书的前十一章时，我们已经共同完成了一段非凡的旅程。我们从最基础的硅芯片出发，穿越了网络、存储、容器、调度的丛林，攀登了训练、推理、监控的险峰，甚至还为我们庞大的智算帝国设计了协同运作的法律（运营体系）和货币（计费模型）。我们已经不仅仅是“会用AI”的人，而是“能支撑起AI”的人。
"><meta name=twitter:image content="https://zhurongshuo.com/images/favicon.ico"><meta name=keywords content="智算中心运营实战：从基础设施到大模型全栈优化,第12章：LLMOps与AI Infra的未来"><link rel="shortcut icon" href=https://zhurongshuo.com/images/favicon.ico><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/normalize.css><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/animate-custom.css><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/zozo.css><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/remixicon-custom.css><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/highlight.css><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","headline":"第12章：LLMOps与AI Infra的未来","description":"当我们合上这本书的前十一章时，我们已经共同完成了一段非凡的旅程。我们从最基础的硅芯片出发，穿越了网络、存储、容器、调度的丛林，攀登了训练、推理、监控的险峰，甚至还为我们庞大的智算帝国设计了协同运作的法律（运营体系）和货币（计费模型）。我们已经不仅仅是“会用AI”的人，而是“能支撑起AI”的人。\n","datePublished":"2025-11-29T00:00:00\u002b08:00","dateModified":"2025-11-29T00:00:00\u002b08:00","author":{"@type":"Person","name":"祝融"},"publisher":{"@type":"Organization","name":"祝融说。","logo":{"@type":"ImageObject","url":"https:\/\/zhurongshuo.com\/images\/favicon.ico"}},"image":"https:\/\/zhurongshuo.com\/images\/favicon.ico","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/zhurongshuo.com\/practices\/season-4\/intelligent-computing-center-operations-in-practice\/part-05\/chapter-12\/"},"keywords":"书稿"}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"首页","item":"https:\/\/zhurongshuo.com\/"},{"@type":"ListItem","position":2,"name":"practices","item":"https:\/\/zhurongshuo.com\/practices/"},{"@type":"ListItem","position":3,"name":"第12章：LLMOps与AI Infra的未来","item":"https:\/\/zhurongshuo.com\/practices\/season-4\/intelligent-computing-center-operations-in-practice\/part-05\/chapter-12\/"}]}</script></head><body><div class="main animate__animated animate__fadeInDown"><div class="nav_container animated fadeInDown"><div class=site_nav id=site_nav><ul><li><a href=https://zhurongshuo.com/>首页</a></li><li><a href=https://zhurongshuo.com/start/>开始</a></li><li><a href=https://zhurongshuo.com/advanced/>进阶rc</a></li><li><a href=https://zhurongshuo.com/posts/>归档</a></li><li><a href=https://zhurongshuo.com/tags/>标签</a></li><li><a href=https://zhurongshuo.com/about/>关于</a></li></ul></div><div class=menu_icon><a id=menu_icon><i class=ri-menu-line></i></a></div></div><div class="header animated fadeInDown"><div class=site_title_container><div class=site_title><h1><a href=https://zhurongshuo.com/><span class=web-font>祝融说。</span></a></h1></div><div class=description><p class=sub_title>法不净空，觉无性也。</p><div class=my_socials><a href=https://zhurongshuo.com/books/ title=book-open><i class=ri-book-open-line></i></a>
<a href=https://zhurongshuo.com/practices/ title=trophy><i class=ri-trophy-line></i></a>
<a href=https://zhurongshuo.com/gallery/ title=gallery><i class=ri-gallery-line></i></a>
<a href=https://zhurongshuo.com/about/ title=game><i class=ri-game-line></i></a>
<a href type=application/rss+xml title=rss target=_blank><i class=ri-rss-fill></i></a></div></div></div></div><div class=content><div class=post_page><div class="post animate__animated animate__fadeInDown"><div class="post_title post_detail_title"><h2><a href=https://zhurongshuo.com/practices/season-4/intelligent-computing-center-operations-in-practice/part-05/chapter-12/>第12章：LLMOps与AI Infra的未来</a></h2><span class=date>2025.11.29</span></div><div class="post_content markdown"><p>当我们合上这本书的前十一章时，我们已经共同完成了一段非凡的旅程。我们从最基础的硅芯片出发，穿越了网络、存储、容器、调度的丛林，攀登了训练、推理、监控的险峰，甚至还为我们庞大的智算帝国设计了协同运作的法律（运营体系）和货币（计费模型）。我们已经不仅仅是“会用AI”的人，而是“能支撑起AI”的人。</p><p>现在，是时候从具体的战役中抬起头，审视整个战争的全貌，并思考未来的方向了。AI的发展日新月异，大模型的能力边界每天都在被拓展。支撑这一切的AI基础设施，也正处在一个剧烈的变革与演进之中。</p><p>在本章，我们将聚焦于两个核心议题：体系与人。首先，我们将探讨如何将传统的DevOps思想，升维为面向大型语言模型的LLMOps，学习如何用工程化的纪律和自动化的流水线，来管理模型从“出生”到“退役”的全生命周期。这不仅是技术上的整合，更是组织协作模式的革命。接着，我们将把目光投向这条赛道上最重要的资产——也就是你，AI Infra工程师。我们将系统性地梳理成为一名顶尖AI Infra专家所需要具备的能力矩阵，并为你规划一条从新手到架构师的清晰进阶路径。</p><p>这最后一章，既是总结，也是起点。它将帮助你把本书的所有知识融会贯通，形成一个完整的世界观，并为你在这条激动人心的职业道路上继续前行，提供一份宝贵的导航图。</p><h2 id=121-从devops到llmops模型版本管理mlflow与评估流水线>12.1 从DevOps到LLMOps：模型版本管理（MLflow）与评估流水线</h2><p>DevOps（开发与运维一体化）的出现，通过自动化的CI/CD流水线、版本控制和基础设施即代码，极大地提升了传统软件的交付速度和质量。然而，当我们试图将这套体系生搬硬套到大模型开发上时，却发现困难重重。</p><h3 id=1211-为什么需要llmopsai开发的三座大山>12.1.1 为什么需要LLMOps？AI开发的“三座大山”</h3><p>大模型的开发流程，与传统软件相比，多了三个不确定性的维度：</p><ol><li>代码 (Code)： 与传统软件相同，指训练、推理的脚本和业务逻辑代码。</li><li>数据 (Data)： 这是第一个新增的维度。同样的代码，用不同的数据集（预训练数据、微调数据）去训练，会产生完全不同的模型。数据的清洗、标注、版本管理，成了和代码管理同等重要的事情。</li><li>模型 (Model)： 这是第二个新增的维度。模型本身成了一种需要被版本化、被评估、被部署的核心产物。它不仅仅是代码编译的结果，更是数据“喂养”的结果，还与训练时的超参数紧密相关。</li></ol><p>这“代码-数据-模型”三位一体的复杂性，使得传统的DevOps流程捉襟见肘。我们需要一套新的、能够同时管理这三个变量的工程实践体系——这就是LLMOps（Large Language Model Operations）。</p><p>LLMOps的核心目标是：将AI模型的开发、训练、评估、部署和监控，流程化、自动化、可复现。</p><h3 id=1212-mlflowllmops的瑞士军刀>12.1.2 MLflow：LLMOps的“瑞士军刀”</h3><p>要实现LLMOps，我们需要一套合适的工具链。MLflow是一个开源平台，它恰好为我们提供了管理机器学习生命周期所需的一系列核心组件，堪称LLMOps领域的“瑞士军刀”。</p><p>MLflow的四大组件：</p><ol><li>MLflow Tracking (追踪):<ul><li>作用： 记录和查询实验的“族谱”。</li><li>使用方式： 在你的训练脚本中，加入几行MLflow的代码，就可以将每一次运行的超参数（Hyperparameters）、性能指标（Metrics，如Loss, Accuracy）、产出的模型文件（Artifacts）以及代码版本（Git Commit Hash）等，全部自动记录到MLflow的服务器上。</li><li>价值： 彻底解决了“我这个效果最好的模型，当时是用什么参数跑出来的？”这一灵魂拷问。它为每一次实验都提供了可追溯的、完整的“出生证明”。</li></ul></li><li>MLflow Models (模型):<ul><li>作用： 定义一种标准化的模型打包格式。</li><li>使用方式： MLflow可以将一个训练好的模型，连同其依赖环境（<code>conda.yaml</code>或<code>requirements.txt</code>），打包成一个统一的格式。</li><li>价值： 这个标准化的包，可以被轻松地部署到各种环境中（如本地、Docker容器、云平台），实现了模型的“一次打包，到处运行”。</li></ul></li><li>MLflow Model Registry (模型仓库):<ul><li>作用： 我们在11.1节已经详细介绍过。它是一个对MLflow Models进行集中式版本管理和生命周期控制的“模型档案馆”。</li><li>价值： 提供了模型从<code>Staging</code>到<code>Production</code>的清晰晋升路径，是连接训练和部署的桥梁。</li></ul></li><li>MLflow Projects (项目):<ul><li>作用： 定义一种可复现运行代码的标准格式。</li><li>使用方式： 通过一个<code>MLproject</code>文件，可以定义项目的依赖、入口点和参数。这使得其他人可以轻松地<code>mlflow run</code>你的项目，复现你的实验。</li></ul></li></ol><h3 id=1213-构建自动化的模型评估流水线>12.1.3 构建自动化的模型评估流水线</h3><p>模型评估是LLMOps中至关重要、也最能体现其价值的一环。一个新训练出的模型版本，不能仅凭<code>Loss</code>下降就认为它更好。它必须经过一系列客观、全面的评估，才能被允许上线。我们可以利用CI/CD引擎（如Jenkins, GitLab CI）和MLflow，构建一条自动化的评估流水线。</p><p>流水线触发器：</p><p>当一个训练任务成功结束，并向MLflow Tracking注册了一个新的模型产物时，自动触发评估流水线。</p><p>流水线核心步骤：</p><p>Stage 1: 部署到评估环境 (Deploy to Staging)</p><ol><li>获取模型： 从MLflow Tracking获取刚刚产出的新模型。</li><li>打包与部署： 使用MLflow Models的能力，将其打包，并通过一个临时的推理服务（如vLLM）部署到一个专用的、隔离的“评估K8s集群”或命名空间中。</li></ol><p>Stage 2: 运行客观指标评估 (Objective Evaluation)</p><ol><li>标准Benchmark测试：<ul><li>准备一系列标准的、开源的评估数据集（如MMLU, C-Eval, GSM8K）。</li><li>运行评估脚本，调用刚刚部署的临时推理服务，在这些数据集上进行测试，计算出模型的得分。</li></ul></li><li>与基线模型对比：<ul><li>流水线同时会部署当前生产环境中的“基线模型”（Baseline Model）。</li><li>让新模型和基线模型在相同的评估集上“同场竞技”。</li></ul></li><li>结果上报：<ul><li>将新模型的评估得分（<code>MMLU_score</code>, <code>C-Eval_score</code>等），通过MLflow Tracking API，更新到这次实验的记录中。</li></ul></li></ol><p>Stage 3: 运行主观/对抗性评估 (Subjective/Adversarial Evaluation)</p><ol><li>“红队测试” (Red Teaming)：<ul><li>运行一系列预设的、旨在诱导模型产生不安全、有偏见或错误回答的“刁钻”问题（对抗性提示）。</li><li>检查模型的回答是否“越界”，记录安全评分。</li></ul></li><li>领域知识评估：<ul><li>如果模型是为特定领域（如金融、医疗）微调的，需要用该领域的专业问题集进行测试。</li></ul></li></ol><p>Stage 4: 生成评估报告与决策 (Report & Decision)</p><ol><li>生成报告：<ul><li>流水线汇总所有评估结果，自动生成一份图文并茂的评估报告。报告中会清晰地对比新模型与基线模型在各个维度上的优劣。</li></ul></li><li>人工审核与批准 (Human-in-the-loop)：<ul><li>将评估报告发送给模型审查委员会（通常由算法负责人、产品经理、SRE负责人组成）。</li><li>决策门禁 (Decision Gate)： 只有当新模型在关键指标上显著优于基线模型，并且没有引入新的安全问题时，审查委员会才会批准其上线。</li></ul></li><li>更新模型仓库状态：<ul><li>一旦批准，审查委员会成员或自动化脚本会登录到MLflow Model Registry，将这个模型版本从“无状态”或<code>Staging</code>，正式提升为<code>Production</code>阶段。</li></ul></li></ol><p>触发部署流水线：</p><p>这个<code>Production</code>状态的变更，又会成为我们11.1节中讨论的模型分发与部署流水线的触发器，从而开启模型到线上服务的“最后一公里”。</p><p>LLMOps的价值闭环：</p><p>通过这套体系，我们实现了：</p><ul><li>实验可追溯： 所有的模型都有清晰的“血缘关系”。</li><li>评估标准化： 所有的模型都经过同一套严格的“大考”。</li><li>上线有依据： 模型的上线不再是“拍脑袋”，而是基于数据的、可审计的决策。</li><li>流程自动化： 将人力从繁琐的重复性工作中解放出来，专注于模型的创新和优化。</li></ul><p>这，就是LLMOps为大模型开发带来的工程纪律与效率革命。</p><h2 id=122-职业发展ai-infra工程师的能力矩阵与进阶路径>12.2 职业发展：AI Infra工程师的能力矩阵与进阶路径</h2><p>走到了这里，你可能正在思考：我掌握了这么多知识，未来该如何发展？AI Infra工程师这个角色，在未来AI浪潮中的定位是什么？它的天花板在哪里？</p><p>AI Infra工程师是一个典型的交叉学科角色，它要求从业者既要“上知天文（AI算法）”，又要“下知地理（底层硬件）”，还要“中通人和（平台与工程）”。这是一个挑战与机遇并存的领域，其职业路径宽广，天花板极高。</p><h3 id=1221-ai-infra工程师能力矩阵>12.2.1 AI Infra工程师能力矩阵</h3><p>我们可以从“技术广度”和“技术深度”两个维度，来构建一个AI Infra工程师的能力矩阵。</p><p>(横轴：技术广度——三大知识域)</p><ol><li><p>底层基础设施 (IaaS - Infrastructure as a Service):</p><ul><li>计算： 深入理解CPU、GPU、NPU的体系结构（如Ampere/Hopper, DaVinci），熟悉服务器硬件、BIOS配置。</li><li>网络： 精通TCP/IP协议栈，深入掌握RDMA技术（InfiniBand/RoCE），熟悉数据中心网络架构（如Fat-Tree）。</li><li>存储： 熟悉各种存储介质（HDD, SSD, NVMe），精通并行文件系统（Lustre/GPFS）和对象存储（S3/Ceph）的原理与运维。</li></ul></li><li><p>云原生与平台工程 (PaaS - Platform as a Service):</p><ul><li>容器技术： 精通Docker原理，熟悉NVIDIA Container Toolkit、Ascend Docker Runtime等AI容器化方案。</li><li>编排调度： 深入理解Kubernetes架构，精通K8s Device Plugin原理，熟练掌握Volcano/Yunikorn等批处理调度器的使用与配置。</li><li>可观测性： 精通Prometheus/Grafana技术栈，能够设计和构建全链路监控告警体系。</li><li>CI/CD & DevOps： 熟悉GitLab CI, Jenkins, ArgoCD等工具，能够构建自动化运维流水线。</li></ul></li><li><p>AI算法与框架 (SaaS - Software as a Service / MLaaS):</p><ul><li>AI基础知识： 理解机器学习和深度学习的基本原理，特别是Transformer模型的结构。</li><li>主流框架： 熟悉PyTorch, TensorFlow, MindSpore等框架的使用，特别是其分布式训练模块（如DDP）。</li><li>分布式并行策略： 深入理解数据并行、张量并行、流水线并行的原理、通信模式和适用场景。</li><li>LLMOps工具链： 熟悉MLflow, Kubeflow, WandB等工具，理解模型生命周期管理。</li></ul></li></ol><p>(纵轴：技术深度——四大进阶层级)</p><ol><li><p>L1: 执行与运维 (Operator)</p><ul><li>核心职责： 响应告警，处理工单，执行标准操作流程（SOP），保障系统稳定运行。</li><li>能力要求：<ul><li>熟练使用<code>nvidia-smi</code>, <code>npu-smi</code>, <code>kubectl</code>, <code>docker</code>等基础命令。</li><li>能够按照文档部署和配置Exporter、Device Plugin等组件。</li><li>能够处理常见的、有明确SOP的硬件故障（如换卡、换线）。</li><li>能够读懂Grafana大盘，识别基本异常。</li></ul></li></ul></li><li><p>L2: 优化与支持 (Specialist / SRE)</p><ul><li>核心职责： 解决疑难杂症，进行性能调优，为算法团队提供专家级支持，编写和完善SOP。</li><li>能力要求：<ul><li>精通DCGM/NPU-Exporter指标，能够独立搭建和定制Grafana大盘。</li><li>具备系统性的故障排查能力，能够处理NCCL Timeout、训练卡死等复杂问题。</li><li>精通Volcano/Yunikorn的调度策略，能够通过调整策略优化集群利用率。</li><li>能够编写自动化运维脚本（Python/Shell），提升运维效率。</li><li>对分布式并行策略有深入理解，能够帮助算法工程师分析和解决OOM、性能瓶颈等问题。</li></ul></li></ul></li><li><p>L3: 设计与构建 (Architect)</p><ul><li>核心职责： 负责整个AI平台的技术选型、架构设计和系统建设。</li><li>能力要求：<ul><li>对IaaS/PaaS/SaaS三个知识域都有深入的理解，能够做出权衡和决策（如IB vs RoCE, Volcano vs Yunikorn, vLLM vs TRT-LLM）。</li><li>具备从零开始构建一个千卡级别智算中心的能力，包括网络拓扑设计、存储选型、K8s集群规划等。</li><li>能够设计和主导LLMOps体系的建设，打通从训练到部署的全流程。</li><li>能够设计算力计费模型，并推动其在组织内落地。</li><li>具备优秀的技术前瞻性，能够洞察行业发展趋势，为公司的技术路线图提供建议。</li></ul></li></ul></li><li><p>L4: 战略与领导 (Leader / Director)</p><ul><li>核心职责： 制定AI基础设施的长期战略，管理团队，控制预算，向上对齐业务目标，向下赋能技术创新。</li><li>能力要求：<ul><li>深厚的行业洞察力和商业敏感度，能够将技术投入与公司的商业价值直接挂钩。</li><li>卓越的领导力和团队管理能力，能够吸引、培养和激励顶尖的AI Infra人才。</li><li>强大的沟通和协调能力，能够与算法、产品、财务、高管等不同角色进行有效沟通。</li><li>对整个AI产业链有宏观的认识，能够在供应链、开源生态、自主可控等战略层面进行布局。</li></ul></li></ul></li></ol><h3 id=1222-你的进阶路径>12.2.2 你的进阶路径</h3><ul><li><p>从L1到L2：深化专业技能，成为“排障专家”。</p><ul><li>行动项： 不要满足于执行SOP，要深入理解每一个故障背后的原理。主动去复现和研究NCCL Timeout、Loss NaN等问题。学习PromQL，尝试自己创建Grafana Dashboard。深入阅读一个开源项目（如Volcano）的源码，理解其调度逻辑。</li></ul></li><li><p>从L2到L3：拓宽技术视野，成为“方案设计师”。</p><ul><li>行动项： 跳出日常运维的舒适区，主动去思考“为什么我们当初要这么选型？”、“如果让我从头设计，我会怎么做？”。多去阅读业界顶尖公司（如Google, Meta, OpenAI）的技术博客和论文，了解他们的架构实践。尝试参与或主导一个新项目的设计，例如LLMOps平台的建设。开始关注成本，学习如何做TCO分析。</li></ul></li><li><p>从L3到L4：提升商业思维，成为“价值创造者”。</p><ul><li>行动项： 开始思考技术之外的问题。你设计的平台，如何能帮助公司的产品更快地推向市场？你的成本优化，如何能直接体现在公司的财报上？多与业务部门和产品经理交流，理解他们的痛点。学习如何管理项目、管理人、管理预算。开始在技术社区或行业会议上分享你的经验，建立个人和团队的影响力。</li></ul></li></ul><p>AI Infra的未来：挑战与机遇并存</p><p>AI Infra领域正处在一个前所未有的黄金时代。模型的规模仍在指数级增长，新的硬件架构层出不穷，多模态、端侧AI等新范式不断涌现。这一切，都对底层的AI基础设施提出了永无止境的新要求。</p><ul><li>未来的挑战：<ul><li>异构算力的融合与统一： 如何在一个平台中，无缝地管理和调度来自NVIDIA, Huawei, Google, AMD以及各种AI创业公司的芯片？</li><li>能效比的极致追求： 随着“万亿参数”成为常态，如何通过软件定义、液冷等技术，在PUE、HFU等指标上做到极致，将是衡量AI Infra能力的核心标准。</li><li>AI for Infra： 如何利用AI技术，来反向优化和管理AI基础设施自身？例如，使用AI模型来预测硬件故障、智能调度训练任务、自动诊断性能瓶颈。</li></ul></li></ul><p>你的机遇：</p><p>在这个时代，AI Infra工程师不再是传统IT部门中“支持业务”的后台角色。你们是AI时代的“军火商”和“能源供应商”，是驱动整个AI革命的核心引擎。你们的每一个决策，都深刻地影响着AI技术发展的速度和成本。</p><h2 id=结语>结语</h2><p>本书的旅程即将结束，但你的AI Infra征途才刚刚开始。希望这本书能成为你地图和指南针，在你遇到迷雾时为你指明方向，在你攀登高峰时为你提供支撑。</p><p>请永远保持好奇心，对底层原理刨根问底；请永远保持开放性，拥抱层出不穷的新技术和新思想；请永远保持实践的热情，在解决一个个真实的问题中，淬炼自己的技艺。</p><p>前路浩荡，未来已来。愿你在这波澜壮阔的AI时代，乘风破浪，实现自己的技术理想与价值。</p></div><div class=post_footer><div class=meta><div class=info><span class="field tags"><i class=ri-stack-line></i>
<a href=https://zhurongshuo.com/tags/%E4%B9%A6%E7%A8%BF/>书稿</a></span></div></div></div></div><div class=doc_comments></div></div></div></div><a id=back_to_top href=# class=back_to_top><i class=ri-arrow-up-s-line></i></a><footer class=footer><div class=powered_by><a href=https://varkai.com>Designed by VarKai, </a><a href=http://www.gohugo.io/>Proudly published with Hugo,</a></div><div class=footer_slogan><span>法不净空，觉无性也。</span></div><div class=powered_by style=margin-top:10px;font-size:14px><a href=https://zhurongshuo.com/>Copyright © 2010-2025 祝融说 zhurongshuo.com All Rights Reserved.</a></div></footer><script defer src=https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><link href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.css rel=stylesheet integrity="sha256-7qiTu3a8qjjWtcX9w+f2ulVUZSUdCZFEK62eRlmLmCE=" crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin=anonymous></script><script defer src=https://zhurongshuo.com/js/zozo.js></script><script type=text/javascript async src="https://cdn.jsdelivr.net/npm/mathjax@2.7.8/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["[[","]]"]],processEscapes:!0,processEnvironments:!0,skipTags:["script","noscript","style","textarea","pre"],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}}),MathJax.Hub.Queue(function(){var e,t=MathJax.Hub.getAllJax();for(e=0;e<t.length;e+=1)t[e].SourceElement().parentNode.className+=" has-jax"})</script><style>code.has-jax{font:inherit;font-size:100%;background:inherit;border:inherit;color:#515151}</style><script async src="https://www.googletagmanager.com/gtag/js?id=G-KKJ5ZEG1NB"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KKJ5ZEG1NB")</script></body></html>