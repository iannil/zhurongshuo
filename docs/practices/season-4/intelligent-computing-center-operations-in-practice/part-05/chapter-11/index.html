<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><meta name=google-site-verification content="8_xpI-TS3tNV8UPug-Q6Ef3BhKTcy0WOG7dEdAcm2zk"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="祝融"><link rel=dns-prefetch href=//cdn.jsdelivr.net><link rel=preconnect href=https://cdn.jsdelivr.net crossorigin><link rel=dns-prefetch href=//www.googletagmanager.com><link rel=preconnect href=https://www.googletagmanager.com crossorigin><link rel=canonical href=https://zhurongshuo.com/practices/season-4/intelligent-computing-center-operations-in-practice/part-05/chapter-11/><title>祝融说。 第11章：两级智算运营体系设计</title><meta property="og:title" content="第11章：两级智算运营体系设计"><meta property="og:url" content="https://zhurongshuo.com/practices/season-4/intelligent-computing-center-operations-in-practice/part-05/chapter-11/"><meta property="og:site_name" content="祝融说。"><meta property="og:locale" content="zh_CN"><meta property="og:type" content="article"><meta property="article:published_time" content="2025-11-29T00:00:00+08:00"><meta property="article:modified_time" content="2025-11-29T00:00:00+08:00"><meta property="article:tag" content="书稿"><meta name=description content="在前面的篇章中，我们投入了巨大的精力，构建了一座功能强大的智算“巨舰”。我们精通了它的引擎（GPU/NPU），熟悉了它的航道（网络与存储），掌握了它的驾驶术（调度与并行），甚至还演练了各种紧急情况下的损管流程。现在，是时候为这艘巨舰任命一位“舰长”，并为它制定一套远航的“作战条令”了。
"><meta property="og:description" content="在前面的篇章中，我们投入了巨大的精力，构建了一座功能强大的智算“巨舰”。我们精通了它的引擎（GPU/NPU），熟悉了它的航道（网络与存储），掌握了它的驾驶术（调度与并行），甚至还演练了各种紧急情况下的损管流程。现在，是时候为这艘巨舰任命一位“舰长”，并为它制定一套远航的“作战条令”了。
"><meta property="og:image" content="https://zhurongshuo.com/images/favicon.ico"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="第11章：两级智算运营体系设计"><meta name=twitter:description content="在前面的篇章中，我们投入了巨大的精力，构建了一座功能强大的智算“巨舰”。我们精通了它的引擎（GPU/NPU），熟悉了它的航道（网络与存储），掌握了它的驾驶术（调度与并行），甚至还演练了各种紧急情况下的损管流程。现在，是时候为这艘巨舰任命一位“舰长”，并为它制定一套远航的“作战条令”了。
"><meta name=twitter:image content="https://zhurongshuo.com/images/favicon.ico"><meta name=keywords content="智算中心运营实战：从基础设施到大模型全栈优化,第11章：两级智算运营体系设计"><link rel="shortcut icon" href=https://zhurongshuo.com/images/favicon.ico><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/normalize.css><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/animate-custom.css><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/zozo.css><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/remixicon-custom.css><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/highlight.css><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","headline":"第11章：两级智算运营体系设计","description":"在前面的篇章中，我们投入了巨大的精力，构建了一座功能强大的智算“巨舰”。我们精通了它的引擎（GPU\/NPU），熟悉了它的航道（网络与存储），掌握了它的驾驶术（调度与并行），甚至还演练了各种紧急情况下的损管流程。现在，是时候为这艘巨舰任命一位“舰长”，并为它制定一套远航的“作战条令”了。\n","datePublished":"2025-11-29T00:00:00\u002b08:00","dateModified":"2025-11-29T00:00:00\u002b08:00","author":{"@type":"Person","name":"祝融"},"publisher":{"@type":"Organization","name":"祝融说。","logo":{"@type":"ImageObject","url":"https:\/\/zhurongshuo.com\/images\/favicon.ico"}},"image":"https:\/\/zhurongshuo.com\/images\/favicon.ico","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/zhurongshuo.com\/practices\/season-4\/intelligent-computing-center-operations-in-practice\/part-05\/chapter-11\/"},"keywords":"书稿"}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"首页","item":"https:\/\/zhurongshuo.com\/"},{"@type":"ListItem","position":2,"name":"practices","item":"https:\/\/zhurongshuo.com\/practices/"},{"@type":"ListItem","position":3,"name":"第11章：两级智算运营体系设计","item":"https:\/\/zhurongshuo.com\/practices\/season-4\/intelligent-computing-center-operations-in-practice\/part-05\/chapter-11\/"}]}</script></head><body><div class="main animate__animated animate__fadeInDown"><div class="nav_container animated fadeInDown"><div class=site_nav id=site_nav><ul><li><a href=https://zhurongshuo.com/>首页</a></li><li><a href=https://zhurongshuo.com/start/>开始</a></li><li><a href=https://zhurongshuo.com/advanced/>进阶rc</a></li><li><a href=https://zhurongshuo.com/posts/>归档</a></li><li><a href=https://zhurongshuo.com/tags/>标签</a></li><li><a href=https://zhurongshuo.com/about/>关于</a></li></ul></div><div class=menu_icon><a id=menu_icon><i class=ri-menu-line></i></a></div></div><div class="header animated fadeInDown"><div class=site_title_container><div class=site_title><h1><a href=https://zhurongshuo.com/><span class=web-font>祝融说。</span></a></h1></div><div class=description><p class=sub_title>法不净空，觉无性也。</p><div class=my_socials><a href=https://zhurongshuo.com/books/ title=book-open><i class=ri-book-open-line></i></a>
<a href=https://zhurongshuo.com/practices/ title=trophy><i class=ri-trophy-line></i></a>
<a href=https://zhurongshuo.com/gallery/ title=gallery><i class=ri-gallery-line></i></a>
<a href=https://zhurongshuo.com/about/ title=game><i class=ri-game-line></i></a>
<a href type=application/rss+xml title=rss target=_blank><i class=ri-rss-fill></i></a></div></div></div></div><div class=content><div class=post_page><div class="post animate__animated animate__fadeInDown"><div class="post_title post_detail_title"><h2><a href=https://zhurongshuo.com/practices/season-4/intelligent-computing-center-operations-in-practice/part-05/chapter-11/>第11章：两级智算运营体系设计</a></h2><span class=date>2025.11.29</span></div><div class="post_content markdown"><p>在前面的篇章中，我们投入了巨大的精力，构建了一座功能强大的智算“巨舰”。我们精通了它的引擎（GPU/NPU），熟悉了它的航道（网络与存储），掌握了它的驾驶术（调度与并行），甚至还演练了各种紧急情况下的损管流程。现在，是时候为这艘巨舰任命一位“舰长”，并为它制定一套远航的“作战条令”了。</p><p>“两级智算”——即“总部训练、边缘推理”的协同架构——是我们为这座智算工厂设计的核心生产模式。然而，一个成功的体系，绝不仅仅是物理上的划分，更是一套流畅的、自动化的、可度量的运营流程。如果总部训练出的新模型，需要一周时间才能部署到边缘节点；如果业务部门使用了多少算力，成了一笔算不清的“糊涂账”，那么我们之前所有的技术投入都将大打折扣。</p><p>本章，我们将以“体系设计师”和“运营规划师”的身份，来解决两个至关重要的问题。首先，我们将设计一套完整的模型分发与同步机制，打通从总部到边缘的“最后一公里”，实现模型生命周期的闭环。接着，我们将深入探讨一个在企业内部极具挑战性、也极具价值的话题——算力计费模型，学习如何科学地为昂贵的AI算力定价，并建立公平、透明的内部结算体系。这不仅关乎成本控制，更关乎在整个企业内培育一种“珍惜算力、高效用云”的文化。</p><h2 id=111-总部-边缘协同模型分发机制与镜像仓库同步>11.1 总部-边缘协同：模型分发机制与镜像仓库同步</h2><p>在“两级智算”体系中，总部训练中心是“模型工厂”，边缘推理节点是“产品展厅”。连接这两者、确保新产品能够被快速、安全、可靠地送进展厅的，就是模型分发（Model Distribution）流水线。一套现代化的模型分发机制，必须具备自动化、版本化、安全性、高效性等特征，它本质上是一个面向模型的CI/CD（持续集成/持续部署）流程，是LLMOps体系的核心动脉。</p><h3 id=1111-挑战从u盘拷贝到全球同步>11.1.1 挑战：从“U盘拷贝”到“全球同步”</h3><p>在项目初期，模型分发可能非常原始：算法工程师训练好一个模型，将其打包成一个几十GB的压缩文件，然后通过FTP、对象存储甚至U盘，手动拷贝到推理服务器上。这种方式的弊端显而易见：</p><ul><li>效率低下： 纯手动操作，耗时耗力，容易出错。</li><li>版本混乱： 缺乏严格的版本控制，容易出现线上运行的模型版本与代码、数据对不上的情况。</li><li>安全性差： 模型文件在传输过程中可能被篡改或泄露。</li><li>无法扩展： 当推理节点从一台服务器扩展到全球多个地域的成百上千个节点时，手动分发将成为不可能完成的任务。</li></ul><h3 id=1112-现代化模型分发流水线设计>11.1.2 现代化模型分发流水线设计</h3><p>一个标准化的模型分发流水线，通常由模型仓库（Model Registry）、CI/CD引擎和镜像仓库（Image Registry）三大部分组成，它们协同工作，完成从“模型checkpiont”到“线上服务”的转化。</p><h4 id=模型仓库-model-registry模型的身份证与档案馆>模型仓库 (Model Registry)：模型的“身份证与档案馆”</h4><p>模型仓库是整个流程的起点和“单一事实来源（Single Source of Truth）”。它不是一个简单的文件存储，而是一个对模型进行版本化管理和元数据记录的系统。MLflow Model Registry和Hugging Face Hub是其中的优秀代表。</p><p>核心功能：</p><ul><li>版本化注册： 当一个训练任务（无论是预训练还是SFT）成功结束后，其输出的模型文件（权重、配置文件等）会被注册到模型仓库中，并被赋予一个唯一的版本号（如<code>Llama3-8B-SFT-v2.1.0</code>）。</li><li>元数据管理： 伴随模型一起被记录的，还有丰富的元数据：<ul><li>溯源信息： 这个模型是由哪个训练脚本、哪个版本的代码、哪个数据集、哪些超参数训练出来的？</li><li>性能指标： 它的评估结果（如Accuracy, BLEU）是多少？</li><li>环境依赖： 它依赖哪个版本的PyTorch、CUDA？</li></ul></li><li>生命周期阶段管理 (Staging)： 一个模型版本可以被标记为不同的阶段，如<code>Staging</code>（测试中）、<code>Production</code>（生产可用）、<code>Archived</code>（已归档）。这为模型的灰度发布和安全上线提供了流程保障。</li></ul><h4 id=cicd引擎自动化的模型包装工厂>CI/CD引擎：自动化的“模型包装工厂”</h4><p>CI/CD引擎（如Jenkins, GitLab CI, Tekton）是整个流水线的“调度中枢”。它会监听模型仓库的状态变化，并自动触发一系列的构建、测试和部署任务。</p><p>流水线触发器 (Trigger)：</p><p>可以配置一个Webhook。当模型仓库中有一个新版本被从<code>Staging</code>阶段推向<code>Production</code>阶段时，自动触发CI/CD流水线。</p><p>核心构建步骤 (Pipeline Stages)：</p><ul><li>模型优化与转换 (Optimization & Conversion):<ul><li>拉取模型： 流水线的第一步，是从模型仓库拉取被标记为<code>Production</code>的新版模型文件。</li><li>量化/剪枝： 调用量化工具（如AutoGPTQ, AWQ），将FP16的模型转换为INT8或INT4，以减小体积、提升推理速度。</li><li>编译为引擎格式： 如果后端使用TensorRT-LLM或MindIE，这一步会调用相应的编译器，将模型编译成<code>.engine</code>或<code>.om</code>格式的优化后文件。</li></ul></li><li>构建推理镜像 (Image Building):<ul><li>将上一步优化好的模型文件，连同一个轻量级的推理服务器（如vLLM或Triton的封装），打包成一个新的Docker镜像。</li><li>这个镜像的Dockerfile应该基于我们在第四章学习的最佳实践来编写，确保其足够轻量和安全。</li><li>镜像的Tag应该与模型版本强关联，例如<code>my-registry/llama3-8b-sft-service:v2.1.0</code>。</li></ul></li><li>推送镜像 (Image Pushing):<ul><li>将构建好的新版镜像，推送到总部的中央镜像仓库（如Harbor, Artifactory）。</li></ul></li></ul><h4 id=镜像仓库同步连接总部与边缘的物流网络>镜像仓库同步：连接总部与边缘的“物流网络”</h4><p>当推理节点分布在全球各地的数据中心或边缘站点时，让所有节点都从总部的中央镜像仓库拉取镜像是低效且不可靠的。我们需要一个分层、分布式的镜像仓库体系。</p><p>架构设计：</p><ul><li>中央仓库（HQ Registry）： 位于总部数据中心，是所有镜像的权威来源。</li><li>区域/边缘仓库（Edge Registry）： 在每一个主要的地理区域或边缘数据中心，都部署一个镜像仓库的“拉取式缓存”或“只读副本”。</li><li>同步机制： 使用镜像仓库自带的复制（Replication）功能。例如，在Harbor中，可以配置复制规则：<ul><li>源： 总部仓库中的<code>production-models</code>项目。</li><li>目标： 所有边缘仓库。</li><li>触发方式： 事件驱动 (Event-based)。当总部仓库有新镜像被推入时，立即自动触发同步任务，将该镜像分发到所有边缘仓库。</li></ul></li></ul><p>工作流程：</p><ol><li>CI/CD流水线将新构建的<code>...:v2.1.0</code>镜像推送到总部中央仓库。</li><li>中央仓库的事件触发器被激活，启动到所有边缘仓库的复制任务。</li><li>镜像通过专线或公网，被高效地同步到各个边缘节点。</li><li>边缘站点的Kubernetes集群在部署新模型时，会配置为优先从本地的边缘仓库拉取镜像。这极大地加快了部署速度，并减少了对总部出口带宽的依赖。</li></ol><h4 id=边缘部署-edge-deployment>边缘部署 (Edge Deployment)</h4><p>当镜像同步到边缘仓库后，总部的CD系统（如ArgoCD）会通过GitOps的方式，更新边缘K8s集群中对应服务的部署配置（如Deployment的image tag），从而触发滚动更新，将新模型平滑上线。</p><p>总结：模型分发的“高速公路”</p><ol><li>起点： 算法工程师在模型仓库中将一个模型版本标记为<code>Production</code>。</li><li>触发： CI/CD引擎监听到事件，自动启动流水线。</li><li>加工： 流水线对模型进行优化、编译，并打包成一个标准化的Docker镜像。</li><li>入库： 新镜像被推送到总部中央镜像仓库。</li><li>物流： 镜像仓库的复制机制被触发，将新镜像自动同步到全球各地的边缘仓库。</li><li>上架： GitOps系统更新边缘集群的部署配置，触发服务滚动更新，新模型上线。</li></ol><p>这套自动化的体系，将原本需要数天甚至数周的手动流程，缩短到了几十分钟，是实现敏捷LLMOps、快速迭代模型能力的核心保障。</p><h2 id=112-算力计费模型如何设计内部结算单价按卡时-vs-按token>11.2 算力计费模型：如何设计内部结算单价（按卡时 vs 按Token）</h2><p>智算中心的建设和运营成本是惊人的。一块H800 GPU的售价高达数十万，一个千卡集群每年的电费就可能超过千万元。如果不建立一套科学的成本核算与内部结算机制，算力资源将很快被滥用和浪费，最终成为企业沉重的负担。</p><p>设计内部计费模型的目的，不仅仅是“分摊成本”，更是通过价格杠杆，引导用户（内部的业务部门或算法团队）更高效、更节约地使用算力。</p><p>目前，主流的内部计费模型主要有两种：“基础设施”视角的按时计费 和 “服务”视角的按量计费。</p><h3 id=1121-按卡时计费-gpu-hour-billing简单直接的包场模式>11.2.1 按卡时计费 (GPU-Hour Billing)：简单直接的“包场”模式</h3><p>这是最基础、最容易实现的计费模式。它将GPU/NPU视为一种类似云服务器的资源，按照“资源类型 * 占用数量 * 占用时间”来计费。</p><ul><li>计费公式：<code>费用 = 单价 (元/卡·时) * GPU数量 * 使用时长 (小时)</code></li><li>如何制定“单价”？—— TCO核算法
“单价”的制定，是这种模式的核心。一个合理的单价，应该能覆盖这块GPU全生命周期的总拥有成本（TCO - Total Cost of Ownership）。<ol><li>硬件成本摊销 (Hardware Amortization):<ul><li><code>C_hw = (服务器成本 + GPU成本 + 网络设备成本/端口) / (折旧年限 * 365 * 24)</code></li><li>服务器成本包括CPU、内存、硬盘等。</li><li>网络设备成本需要按端口均摊。</li><li>折旧年限通常取3-5年。</li></ul></li><li>电力成本 (Power Cost):<ul><li><code>C_power = (GPU功耗 + 服务器其他部件功耗) * PUE * 电价</code></li><li>PUE (Power Usage Effectiveness): 数据中心的能源效率指标，通常在1.2-1.5之间。PUE=1.3意味着IT设备每消耗1度电，数据中心总共要消耗1.3度电（额外的用在制冷、照明等）。</li><li>电价需要考虑峰谷电价。</li></ul></li><li>数据中心托管成本 (IDC Colocation Cost):<ul><li><code>C_idc = (机柜租金 + 带宽费用) / (机柜内容纳的GPU数量 * 24 * 30)</code></li><li>这部分通常按机柜每月费用来计算，再摊到单卡上。</li></ul></li><li>人力和软件成本 (O&amp;M, Software Cost):<ul><li><code>C_sw_om = (AI Infra团队人力成本 + 商业软件许可费) / (集群总GPU数 * 24 * 30)</code></li></ul></li></ol></li></ul><p>最终单价 (元/卡·时) = <code>(C_hw + C_power + C_idc + C_sw_om) * (1 + 利润率)</code></p><p><code>利润率</code>：即便是内部结算，也通常会增加一个10-20%的“利润率”或“资源池发展基金”，用于未来的技术升级和扩容。</p><p>数据采集与出账：</p><ul><li><p>数据源： K8s的调度日志（Pod的创建和销毁时间）、Device Plugin的分配记录、Volcano/Yunikorn的作业事件。</p></li><li><p>流程：</p><ol><li>定期（如每小时）扫描所有正在运行的、且申请了GPU资源的Pod。</li><li>记录下每个Pod的<code>namespace</code>（代表业务部门）、<code>pod_name</code>、申请的GPU数量、Pod的生命周期。</li><li>月末，汇总每个<code>namespace</code>下所有Pod的“卡时”消耗，乘以单价，生成账单。</li></ol></li><li><p>优点：</p><ul><li>实现简单： 数据源清晰，逻辑直接。</li><li>成本回收稳定： 只要卡被分配出去，就能产生收入，便于财务预测。</li></ul></li><li><p>缺点：</p><ul><li>无法反映真实使用效率： 一个用户申请了8张卡跑了10小时，但如果他的代码写得很差，GPU利用率（HFU）只有10%，他付的钱和一个将GPU利用率跑到90%的用户是一样的。这无法激励用户去优化自己的程序。</li><li>不适用于推理服务： 推理服务是共享的，多个用户同时使用，无法简单地按“占用”来计费。</li></ul></li></ul><p>1.2.2 按Token计费 (Token-based Billing)：精细化的“按劳付费”模式</p><p>这种模式主要应用于推理服务的计费，它完全从“业务价值”出发，用户使用了多少，就付多少钱。这与OpenAI等公有云服务的计费方式完全一致。</p><ul><li><p>计费公式：<code>费用 = 输入Token单价 * 输入Token总量 + 输出Token单价 * 输出Token总量</code></p></li><li><p>为什么输入和输出要分开计费？</p><ul><li>处理输入（Prompt Processing）和生成输出（Decoding）在计算上是不同的。</li><li>通常，处理输入的计算强度更大（并行处理），但只做一次；生成输出是串行的，但要做很多步。</li><li>分开定价，可以更精细地反映成本结构，并鼓励用户优化他们的Prompt（例如，使用更简短、更有效的Prompt）。</li></ul></li><li><p>如何制定“单价”？——反向成本推算
Token单价的制定，是一个“逆向工程”，需要结合成本、吞吐量和预期的利润。</p><ol><li>计算单卡小时成本 (<code>Cost_per_GPU_Hour</code>):<ul><li>使用11.2.1中计算出的TCO单价。假设是10元/卡·时。</li></ul></li><li>压测获取单卡性能 (<code>TPS_per_GPU</code>):<ul><li>使用Locust等工具，对部署了目标模型的单卡服务进行压力测试（见8.3节）。</li><li>找到在满足SLA（如99% TTFT &lt; 500ms）前提下的最大可持续Token生成速率 (TPS)。假设对于Llama 3-8B模型，压测出单张A800的TPS是500 tokens/sec。</li></ul></li><li>计算每Token成本 (<code>Cost_per_Token</code>):<ul><li><code>单卡每秒成本 = Cost_per_GPU_Hour / 3600 = 10 / 3600 ≈ 0.00278 元/秒</code></li><li><code>每生成1个Token的成本 = 单卡每秒成本 / TPS_per_GPU = 0.00278 / 500 ≈ 0.00000556元</code></li></ul></li><li>制定最终售价 (Price):<ul><li><code>Price_per_Token = Cost_per_Token * (1 + 利润率)</code></li><li><code>Price_per_1K_Tokens ≈ 0.00000556 * 1000 * 1.2 (20%利润率) ≈ 0.0067元</code></li><li>为了便于市场比较，通常以“元/百万Token”为单位。<code>0.0067 * 1000 = 6.7元/百万Token</code>。</li></ul></li></ol></li></ul><p>数据采集与出账：</p><ul><li><p>数据源： 推理服务的API网关或服务本身产生的访问日志。每一条请求的日志中，都必须包含用户身份、输入Token数、输出Token数。</p></li><li><p>流程：</p><ol><li>所有推理请求都经过一个统一的API网关。</li><li>网关负责认证用户身份，并记录下每次调用的Token详情。</li><li>将这些日志数据准实时地送入一个计费数据库或数据仓库。</li><li>月末，按用户ID聚合Token消耗量，乘以单价，生成账单。</li></ol></li><li><p>优点：</p><ul><li>公平且精细： 完全按实际使用量计费，对用户最公平。</li><li>激励优化： 鼓励用户使用更短的Prompt，或者选择更小的模型来完成任务，自然地引导了成本优化。</li><li>与业务价值挂钩： 计费模型与业务调用的次数直接关联，易于业务部门理解和做预算。</li></ul></li><li><p>缺点：</p><ul><li>实现复杂： 需要构建一套完整的API网关、认证、日志采集和计费数据处理系统。</li><li>定价困难： 单价的制定依赖于准确的压测，且需要根据模型、硬件的不同，制定一个复杂的价格表。</li></ul></li></ul><h3 id=1123-混合模式两级运营体系的最佳实践>11.2.3 混合模式：两级运营体系的最佳实践</h3><p>在“两级智算”体系中，单一的计费模式是不够的。最佳实践是采用混合计费模式：</p><ul><li>对总部训练中心：采用“按卡时计费”<ul><li>训练任务的特点是长时间、独占式地使用大量GPU。按卡时计费最符合其资源使用模式。</li><li>高级玩法：引入利用率惩罚/奖励机制。<ul><li>可以对“卡时”单价进行浮动。例如，如果一个用户的作业平均HFU低于20%，则其卡时单价上浮20%作为“资源浪费惩罚”。如果平均MFU高于50%，则其单价下调10%作为“高效使用奖励”。</li><li>这需要将计费系统与监控系统深度打通，但能极大地激励用户优化其训练任务。</li></ul></li></ul></li><li>对边缘推理节点：采用“按Token计费”<ul><li>推理服务是共享的、多租户的在线服务，按Token计费是唯一的科学方法。</li><li>AI Infra平台团队作为推理服务的“提供商”，向调用该服务的业务部门进行内部结算。</li><li>业务部门无需关心底层用了多少张卡，只需要为自己产生的API调用付费。</li></ul></li></ul><p>总结：</p><p>算力计费是智算中心运营从“技术”走向“经营”的关键一步。它将技术成本以一种可度量、可理解的方式，传递给了最终的业务使用者。通过设计一套“按卡时计日”为主、“按Token计费”为辅，并结合利用率奖惩的混合计费模型，AI Infra团队不仅能有效地回收成本，更能扮演起“算力优化顾问”的角色，驱动整个组织的技术和文化向“精益智算（Lean AI Computing）”的方向发展。</p></div><div class=post_footer><div class=meta><div class=info><span class="field tags"><i class=ri-stack-line></i>
<a href=https://zhurongshuo.com/tags/%E4%B9%A6%E7%A8%BF/>书稿</a></span></div></div></div></div><div class=doc_comments></div></div></div></div><a id=back_to_top href=# class=back_to_top><i class=ri-arrow-up-s-line></i></a><footer class=footer><div class=powered_by><a href=https://varkai.com>Designed by VarKai, </a><a href=http://www.gohugo.io/>Proudly published with Hugo,</a></div><div class=footer_slogan><span>法不净空，觉无性也。</span></div><div class=powered_by style=margin-top:10px;font-size:14px><a href=https://zhurongshuo.com/>Copyright © 2010-2025 祝融说 zhurongshuo.com All Rights Reserved.</a></div></footer><script defer src=https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><link href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.css rel=stylesheet integrity="sha256-7qiTu3a8qjjWtcX9w+f2ulVUZSUdCZFEK62eRlmLmCE=" crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin=anonymous></script><script defer src=https://zhurongshuo.com/js/zozo.js></script><script type=text/javascript async src="https://cdn.jsdelivr.net/npm/mathjax@2.7.8/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["[[","]]"]],processEscapes:!0,processEnvironments:!0,skipTags:["script","noscript","style","textarea","pre"],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}}),MathJax.Hub.Queue(function(){var e,t=MathJax.Hub.getAllJax();for(e=0;e<t.length;e+=1)t[e].SourceElement().parentNode.className+=" has-jax"})</script><style>code.has-jax{font:inherit;font-size:100%;background:inherit;border:inherit;color:#515151}</style><script async src="https://www.googletagmanager.com/gtag/js?id=G-KKJ5ZEG1NB"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KKJ5ZEG1NB")</script></body></html>