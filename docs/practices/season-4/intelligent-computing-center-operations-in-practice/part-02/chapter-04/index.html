<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><meta name=google-site-verification content="8_xpI-TS3tNV8UPug-Q6Ef3BhKTcy0WOG7dEdAcm2zk"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="祝融"><link rel=dns-prefetch href=//cdn.jsdelivr.net><link rel=preconnect href=https://cdn.jsdelivr.net crossorigin><link rel=dns-prefetch href=//www.googletagmanager.com><link rel=preconnect href=https://www.googletagmanager.com crossorigin><link rel=canonical href=https://zhurongshuo.com/practices/season-4/intelligent-computing-center-operations-in-practice/part-02/chapter-04/><title>祝融说。 第4章：AI容器化技术</title><meta property="og:title" content="第4章：AI容器化技术"><meta property="og:url" content="https://zhurongshuo.com/practices/season-4/intelligent-computing-center-operations-in-practice/part-02/chapter-04/"><meta property="og:site_name" content="祝融说。"><meta property="og:locale" content="zh_CN"><meta property="og:type" content="article"><meta property="article:published_time" content="2025-11-29T00:00:00+08:00"><meta property="article:modified_time" content="2025-11-29T00:00:00+08:00"><meta property="article:tag" content="书稿"><meta name=description content="在第一篇中，我们完成了智算中心“地基”的建设。我们深入了AI芯片的内核，铺设了RDMA的高速网络，并构建了并行的存储系统。现在，我们拥有了强大的、但却原始的裸金属算力。这就像我们拥有了一座装备精良的工厂，但里面还没有标准化的生产线。直接在裸金属服务器上部署AI应用，会让我们迅速陷入“依赖地狱”——不同项目的Python版本冲突、系统库不兼容、环境难以迁移和复现。这在追求快速迭代的AI时代是不可接受的。
"><meta property="og:description" content="在第一篇中，我们完成了智算中心“地基”的建设。我们深入了AI芯片的内核，铺设了RDMA的高速网络，并构建了并行的存储系统。现在，我们拥有了强大的、但却原始的裸金属算力。这就像我们拥有了一座装备精良的工厂，但里面还没有标准化的生产线。直接在裸金属服务器上部署AI应用，会让我们迅速陷入“依赖地狱”——不同项目的Python版本冲突、系统库不兼容、环境难以迁移和复现。这在追求快速迭代的AI时代是不可接受的。
"><meta property="og:image" content="https://zhurongshuo.com/images/favicon.ico"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="第4章：AI容器化技术"><meta name=twitter:description content="在第一篇中，我们完成了智算中心“地基”的建设。我们深入了AI芯片的内核，铺设了RDMA的高速网络，并构建了并行的存储系统。现在，我们拥有了强大的、但却原始的裸金属算力。这就像我们拥有了一座装备精良的工厂，但里面还没有标准化的生产线。直接在裸金属服务器上部署AI应用，会让我们迅速陷入“依赖地狱”——不同项目的Python版本冲突、系统库不兼容、环境难以迁移和复现。这在追求快速迭代的AI时代是不可接受的。
"><meta name=twitter:image content="https://zhurongshuo.com/images/favicon.ico"><meta name=keywords content="智算中心运营实战：从基础设施到大模型全栈优化,第4章：AI容器化技术"><link rel="shortcut icon" href=https://zhurongshuo.com/images/favicon.ico><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/normalize.css><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/animate-custom.css><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/zozo.css><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/remixicon-custom.css><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/highlight.css><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","headline":"第4章：AI容器化技术","description":"在第一篇中，我们完成了智算中心“地基”的建设。我们深入了AI芯片的内核，铺设了RDMA的高速网络，并构建了并行的存储系统。现在，我们拥有了强大的、但却原始的裸金属算力。这就像我们拥有了一座装备精良的工厂，但里面还没有标准化的生产线。直接在裸金属服务器上部署AI应用，会让我们迅速陷入“依赖地狱”——不同项目的Python版本冲突、系统库不兼容、环境难以迁移和复现。这在追求快速迭代的AI时代是不可接受的。\n","datePublished":"2025-11-29T00:00:00\u002b08:00","dateModified":"2025-11-29T00:00:00\u002b08:00","author":{"@type":"Person","name":"祝融"},"publisher":{"@type":"Organization","name":"祝融说。","logo":{"@type":"ImageObject","url":"https:\/\/zhurongshuo.com\/images\/favicon.ico"}},"image":"https:\/\/zhurongshuo.com\/images\/favicon.ico","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/zhurongshuo.com\/practices\/season-4\/intelligent-computing-center-operations-in-practice\/part-02\/chapter-04\/"},"keywords":"书稿"}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"首页","item":"https:\/\/zhurongshuo.com\/"},{"@type":"ListItem","position":2,"name":"practices","item":"https:\/\/zhurongshuo.com\/practices/"},{"@type":"ListItem","position":3,"name":"第4章：AI容器化技术","item":"https:\/\/zhurongshuo.com\/practices\/season-4\/intelligent-computing-center-operations-in-practice\/part-02\/chapter-04\/"}]}</script></head><body><div class="main animate__animated animate__fadeInDown"><div class="nav_container animated fadeInDown"><div class=site_nav id=site_nav><ul><li><a href=https://zhurongshuo.com/>首页</a></li><li><a href=https://zhurongshuo.com/start/>开始</a></li><li><a href=https://zhurongshuo.com/advanced/>进阶rc</a></li><li><a href=https://zhurongshuo.com/posts/>归档</a></li><li><a href=https://zhurongshuo.com/tags/>标签</a></li><li><a href=https://zhurongshuo.com/about/>关于</a></li></ul></div><div class=menu_icon><a id=menu_icon><i class=ri-menu-line></i></a></div></div><div class="header animated fadeInDown"><div class=site_title_container><div class=site_title><h1><a href=https://zhurongshuo.com/><span class=web-font>祝融说。</span></a></h1></div><div class=description><p class=sub_title>法不净空，觉无性也。</p><div class=my_socials><a href=https://zhurongshuo.com/books/ title=book-open><i class=ri-book-open-line></i></a>
<a href=https://zhurongshuo.com/practices/ title=trophy><i class=ri-trophy-line></i></a>
<a href=https://zhurongshuo.com/gallery/ title=gallery><i class=ri-gallery-line></i></a>
<a href=https://zhurongshuo.com/about/ title=game><i class=ri-game-line></i></a>
<a href type=application/rss+xml title=rss target=_blank><i class=ri-rss-fill></i></a></div></div></div></div><div class=content><div class=post_page><div class="post animate__animated animate__fadeInDown"><div class="post_title post_detail_title"><h2><a href=https://zhurongshuo.com/practices/season-4/intelligent-computing-center-operations-in-practice/part-02/chapter-04/>第4章：AI容器化技术</a></h2><span class=date>2025.11.29</span></div><div class="post_content markdown"><p>在第一篇中，我们完成了智算中心“地基”的建设。我们深入了AI芯片的内核，铺设了RDMA的高速网络，并构建了并行的存储系统。现在，我们拥有了强大的、但却原始的裸金属算力。这就像我们拥有了一座装备精良的工厂，但里面还没有标准化的生产线。直接在裸金属服务器上部署AI应用，会让我们迅速陷入“依赖地狱”——不同项目的Python版本冲突、系统库不兼容、环境难以迁移和复现。这在追求快速迭代的AI时代是不可接受的。</p><p>答案是容器化。容器技术（以Docker为代表）通过将应用及其所有依赖打包到一个轻量、可移植的“集装箱”中，实现了完美的隔离和环境一致性。它正是我们为AI工厂构建标准化生产线所需要的核心技术。</p><p>然而，让AI应用“住进”容器并非易事。标准的容器与底层的GPU/NPU硬件之间存在一道天然的鸿沟。本章，我们将手把手地为你打通这条路。我们将首先解决最基础的问题：如何构建一个能“看见”并使用GPU/NPU的容器环境。接着，我们将把这个能力接入到Kubernetes的宏大叙事中，让K8s能够像调度CPU一样智能地调度和管理AI算力。最后，我们将传授“镜像工程”的独门秘籍，教你如何为大模型打造轻量、高效、可跨平台部署的标准化镜像。掌握本章内容，你将能构建起云原生AI平台的第一个、也是最核心的支柱。</p><h2 id=41-基础环境构建nvidia-container-toolkit-与-ascend-docker-runtime>4.1 基础环境构建：NVIDIA Container Toolkit 与 Ascend Docker Runtime</h2><h3 id=411-问题的根源容器的视而不见>4.1.1 问题的根源：容器的“视而不见”</h3><p>让我们从一个简单的实验开始。在一台已经正确安装了NVIDIA驱动的主机上，打开终端，输入<code>nvidia-smi</code>，你会看到GPU的详细信息。现在，我们启动一个标准的Ubuntu容器，并尝试在容器内执行同样的命令：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># 在主机上执行，成功显示GPU信息</span>
</span></span><span class=line><span class=cl>$ nvidia-smi
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 启动一个标准Ubuntu容器</span>
</span></span><span class=line><span class=cl>$ docker run --rm -it ubuntu:20.04 /bin/bash
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 在容器内执行，命令不存在或报错</span>
</span></span><span class=line><span class=cl>root@container:/# nvidia-smi
</span></span><span class=line><span class=cl>bash: nvidia-smi: <span class=nb>command</span> not found
</span></span></code></pre></div><p>为什么会这样？原因在于容器的核心隔离机制：Linux命名空间（Namespaces）。容器拥有自己独立的文件系统、进程空间和网络栈。它与主机（Host）是隔离的，默认情况下，它“看不见”主机上的设备文件（如<code>/dev/nvidia0</code>）和驱动库（如<code>libcuda.so</code>）。</p><p>为了打破这层隔离，让容器能够访问到GPU/NPU，我们需要一个“中间人”或“翻译官”。这个角色，就由NVIDIA的Container Toolkit和华为的Ascend Docker Runtime来扮演。</p><h3 id=412-nvidia-container-toolkit为容器注入cuda之力>4.1.2 NVIDIA Container Toolkit：为容器注入CUDA之力</h3><p>NVIDIA Container Toolkit是一套组件，它扩展了标准的容器运行时（如Docker的runc），使其能够感知并利用NVIDIA GPU。</p><h4 id=核心组件剖析>核心组件剖析</h4><ol><li><code>libnvidia-container</code>: 这是一个核心库，提供了与NVIDIA驱动交互的底层API，负责查询GPU信息、配置容器环境等。</li><li><code>nvidia-container-cli</code>: 一个命令行工具，供容器运行时调用，它会使用<code>libnvidia-container</code>来准备GPU环境。</li><li><code>nvidia-container-runtime</code>: 这才是关键的“翻译官”。它是一个自定义的OCI（Open Container Initiative）运行时，它会拦截标准的容器创建请求。当它发现请求需要GPU时，它会调用<code>nvidia-container-cli</code>，将必要的NVIDIA驱动文件、设备节点和库文件动态地挂载（mount）到容器的命名空间内，然后再调用标准的运行时（runc）来启动容器。</li></ol><h4 id=工作流程>工作流程</h4><ol><li>用户执行<code>docker run --gpus all ...</code>。</li><li>Docker Daemon收到请求，看到<code>--gpus</code>参数，知道需要GPU。</li><li>Docker Daemon不直接调用默认的<code>runc</code>，而是调用<code>nvidia-container-runtime</code>。</li><li><code>nvidia-container-runtime</code>接管请求，它调用<code>nvidia-container-cli</code>去查询主机上有哪些GPU设备，以及驱动库在哪里。</li><li><code>nvidia-container-cli</code>将这些设备文件（如<code>/dev/nvidia0</code>, <code>/dev/nvidia-uvm</code>）和库文件自动添加到容器的配置中。</li><li>最后，<code>nvidia-container-runtime</code>带着这份“增强版”的配置，调用<code>runc</code>来创建和启动容器。</li><li>最终，容器启动时，内部就已经有了访问GPU所需的一切。</li></ol><h4 id=安装与配置实战>安装与配置实战</h4><ol><li>确保NVIDIA驱动已安装。 这是所有工作的前提。</li><li>添加NVIDIA的软件源</li></ol><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey <span class=p>|</span> sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
</span></span><span class=line><span class=cl>curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list <span class=p>|</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>sed <span class=s1>&#39;s#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g&#39;</span> <span class=p>|</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
</span></span></code></pre></div><ol start=3><li>安装Toolkit</li></ol><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>sudo apt-get update
</span></span><span class=line><span class=cl>sudo apt-get install -y nvidia-container-toolkit
</span></span></code></pre></div><ol start=4><li>配置Docker Daemon并重启</li></ol><p>安装脚本通常会自动配置Docker。你可以检查<code>/etc/docker/daemon.json</code>文件，确保<code>nvidia-container-runtime</code>被设置为了默认运行时。然后重启Docker服务：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>sudo systemctl restart docker
</span></span></code></pre></div><ol start=5><li>验证安装</li></ol><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>docker run --rm --gpus all nvidia/cuda:12.1.0-base-ubuntu22.04 nvidia-smi
</span></span></code></pre></div><p>如果能成功打印出GPU信息，说明你的基础环境已经构建完成！</p><p>环境变量的妙用：</p><p>Toolkit还会向容器内注入一些关键的环境变量，用于精细化控制GPU的使用：</p><ul><li><code>NVIDIA_VISIBLE_DEVICES</code>: 这是最重要的一个。它的值决定了容器内“可见”的GPU。可以是<code>all</code>，也可以是GPU的索引（如<code>0,1</code>）或UUID。<code>docker run --gpus '"device=0,1"'</code> 最终就会体现为容器内的 <code>NVIDIA_VISIBLE_DEVICES=0,1</code>。</li><li><code>NVIDIA_DRIVER_CAPABILITIES</code>: 控制挂载到容器内的驱动库。默认是<code>compute,utility</code>，对于图形应用可能需要<code>graphics</code>。</li></ul><h3 id=413-ascend-docker-runtime昇腾平台的容器化基石>4.1.3 Ascend Docker Runtime：昇腾平台的容器化基石</h3><p>与NVIDIA的思路类似，华为也提供了一套机制来让容器使用昇腾NPU。这套机制通常作为CANN（异构计算架构）软件包的一部分提供。</p><h4 id=核心组件与原理>核心组件与原理</h4><ul><li>Ascend Docker Runtime: 同样是一个自定义的Docker运行时。它的作用是在启动容器时，将主机的CANN驱动、固件、必要的库文件以及NPU设备文件（如<code>/dev/davinci0</code>, <code>/dev/devmm_svm</code>）挂载到容器内部。</li><li>与NVIDIA的不同： 相比NVIDIA Toolkit的独立安装，Ascend Docker Runtime的安装和配置通常与CANN Toolkit的安装过程更紧密地绑定在一起。</li></ul><h4 id=安装与配置实战-1>安装与配置实战</h4><ol><li>确保CANN软件包已正确安装。 这包括驱动、固件和工具包。</li><li>安装Ascend Docker Runtime：CANN的安装包中通常会包含一个名为<code>ascend-docker-runtime</code>的deb或rpm包。</li></ol><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># 假设在CANN的安装目录下</span>
</span></span><span class=line><span class=cl>sudo dpkg -i ascend-docker-runtime_*.deb
</span></span></code></pre></div><ol start=3><li>配置Docker Daemon</li></ol><p>编辑<code>/etc/docker/daemon.json</code>文件，添加Ascend运行时，并可能将其设为默认。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;runtimes&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;ascend&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=nt>&#34;path&#34;</span><span class=p>:</span> <span class=s2>&#34;/usr/local/bin/ascend-docker-runtime&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=nt>&#34;runtimeArgs&#34;</span><span class=p>:</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;default-runtime&#34;</span><span class=p>:</span> <span class=s2>&#34;ascend&#34;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><ol start=4><li>重启Docker服务</li></ol><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>sudo systemctl restart docker
</span></span></code></pre></div><ol start=5><li>验证安装</li></ol><p>启动容器时，需要使用<code>--device</code>参数来手动指定要映射的NPU设备。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>docker run -it --device<span class=o>=</span>/dev/davinci0 --device<span class=o>=</span>/dev/davinci_manager --device<span class=o>=</span>/dev/devmm_svm --device<span class=o>=</span>/dev/hisi_hdc ascendhub.huawei.com/public-ascendhub/mindspore-ascend:2.2.11-cann7.0.1-py39-euleros2.10-aarch64 npu-smi info
</span></span></code></pre></div><p>如果能看到NPU 0的信息，则说明配置成功。</p><p>环境变量：昇腾容器同样依赖环境变量来指定使用的设备，最核心的是<code>ASCEND_VISIBLE_DEVICES</code>，其作用与<code>NVIDIA_VISIBLE_DEVICES</code>完全相同。</p><p>小结： 无论NVIDIA还是华为昇腾，其容器化方案的核心思想都是一致的：提供一个特权的、了解底层硬件的自定义运行时，在标准容器启动流程中“做手脚”，将主机上的驱动和设备“偷渡”到容器的隔离环境中。 完成这一步，我们就为在K8s中管理AI算力铺平了道路。</p><h2 id=42-k8s-device-plugin原理如何让k8s看见并分配gpunpu>4.2 K8s Device Plugin原理：如何让K8s“看见”并分配GPU/NPU</h2><p>在单机上通过<code>--gpus</code>或<code>--device</code>参数运行容器只是第一步。在真实的智算中心，我们面对的是由成百上千台服务器组成的庞大集群。我们不可能手动<code>ssh</code>到每台机器去执行<code>docker run</code>。我们需要一个“中央大脑”——Kubernetes——来统一调度和管理这些宝贵的AI算力。</p><p>问题来了：K8s天生只认识CPU和Memory这两种资源。它如何知道<code>node-01</code>上有8张H800，而<code>node-02</code>上有8张910B？答案就是 K8s Device Plugin（设备插件）框架。</p><h3 id=421-device-plugink8s的硬件翻译官>4.2.1 Device Plugin：K8s的“硬件翻译官”</h3><p>Device Plugin是K8s提供的一套标准的、开放的扩展机制，允许第三方硬件厂商将自己的专有硬件（如GPU、NPU、FPGA、高性能网卡等）注册为K8s集群中的一等公民资源，使其可以被调度、被申请。</p><p>工作模式：</p><ul><li>Device Plugin本身通常是一个Pod，以DaemonSet的形式运行在集群中的每个（或指定标签的）计算节点上。</li><li>它通过一个定义好的gRPC接口，与该节点上的Kubelet进行通信。通信使用的Unix Socket文件通常位于<code>/var/lib/kubelet/device-plugins/kubelet.sock</code>。</li></ul><h3 id=422-device-plugin的生命周期核心原理>4.2.2 Device Plugin的生命周期（核心原理）</h3><p>理解Device Plugin的工作流程，是理解K8s如何管理AI算力的关键。整个过程可以分为三步：</p><h4 id=第一步发现与注册-registration>第一步：发现与注册 (Registration)</h4><ul><li>当Device Plugin的Pod在某个节点上启动后，它首先会扫描该节点，通过调用<code>nvidia-smi</code>或<code>npu-smi</code>等原生工具，发现节点上存在哪些AI加速卡及其ID。</li><li>然后，它会连接到Kubelet的gRPC服务，并调用<code>Register</code>方法。</li><li>在这次调用中，它会告诉Kubelet：“你好，我是NVIDIA的插件，我提供一种名为 <code>nvidia.com/gpu</code> 的新资源。请记录一下。”（华为的插件则会注册如<code>huawei.com/npu</code>的资源）。</li><li>Kubelet收到注册后，就知道了这种新资源的存在。</li></ul><h4 id=第二步上报与监控-listandwatch>第二步：上报与监控 (ListAndWatch)</h4><ul><li>注册成功后，Kubelet会反过来调用Device Plugin的<code>ListAndWatch</code>方法。</li><li>Device Plugin会立即返回一个当前节点上所有可用设备ID的列表，例如<code>[GPU-UUID-1, GPU-UUID-2, ...]</code>。</li><li>Kubelet收到这个列表后，会更新该Node对象的<code>status.capacity</code>和<code>status.allocatable</code>字段，写入<code>nvidia.com/gpu: 8</code>这样的信息。</li><li>就是在这个瞬间，K8s的“中央大脑”（API Server和Scheduler）才真正“看见”了这8张GPU！</li><li><code>ListAndWatch</code>是一个流式RPC。Device Plugin会持续监控硬件状态。如果一张GPU卡因为故障离线了，它会立刻通过这个流告诉Kubelet，Kubelet会相应地更新节点的可分配资源，避免调度器将任务调度到一张已经坏掉的卡上。</li></ul><h4 id=第三步申请与分配-allocate>第三步：申请与分配 (Allocate)</h4><p>现在，用户可以提交一个Pod的YAML文件，在<code>resources.limits</code>中申请GPU资源了：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>resources</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>limits</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>nvidia.com/gpu</span><span class=p>:</span><span class=w> </span><span class=m>2</span><span class=w>
</span></span></span></code></pre></div><ul><li>Kube-scheduler在调度时，会遍历所有节点，查找<code>status.allocatable</code>中<code>nvidia.com/gpu</code>数量大于等于2的节点，然后将Pod调度过去。</li><li>Pod到达目标节点后，Kubelet在创建容器前，会再次调用Device Plugin的<code>Allocate</code>方法，并告诉它：“这个Pod需要2张GPU，请把分配结果告诉我。”</li><li>Device Plugin会从自己维护的空闲设备列表中选出2张卡（例如ID为<code>GPU-UUID-3</code>和<code>GPU-UUID-5</code>的卡），然后返回一个<code>ContainerAllocateResponse</code>。这个响应中包含了启动容器所必需的信息：<ul><li>设备（Devices）： 需要挂载到容器的设备文件路径，如<code>/dev/nvidia3</code>, <code>/dev/nvidia5</code>。</li><li>环境变量（Envs）： 需要注入到容器的环境变量，最重要的就是<code>NVIDIA_VISIBLE_DEVICES=3,5</code>。</li></ul></li><li>Kubelet拿到这些信息后，将其传递给底层的容器运行时（Docker + NVIDIA Container Toolkit）。Toolkit根据<code>NVIDIA_VISIBLE_DEVICES=3,5</code>，精确地只将第3和第5张GPU暴露给容器。</li><li>至此，一个申请了特定GPU的Pod就成功运行起来了。</li></ul><h3 id=423-实战部署>4.2.3 实战部署</h3><p>部署NVIDIA Device Plugin：</p><p>NVIDIA官方提供了Helm Chart和YAML文件，部署非常简单。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># 使用Helm安装</span>
</span></span><span class=line><span class=cl>helm repo add nvdp https://nvidia.github.io/k8s-device-plugin
</span></span><span class=line><span class=cl>helm repo update
</span></span><span class=line><span class=cl>helm install <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    --generate-name <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    --set-string nodeSelector.accelerator<span class=o>=</span>nvidia-h800 <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    nvdp/nvidia-device-plugin
</span></span></code></pre></div><p>注意这里的<code>nodeSelector</code>，它确保了NVIDIA的插件只运行在装有NVIDIA GPU的节点上。</p><p>部署Ascend Device Plugin：</p><p>华为同样提供了插件的YAML部署文件，通常包含在CANN的软件包或其开源社区中。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=c># 示例DaemonSet片段</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>apps/v1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>DaemonSet</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>ascend-device-plugin-daemonset</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>template</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>nodeSelector</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>accelerator</span><span class=p>:</span><span class=w> </span><span class=l>ascend-910b</span><span class=w> </span><span class=c># 确保只在昇腾节点运行</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>containers</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>ascend-device-plugin</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>ascend-device-plugin:latest</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=c># ... 挂载必要的socket和设备目录</span><span class=w>
</span></span></span></code></pre></div><p>小结： Device Plugin是Kubernetes设计哲学中“开放与扩展”的典范。它通过一套优雅的gRPC协议，将五花八门的硬件抽象成了统一的、可声明的资源，是实现异构算力统一调度的基石。作为AI Infra工程师，你不仅要会部署它，更要深刻理解<code>Register</code> -> <code>ListAndWatch</code> -> <code>Allocate</code>这一核心流程，这在你排查“Pod为什么调度不上去”、“GPU为什么分配错了”这类问题时，将给予你清晰的思路。</p><h2 id=43-镜像工程大模型各异构环境下的docker镜像瘦身与分层构建>4.3 镜像工程：大模型各异构环境下的Docker镜像瘦身与分层构建</h2><p>我们已经能在K8s上调度需要GPU/NPU的Pod了。现在，我们面临一个新的、同样棘手的问题：这些Pod运行的容器镜像从何而来？</p><p>对于大模型应用，其依赖环境极其复杂：特定版本的CUDA或CANN、特定版本的PyTorch、海量的Python依赖包，再加上模型本身的权重文件。一个不经优化的“朴素”镜像，体积轻松超过20GB。在一个需要频繁更新、快速部署的LLMOps流程中，这样的“巨无霸”镜像是灾难性的：</p><ul><li>拉取缓慢： 集群中一个新节点启动时，拉取20GB的镜像可能需要几十分钟，严重影响弹性伸缩的效率。</li><li>存储昂贵： 在镜像仓库中存储成百上千个版本，每个都20GB，是一笔巨大的开销。</li><li>安全性差： 镜像中包含了大量不必要的编译工具和库，增加了攻击面。</li></ul><p>镜像工程（Image Engineering），就是一门将臃肿的AI应用镜像，打造成轻量、高效、安全、可维护的“艺术”。</p><h3 id=431-核心原则一选择正确的地基基础镜像>4.3.1 核心原则一：选择正确的“地基”——基础镜像</h3><p>选择一个好的基础镜像，是镜像瘦身的第一步，也是最重要的一步。</p><ul><li><p>避免使用<code>-devel</code>镜像生产：</p><ul><li>NVIDIA的<code>nvidia/cuda</code>镜像标签中，通常有<code>-base</code>、<code>-runtime</code>和<code>-devel</code>三种。</li><li><code>-devel</code>包含了完整的CUDA编译器（nvcc）、头文件和调试工具，体积最大，仅用于编译阶段。</li><li><code>-runtime</code>只包含运行CUDA程序所必需的运行时库，体积小得多。</li><li><code>-base</code>则更精简，甚至不包含cuDNN等常用库。</li><li>原则： 生产环境的最终镜像，必须基于<code>-runtime</code>或<code>-base</code>镜像构建。</li></ul></li><li><p>昇腾生态的对应选择：
华为的基础镜像（如<code>mindspore-ascend</code>）同样有不同版本，需要选择只包含CANN运行环境，而不包含完整开发工具链的版本作为生产基础。</p></li></ul><h3 id=432-核心原则二两阶段施工多阶段构建multi-stage-builds>4.3.2 核心原则二：“两阶段施工”——多阶段构建（Multi-Stage Builds）</h3><p>这是镜像瘦身的“核武器”。其思想是：在一个Dockerfile中使用多个<code>FROM</code>指令，将构建过程分为多个阶段。只有最后一个阶段的产物，会成为最终的镜像。</p><p>场景： 我们有一个需要编译的自定义PyTorch C++扩展。</p><p>糟糕的Dockerfile（单阶段）：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-dockerfile data-lang=dockerfile><span class=line><span class=cl><span class=c># 镜像体积巨大 (e.g., 15GB)</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>FROM</span><span class=w> </span><span class=s>nvidia/cuda:12.1.0-devel-ubuntu22.04</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># 安装编译工具和Python</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>RUN</span> apt-get update <span class=o>&amp;&amp;</span> apt-get install -y g++ python3-pip<span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># 安装Python依赖</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>WORKDIR</span><span class=w> </span><span class=s>/app</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>COPY</span> requirements.txt .<span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>RUN</span> pip install -r requirements.txt<span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># 编译和安装自定义扩展</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>COPY</span> . .<span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>RUN</span> python3 setup.py install<span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>CMD</span> <span class=p>[</span><span class=s2>&#34;python3&#34;</span><span class=p>,</span> <span class=s2>&#34;main.py&#34;</span><span class=p>]</span><span class=err>
</span></span></span></code></pre></div><p>这个镜像包含了g++编译器、完整的CUDA SDK、pip的缓存等所有“建筑垃圾”。</p><p>优秀的Dockerfile（多阶段）：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-dockerfile data-lang=dockerfile><span class=line><span class=cl><span class=c># --- Stage 1: Builder ---</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># 使用开发镜像进行编译</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>FROM</span><span class=w> </span><span class=s>nvidia/cuda:12.1.0-devel-ubuntu22.04</span><span class=w> </span><span class=k>AS</span><span class=w> </span><span class=s>builder</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>RUN</span> apt-get update <span class=o>&amp;&amp;</span> apt-get install -y g++ python3-pip python3.10-venv<span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>WORKDIR</span><span class=w> </span><span class=s>/app</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># 在虚拟环境中安装依赖，方便打包</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>RUN</span> python3 -m venv /app/venv<span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>ENV</span> <span class=nv>PATH</span><span class=o>=</span><span class=s2>&#34;/app/venv/bin:</span><span class=nv>$PATH</span><span class=s2>&#34;</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>COPY</span> requirements.txt .<span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>RUN</span> pip install -r requirements.txt<span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>COPY</span> . .<span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>RUN</span> python3 setup.py install<span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># --- Stage 2: Final ---</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># 使用轻量的运行时镜像</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>FROM</span><span class=w> </span><span class=s>nvidia/cuda:12.1.0-runtime-ubuntu22.04</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># 只拷贝必要的运行时文件和编译好的产物</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>COPY</span> --from<span class=o>=</span>builder /app/venv /app/venv<span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>COPY</span> --from<span class=o>=</span>builder /app/main.py /app/main.py<span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>ENV</span> <span class=nv>PATH</span><span class=o>=</span><span class=s2>&#34;/app/venv/bin:</span><span class=nv>$PATH</span><span class=s2>&#34;</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>WORKDIR</span><span class=w> </span><span class=s>/app</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>CMD</span> <span class=p>[</span><span class=s2>&#34;python3&#34;</span><span class=p>,</span> <span class=s2>&#34;main.py&#34;</span><span class=p>]</span><span class=err>
</span></span></span></code></pre></div><p>这个最终生成的镜像，体积可能只有5GB。它不包含g++、CUDA SDK等任何编译时依赖，只包含了干净的Python虚拟环境和最终的运行脚本。</p><h3 id=433-核心原则三精打细算优化dockerfile指令>4.3.3 核心原则三：“精打细算”——优化Dockerfile指令</h3><p>合并RUN指令： Dockerfile中的每一个<code>RUN</code>, <code>COPY</code>, <code>ADD</code>指令都会创建一个新的镜像层。过多的层会增加镜像体积和构建时间。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-dockerfile data-lang=dockerfile><span class=line><span class=cl><span class=c># 不好：创建了多个层</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>RUN</span> apt-get update<span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>RUN</span> apt-get install -y curl<span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>RUN</span> rm -rf /var/lib/apt/lists/*<span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># 好：合并为一层，并及时清理</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>RUN</span> apt-get update <span class=o>&amp;&amp;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    apt-get install -y curl <span class=o>&amp;&amp;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    rm -rf /var/lib/apt/lists/*<span class=err>
</span></span></span></code></pre></div><p>利用构建缓存： Docker会缓存每一层的构建结果。将不常变化的指令（如安装系统包）放在前面，将经常变化的指令（如<code>COPY</code>源代码）放在后面，可以最大化地利用缓存，加快后续构建速度。</p><p>使用<code>.dockerignore</code>： 在项目根目录下创建<code>.dockerignore</code>文件，忽略掉不需要拷贝到镜像中的文件（如<code>.git</code>目录、测试数据、本地配置文件），这可以减小构建上下文（Build Context）的大小，并避免敏感信息泄露。</p><h3 id=434-挑战异构环境下的镜像策略>4.3.4 挑战：异构环境下的镜像策略</h3><p>当你的集群中同时有NVIDIA和昇腾时，如何管理应用的镜像？</p><h4 id=策略一推荐专镜专用-separate-images>策略一（推荐）：专镜专用 (Separate Images)</h4><p>为同一个应用，构建两个不同的镜像，并打上清晰的tag：</p><ul><li><code>myapp:1.2.0-cuda12.1</code></li><li><code>myapp:1.2.0-cann7.0</code></li></ul><p>在K8s的部署文件（如Deployment或Job）中，通过<code>nodeSelector</code>来决定使用哪个镜像：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=c># 部署到NVIDIA节点的Pod模板</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>nodeSelector</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>accelerator</span><span class=p>:</span><span class=w> </span><span class=l>nvidia-h800</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>containers</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>main</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>my-registry/myapp:1.2.0-cuda12.1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c># 部署到Ascend节点的Pod模板</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>nodeSelector</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>accelerator</span><span class=p>:</span><span class=w> </span><span class=l>ascend-910b</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>containers</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>main</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>my-registry/myapp:1.2.0-cann7.0</span><span class=w>
</span></span></span></code></pre></div><ul><li>优点： 镜像清晰、最小化、无冗余。这是最干净、最符合云原生思想的做法。</li><li>缺点： 需要维护两条CI/CD流水线。</li></ul><h4 id=策略二高级慎用胖镜像与运行时判断-fat-image>策略二（高级，慎用）：胖镜像与运行时判断 (Fat Image)</h4><p>构建一个同时包含CUDA和CANN依赖的“胖镜像”。</p><p>修改容器的入口脚本（<code>ENTRYPOINT</code>），让它在启动时检测当前环境，然后选择正确的执行路径。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=cp>#!/bin/bash
</span></span></span><span class=line><span class=cl><span class=cp></span><span class=k>if</span> <span class=o>[</span> -d <span class=s2>&#34;/usr/local/cuda&#34;</span> <span class=o>]</span><span class=p>;</span> <span class=k>then</span>
</span></span><span class=line><span class=cl>    <span class=nb>echo</span> <span class=s2>&#34;CUDA environment detected. Starting application for NVIDIA.&#34;</span>
</span></span><span class=line><span class=cl>    <span class=c1># 执行NVIDIA版本的启动命令</span>
</span></span><span class=line><span class=cl>    <span class=nb>exec</span> python3 main_cuda.py <span class=s2>&#34;</span><span class=nv>$@</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl><span class=k>elif</span> <span class=o>[</span> -d <span class=s2>&#34;/usr/local/ascend&#34;</span> <span class=o>]</span><span class=p>;</span> <span class=k>then</span>
</span></span><span class=line><span class=cl>    <span class=nb>echo</span> <span class=s2>&#34;Ascend/CANN environment detected. Starting application for Huawei.&#34;</span>
</span></span><span class=line><span class=cl>    <span class=c1># 执行Ascend版本的启动命令</span>
</span></span><span class=line><span class=cl>    <span class=nb>exec</span> python3 main_ascend.py <span class=s2>&#34;</span><span class=nv>$@</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl><span class=k>else</span>
</span></span><span class=line><span class=cl>    <span class=nb>echo</span> <span class=s2>&#34;Error: No supported AI accelerator environment found.&#34;</span>
</span></span><span class=line><span class=cl>    <span class=nb>exit</span> <span class=m>1</span>
</span></span><span class=line><span class=cl><span class=k>fi</span>
</span></span></code></pre></div><ul><li>优点： 只需要管理一个镜像tag。</li><li>缺点： 镜像极度臃肿，违反了单一职责原则，管理和调试更复杂。通常不推荐，除非有非常特殊的统一交付要求。</li></ul><p>总结：
本章，我们从零开始，成功地将AI应用封装进了标准化的、可在K8s中被统一调度的容器中。我们掌握了连接容器与硬件的“运行时”技术，理解了让K8s感知硬件的“设备插件”原理，并学习了打造轻量高效镜像的“镜像工程”艺术。至此，我们已经拥有了云原生AI平台的坚实“底座”。接下来，我们将在这个底座之上，构建更智能、更高效的调度与资源管理系统。</p></div><div class=post_footer><div class=meta><div class=info><span class="field tags"><i class=ri-stack-line></i>
<a href=https://zhurongshuo.com/tags/%E4%B9%A6%E7%A8%BF/>书稿</a></span></div></div></div></div><div class=doc_comments></div></div></div></div><a id=back_to_top href=# class=back_to_top><i class=ri-arrow-up-s-line></i></a><footer class=footer><div class=powered_by><a href=https://varkai.com>Designed by VarKai, </a><a href=http://www.gohugo.io/>Proudly published with Hugo,</a></div><div class=footer_slogan><span>法不净空，觉无性也。</span></div><div class=powered_by style=margin-top:10px;font-size:14px><a href=https://zhurongshuo.com/>Copyright © 2010-2025 祝融说 zhurongshuo.com All Rights Reserved.</a></div></footer><script defer src=https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><link href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.css rel=stylesheet integrity="sha256-7qiTu3a8qjjWtcX9w+f2ulVUZSUdCZFEK62eRlmLmCE=" crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin=anonymous></script><script defer src=https://zhurongshuo.com/js/zozo.js></script><script type=text/javascript async src="https://cdn.jsdelivr.net/npm/mathjax@2.7.8/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["[[","]]"]],processEscapes:!0,processEnvironments:!0,skipTags:["script","noscript","style","textarea","pre"],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}}),MathJax.Hub.Queue(function(){var e,t=MathJax.Hub.getAllJax();for(e=0;e<t.length;e+=1)t[e].SourceElement().parentNode.className+=" has-jax"})</script><style>code.has-jax{font:inherit;font-size:100%;background:inherit;border:inherit;color:#515151}</style><script async src="https://www.googletagmanager.com/gtag/js?id=G-KKJ5ZEG1NB"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KKJ5ZEG1NB")</script></body></html>