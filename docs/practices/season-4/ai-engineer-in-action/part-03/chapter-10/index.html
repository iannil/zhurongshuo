<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><meta name=google-site-verification content="8_xpI-TS3tNV8UPug-Q6Ef3BhKTcy0WOG7dEdAcm2zk"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="祝融"><link rel=dns-prefetch href=//cdn.jsdelivr.net><link rel=preconnect href=https://cdn.jsdelivr.net crossorigin><link rel=dns-prefetch href=//www.googletagmanager.com><link rel=preconnect href=https://www.googletagmanager.com crossorigin><link rel=canonical href=https://zhurongshuo.com/practices/season-4/ai-engineer-in-action/part-03/chapter-10/><title>祝融说。 第十章：极致性能：LLM推理服务优化</title><meta property="og:title" content="第十章：极致性能：LLM推理服务优化"><meta property="og:url" content="https://zhurongshuo.com/practices/season-4/ai-engineer-in-action/part-03/chapter-10/"><meta property="og:site_name" content="祝融说。"><meta property="og:locale" content="zh_CN"><meta property="og:type" content="article"><meta property="article:published_time" content="2025-12-09T00:00:00+08:00"><meta property="article:modified_time" content="2025-12-09T00:00:00+08:00"><meta property="article:tag" content="书稿"><meta name=description content="在本书的前两个篇章中，我们已经走过了一段漫长而充实的旅程。我们从底层构建了坚实的工程基础，深入了深度学习和Transformer的腹地，并掌握了微调、RAG和Agent等高级应用开发范式。至此，我们已经能够开发出功能强大的、定制化的LLM应用原型。
"><meta property="og:description" content="在本书的前两个篇章中，我们已经走过了一段漫长而充实的旅程。我们从底层构建了坚实的工程基础，深入了深度学习和Transformer的腹地，并掌握了微调、RAG和Agent等高级应用开发范式。至此，我们已经能够开发出功能强大的、定制化的LLM应用原型。
"><meta property="og:image" content="https://zhurongshuo.com/images/favicon.ico"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="第十章：极致性能：LLM推理服务优化"><meta name=twitter:description content="在本书的前两个篇章中，我们已经走过了一段漫长而充实的旅程。我们从底层构建了坚实的工程基础，深入了深度学习和Transformer的腹地，并掌握了微调、RAG和Agent等高级应用开发范式。至此，我们已经能够开发出功能强大的、定制化的LLM应用原型。
"><meta name=twitter:image content="https://zhurongshuo.com/images/favicon.ico"><meta name=keywords content="AI工程师实战：从Python基础到LLM应用与性能优化,第十章：极致性能：LLM推理服务优化"><link rel="shortcut icon" href=https://zhurongshuo.com/images/favicon.ico><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/normalize.css><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/animate-custom.css><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/zozo.css><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/remixicon-custom.css><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/highlight.css><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","headline":"第十章：极致性能：LLM推理服务优化","description":"在本书的前两个篇章中，我们已经走过了一段漫长而充实的旅程。我们从底层构建了坚实的工程基础，深入了深度学习和Transformer的腹地，并掌握了微调、RAG和Agent等高级应用开发范式。至此，我们已经能够开发出功能强大的、定制化的LLM应用原型。\n","datePublished":"2025-12-09T00:00:00\u002b08:00","dateModified":"2025-12-09T00:00:00\u002b08:00","author":{"@type":"Person","name":"祝融"},"publisher":{"@type":"Organization","name":"祝融说。","logo":{"@type":"ImageObject","url":"https:\/\/zhurongshuo.com\/images\/favicon.ico"}},"image":"https:\/\/zhurongshuo.com\/images\/favicon.ico","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/zhurongshuo.com\/practices\/season-4\/ai-engineer-in-action\/part-03\/chapter-10\/"},"keywords":"书稿"}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"首页","item":"https:\/\/zhurongshuo.com\/"},{"@type":"ListItem","position":2,"name":"practices","item":"https:\/\/zhurongshuo.com\/practices/"},{"@type":"ListItem","position":3,"name":"第十章：极致性能：LLM推理服务优化","item":"https:\/\/zhurongshuo.com\/practices\/season-4\/ai-engineer-in-action\/part-03\/chapter-10\/"}]}</script></head><body><div class="main animate__animated animate__fadeInDown"><div class="nav_container animated fadeInDown"><div class=site_nav id=site_nav><ul><li><a href=https://zhurongshuo.com/>首页</a></li><li><a href=https://zhurongshuo.com/start/>开始</a></li><li><a href=https://zhurongshuo.com/advanced/>进阶rc</a></li><li><a href=https://zhurongshuo.com/posts/>归档</a></li><li><a href=https://zhurongshuo.com/tags/>标签</a></li><li><a href=https://zhurongshuo.com/about/>关于</a></li></ul></div><div class=menu_icon><a id=menu_icon><i class=ri-menu-line></i></a></div></div><div class="header animated fadeInDown"><div class=site_title_container><div class=site_title><h1><a href=https://zhurongshuo.com/><span class=web-font>祝融说。</span></a></h1></div><div class=description><p class=sub_title>法不净空，觉无性也。</p><div class=my_socials><a href=https://zhurongshuo.com/books/ title=book-open><i class=ri-book-open-line></i></a>
<a href=https://zhurongshuo.com/practices/ title=trophy><i class=ri-trophy-line></i></a>
<a href=https://zhurongshuo.com/gallery/ title=gallery><i class=ri-gallery-line></i></a>
<a href=https://zhurongshuo.com/about/ title=game><i class=ri-game-line></i></a>
<a href type=application/rss+xml title=rss target=_blank><i class=ri-rss-fill></i></a></div></div></div></div><div class=content><div class=post_page><div class="post animate__animated animate__fadeInDown"><div class="post_title post_detail_title"><h2><a href=https://zhurongshuo.com/practices/season-4/ai-engineer-in-action/part-03/chapter-10/>第十章：极致性能：LLM推理服务优化</a></h2><span class=date>2025.12.09</span></div><div class="post_content markdown"><p>在本书的前两个篇章中，我们已经走过了一段漫长而充实的旅程。我们从底层构建了坚实的工程基础，深入了深度学习和Transformer的腹地，并掌握了微调、RAG和Agent等高级应用开发范式。至此，我们已经能够开发出功能强大的、定制化的LLM应用原型。</p><p>然而，在真实的生产环境中，一个在单用户、低负载下运行良好的原型，与一个能够为成千上万用户提供稳定、快速、经济服务的商业产品之间，存在着一道巨大的鸿沟。这道鸿沟，就是性能。</p><p>想象一下，你构建的智能客服机器人，在面对100个并发用户时，每个回答都需要等待10秒钟；或者，为了部署一个70B的模型，你需要租用一台每月花费数万美元的顶级GPU服务器。这样的产品，无论其功能多么炫酷，在商业上都是不可持续的。</p><p>LLM推理（Inference）——即模型生成文本的过程——是一个计算密集且内存密集型的操作。它不像训练，可以离线、长时间地进行。推理是面向用户的，对延迟（Latency）和成本（Cost）极其敏感。因此，对LLM推理服务进行极致的性能优化，是每一位高级AI工程师都必须面对和征服的挑战。这正是从“优秀”到“卓越”的必经之路。</p><p>本章，我们将聚焦于这场性能优化的“攻坚战”。我们将系统性地学习一系列将LLM推理性能推向极限的前沿技术：</p><ul><li>关键性能指标：我们将首先建立正确的度量衡，明确衡量推理服务性能的三大核心指标——延迟、吞吐量和显存占用，并理解它们之间的权衡关系。</li><li>模型量化技术：我们将深入探讨如何通过量化（Quantization），如GPTQ、AWQ，将模型的权重从高精度的浮点数压缩为低精度的整数，从而在几乎不损失模型性能的前提下，大幅降低显存占用和提升计算速度。我们还会了解专为CPU推理设计的GGUF格式。</li><li>高性能推理框架：我们将学习为什么原生的Hugging Face <code>transformers</code>库不适合生产环境推理，并重点掌握两个业界领先的高性能推理框架——vLLM和TensorRT-LLM。你将理解它们是如何通过PagedAttention、持续批处理等创新技术，将GPU的利用率压榨到极致的。</li><li>Batching策略：我们将探讨不同的批处理（Batching）策略，从简单的静态批处理，到能够动态处理不同长度序列的持续批处理（Continuous Batching），理解其对吞吐量提升的巨大作用。</li></ul><p>最后，我们将通过一个极具实践价值的实战项目——对我们在第八章微调后的模型进行4-bit量化，并使用vLLM框架进行部署，然后通过压测工具对比优化前后的性能差异——将本章所有优化技术落地。</p><p>掌握了本章内容，你将拥有“降本增效”的硬核能力。你将能够将庞大而昂贵的LLM，改造为轻巧、高效、经济的生产力工具，为你的AI产品构建起坚实的商业护城河。现在，让我们开始这场追求极致性能的探索之旅。</p><h2 id=101-推理性能的关键指标延迟吞吐量与显存占用>10.1 推理性能的关键指标：延迟、吞吐量与显存占用</h2><p>在进行任何优化之前，我们必须首先明确优化的目标。LLM推理服务的性能，通常由以下三个相互关联的核心指标来衡量。</p><h3 id=1011-延迟latency>10.1.1 延迟（Latency）</h3><p>定义：延迟指从用户发送请求到接收到完整响应所花费的时间。对于LLM推理，延迟通常可以细分为三个部分：</p><ol><li>Time to First Token (TTFT)：首字延迟。指从接收请求到生成第一个token所花费的时间。这个时间主要消耗在预填充（Prefill）阶段，即模型处理输入Prompt的过程。对于交互式应用（如聊天机器人），TTFT至关重要，它直接影响用户感受到的“响应速度”。一个低的TTFT会让用户觉得系统“活”了起来。</li><li>Time Per Output Token (TPOT)：每字延迟。指生成每个后续token的平均时间。这个时间主要消耗在解码（Decoding）阶段。TPOT决定了文本生成的速度，即文字“蹦出来”的快慢。</li><li>Total Latency (End-to-End Latency)：总延迟。即 <code>TTFT + (TPOT * num_output_tokens)</code>。</li></ol><p>影响因素：模型大小、硬件性能（GPU型号）、Prompt长度、生成长度、量化程度、Batch Size等。</p><p>优化目标：对于实时交互应用，首要目标是降低TTFT和TPOT。</p><h3 id=1012-吞吐量throughput>10.1.2 吞吐量（Throughput）</h3><p>定义：吞吐量指系统在单位时间内能够处理的请求数量或生成的token总数。</p><ol><li>Requests per Second (RPS)：每秒处理的请求数。这是衡量服务承载能力最直观的指标。</li><li>Tokens per Second (TPS)：每秒生成的token总数。这个指标更能反映系统实际的计算负载。<code>TPS = RPS * average_output_tokens_per_request</code>。</li></ol><p>延迟与吞吐量的权衡（Trade-off）：</p><ul><li>延迟和吞吐量通常是一对矛盾体。</li><li>为了降低单个请求的延迟，我们可能会使用小的Batch Size（例如，Batch Size = 1）。</li><li>为了提高系统的总吞吐量，我们希望将多个请求打包成一个大的Batch，一次性送入GPU进行计算，以提高GPU的利用率。但这样做，会导致批次中较早到达的请求，必须等待较晚到达的请求，从而增加了它的延迟。</li></ul><p>优化目标：对于离线批处理任务（如批量生成文章摘要），首要目标是最大化吞吐量。对于在线服务，则需要在满足延迟SLA（服务等级协议）的前提下，尽可能地提高吞吐量。</p><h3 id=1013-显存占用memory-footprint>10.1.3 显存占用（Memory Footprint）</h3><p>定义：指LLM推理服务在运行时所占用的GPU显存大小。这是决定部署成本的最关键因素。显存占用主要来自三个方面：</p><ol><li><p>模型权重（Model Weights）：这是显存占用的最大头。一个7B的FP16模型，其权重就需要 <code>7B * 2 bytes/param ≈ 14 GB</code> 的显存。</p></li><li><p>KV缓存（KV Cache）：这是LLM推理特有的显存消耗。在自回归生成过程中，为了避免重复计算，系统需要缓存下已经生成序列中每个token的Key和Value向量。KV缓存的大小与Batch Size和序列长度成正比，是动态变化的，也是导致显存OOM（Out of Memory）的主要原因。</p><p>KV缓存大小 ≈ <code>Batch Size * Sequence Length * Num Layers * Num Heads * Head Dim * 2 (K&amp;V) * bytes_per_element</code></p></li><li><p>激活值（Activations）：在前向传播过程中产生的中间计算结果。其大小与Batch Size和模型复杂度相关。</p></li></ol><p>优化目标：降低显存占用，可以直接：</p><ul><li>部署更大的模型：在同一张显卡上，原本只能部署7B模型，优化后可能可以部署13B模型。</li><li>支持更大的Batch Size：在显存不变的情况下，减少了模型权重和KV缓存的占用，就可以容纳更大的批次，从而提高吞吐量。</li><li>降低硬件成本：可以使用更便宜的、显存较小的GPU来部署服务。</li></ul><p>三大指标的关系：</p><p>这三个指标构成了一个“不可能三角”。优化通常是在这三者之间进行权衡。例如，模型量化可以同时降低显存占用和延迟，并可能因为支持更大的Batch Size而间接提升吞吐量，是性价比极高的优化手段。而Batching策略则主要是在延迟和吞吐量之间做权衡。</p><h2 id=102-模型量化技术gptqawq与gguf>10.2 模型量化技术：GPTQ、AWQ与GGUF</h2><p>量化（Quantization）是指将模型中高精度的浮点数（如32位FP32或16位FP16/BF16）表示为低精度的整数（如8-bit INT8或4-bit INT4）的过程。</p><h3 id=1021-为什么量化能起作用>10.2.1 为什么量化能起作用？</h3><p>降低显存占用：参数占用的比特数减少，显存占用自然成倍下降。一个7B模型，FP16需要14GB，INT8需要7GB，INT4则只需要3.5GB。</p><p>加速计算：现代GPU对低精度整数运算有专门的硬件加速支持（如Tensor Cores），速度远快于浮点运算。</p><p>减少内存带宽：模型权重更小，从显存加载到计算单元所需的时间也更短。</p><p>量化的挑战：量化的过程是有损压缩，会引入精度误差。关键挑战在于，如何在尽可能降低比特数的同时，最大限度地保持模型的原始性能（通常用困惑度Perplexity或下游任务的准确率来衡量）。</p><h3 id=1022-gptq后训练量化ptq的代表作>10.2.2 GPTQ：后训练量化（PTQ）的代表作</h3><p>GPTQ (Generative Pre-trained Transformer Quantization) 是一种流行的后训练量化（Post-Training Quantization, PTQ）方法。PTQ的特点是，它只需要一个预训练好的模型和一小部分校准数据（Calibration Data），无需重新训练。</p><p>核心思想：</p><p>GPTQ的目标是，找到一个量化后的权重矩阵 <code>W_q</code>，使得 <code>W_q * X</code> 与原始的 <code>W * X</code> 的均方误差最小。它不是逐个地量化权重，而是以一种逐列（column-by-column）的方式，并考虑到了权重之间的相互影响。</p><p>工作流程（简化版）：</p><ol><li>对于一个权重矩阵，从第一列开始。</li><li>量化当前列的权重。</li><li>计算量化误差。</li><li>将这个量化误差，补偿性地更新到矩阵中所有尚未被量化的其他列上。</li><li>移动到下一列，重复此过程。</li></ol><p>通过这种方式，前面列的量化误差，会被后面列的更新所“吸收”和“修正”，从而使得整个矩阵的累积误差最小。</p><p>优点：量化速度快，效果好，尤其在4-bit量化上表现出色。</p><p>缺点：量化过程需要一定的计算资源和校准数据。</p><h3 id=1023-awq激活感知量化>10.2.3 AWQ：激活感知量化</h3><p>AWQ (Activation-aware Weight Quantization) 是另一种先进的PTQ方法，它在GPTQ的基础上更进了一步。</p><p>核心思想：</p><p>AWQ的作者观察到一个现象：在LLM中，不同的权重对于模型性能的重要性是不同的。那些与数值较大的“显著激活值”（salient activation）相乘的权重，对模型的性能影响更大。</p><p>因此，AWQ提出，我们不应该平等地对待所有权重，而应该在量化时，保护那些重要的权重，牺牲那些不重要的权重。</p><p>工作流程（简化版）：</p><ol><li>通过一小部分校准数据，分析模型在前向传播时的激活值分布，识别出那些“显著通道”（即激活值数值较大的通道）。</li><li>在量化权重之前，对权重矩阵进行一次逐通道的缩放（per-channel scaling）。具体来说，它会“放大”那些不重要的权重（对应不显著的激活通道），同时“缩小”那些重要的权重。</li><li>然后，对这个缩放后的权重矩阵进行标准的量化。</li></ol><p>这样做的效果是，重要的权重因为被“缩小”了，其量化误差也相应地变小了，从而得到了保护。而不重要的权重虽然被“放大”了，量化误差也变大了，但由于它们本身就不重要，所以对最终性能的影响很小。</p><p>优点：在极低比特（如3-bit, 4-bit）量化下，通常能比GPTQ取得更好的模型性能。</p><p>缺点：原理比GPTQ稍复杂。</p><h3 id=1024-gguf为cpu推理而生>10.2.4 GGUF：为CPU推理而生</h3><p>GGUF (Georgi Gerganov Universal Format) 是一种专为 <code>llama.cpp</code> 项目设计的文件格式。<code>llama.cpp</code> 是一个用纯C/C++实现的LLM推理框架，其最大特点是可以在CPU上高效地运行LLM，极大地降低了硬件门槛。</p><p>GGUF的特点：</p><p>单一文件格式：将模型架构、权重、分词器等所有信息都打包在一个文件中，非常便于分发和使用。</p><p>CPU优化：支持多种复杂的量化策略（从2-bit到8-bit），并针对不同CPU架构（如AVX2）进行了深度优化。</p><p>内存映射（mmap）：可以不将整个模型加载到RAM中，而是在需要时直接从磁盘映射到内存，从而可以用极小的RAM运行非常大的模型（尽管速度会变慢）。</p><p>适用场景：</p><p>在没有GPU的个人电脑、MacBook上运行LLM。</p><p>在移动设备或边缘设备上部署LLM。</p><p>作为本地开发和快速实验的环境。</p><p>总结：GPTQ和AWQ是面向GPU的高性能PTQ技术，而GGUF则是CPU推理生态的核心。</p><h2 id=103-高性能推理框架vllm与tensorrt-llm的应用>10.3 高性能推理框架：vLLM与TensorRT-LLM的应用</h2><p>虽然Hugging Face的<code>transformers</code>库非常适合模型训练和实验，但其默认的推理实现是为易用性而非性能设计的。在生产环境中，我们需要专门的推理框架来压榨硬件的全部潜力。</p><h3 id=1031-原生transformers推理的瓶颈>10.3.1 原生<code>transformers</code>推理的瓶颈</h3><ol><li>朴素的KV缓存管理：<code>transformers</code>为每个请求预分配一个固定大小的KV缓存，其大小等于模型的最大上下文长度。这导致了巨大的内存浪费。例如，即使一个请求只有100个token，系统也会为它预留4096个token的KV缓存空间。</li><li>静态批处理（Static Batching）：它将多个请求打包成一个批次，但必须等待批次中的所有请求都生成完毕后，才能返回结果并处理下一个批次。这导致GPU在大部分时间里处于空闲状态，因为批次中的短序列早就生成完了，却在等待最长的那个序列。</li></ol><h3 id=1032-vllm以pagedattention革新kv缓存管理>10.3.2 vLLM：以PagedAttention革新KV缓存管理</h3><p>vLLM 是由伯克利大学的研究者推出的一个开源LLM推理和服务框架，它通过引入PagedAttention技术，极大地提升了推理的吞吐量。</p><p>PagedAttention的核心思想：
它借鉴了操作系统中虚拟内存（Virtual Memory）和分页（Paging）的思想来管理KV缓存。</p><ol><li>非连续的物理内存：vLLM不再为KV缓存预留连续的大块显存，而是将其分割成许多固定大小的、非连续的物理块（Physical Block）。</li><li>逻辑块到物理块的映射：vLLM为每个序列维护一个“页表”，用于记录其逻辑块（Logical Block）到物理块的映射关系。</li><li>按需分配：在解码的每一步，只有当需要新的空间时，vLLM才会分配一个新的物理块，并更新页表。</li></ol><p>PagedAttention带来的巨大优势：</p><p>近乎零的内存浪费：显存占用与序列的实际长度成正比，内部碎片率极低（低于4%）。</p><p>高效的内存共享：对于使用并行采样（Parallel Sampling）（一次生成多个输出）或束搜索（Beam Search）的请求，它们共同的Prompt部分的KV缓存可以在物理层面被高效地共享，而无需复制。</p><p>更高的吞吐量：由于内存效率的极大提升，vLLM可以在同样的硬件上支持更大的Batch Size，从而将吞吐量提升2-4倍。</p><h3 id=1033-tensorrt-llmnvidia的官方终极武器>10.3.3 TensorRT-LLM：NVIDIA的官方终极武器</h3><p>TensorRT-LLM 是NVIDIA官方推出的、基于TensorRT的LLM推理优化库。它代表了在NVIDIA GPU上进行LLM推理的性能极限。</p><p>核心特点：</p><ol><li>深度内核融合（Kernel Fusion）：TensorRT-LLM会将模型中的多个操作（如矩阵乘法、加法、激活函数）融合成一个单一的、高度优化的CUDA内核。这减少了从显存中读写数据的次数，也减少了内核启动的开销，从而大幅提升计算效率。</li><li>In-flight Batching：这是TensorRT-LLM对持续批处理（Continuous Batching）的实现，我们将在下一节详述。</li><li>PagedAttention的实现：同样集成了PagedAttention来优化KV缓存。</li><li>硬件特定优化：为NVIDIA的不同GPU架构（如Ampere, Hopper）和特性（如FP8精度）提供了极致的优化。</li></ol><p>vLLM vs TensorRT-LLM：</p><ul><li>易用性：vLLM 更胜一筹。它提供了非常简洁的Python API，与Hugging Face生态无缝集成，上手非常快。</li><li>性能：TensorRT-LLM 通常能达到更高的极致性能，特别是在NVIDIA的最新硬件上。但它的使用流程更复杂，需要一个“编译”步骤，将模型转换为TensorRT引擎。</li><li>灵活性与社区：vLLM 是纯Python的，社区活跃，对新模型和新技术的支持通常更快。TensorRT-LLM 作为NVIDIA的官方产品，更新和支持有保障，但社区生态相对较小。</li></ul><p>选型建议：对于绝大多数用户，vLLM 提供了性能和易用性的最佳平衡点，是快速部署高性能服务的首选。对于追求极致性能、且不介意投入更多工程时间的团队，可以考虑TensorRT-LLM。</p><h2 id=104-batching策略从静态到动态批处理>10.4 Batching策略：从静态到动态批处理</h2><h3 id=1041-静态批处理static-batching>10.4.1 静态批处理（Static Batching）</h3><p>这是最传统的批处理方式。</p><ol><li>收集一批请求（例如，8个请求）。</li><li>将它们打包成一个批次，并进行padding，使所有序列长度一致。</li><li>将整个批次送入GPU进行计算。</li><li>等待批次中所有序列都生成完毕。</li><li>将结果返回给各自的请求。</li></ol><p>缺点：GPU利用率极低。如下图所示，当序列1、2、3都已完成时，GPU却在空闲地等待序列4完成，造成了巨大的浪费。</p><p>静态批处理的GPU空闲问题</p><h3 id=1042-持续批处理continuous-batching--in-flight-batching>10.4.2 持续批处理（Continuous Batching / In-flight Batching）</h3><p>这是vLLM和TensorRT-LLM等现代推理框架采用的核心调度策略。</p><p>工作流程：</p><ol><li>推理服务器维护一个请求队列。</li><li>调度器在每个解码步骤都会检查队列。</li><li>如果一个批次中的某个请求已经生成完毕，系统会立即将其从批次中移除，并释放其占用的资源。</li><li>同时，调度器会尝试从队列中动态地加入新的请求到当前批次中，只要GPU资源允许。</li></ol><p>优点：</p><ul><li>极高的GPU利用率：GPU几乎总是在满负荷工作，因为它总是在处理一个“满”的批次。</li><li>大幅提升吞吐量：相比静态批处理，吞吐量可以提升一个数量级。</li><li>更公平的调度：短请求不会被长请求长时间阻塞。</li></ul><p>持续批处理与PagedAttention的结合，是现代LLM推理框架能够实现超高吞吐量的两大“秘密武器”。</p><h2 id=105-实战项目对微调后的模型进行量化并使用vllm部署进行性能压测对比>10.5 实战项目：对微调后的模型进行量化，并使用vLLM部署，进行性能压测对比</h2><p>项目目标：我们将以第八章微调好的LoRA模型为例，完成以下步骤：</p><ol><li>将LoRA权重与基础模型合并。</li><li>使用<code>auto-gptq</code>库对合并后的模型进行4-bit GPTQ量化。</li><li>分别使用原生的<code>transformers</code>和<code>vLLM</code>来部署量化后的模型。</li><li>使用一个简单的压测脚本，对比两者在延迟和吞吐量上的巨大差异。</li></ol><p>第一步：合并LoRA权重</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># merge_lora.py</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>peft</span> <span class=kn>import</span> <span class=n>PeftModel</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>AutoModelForCausalLM</span><span class=p>,</span> <span class=n>AutoTokenizer</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>base_model_name</span> <span class=o>=</span> <span class=s2>&#34;meta-llama/Llama-3-8B&#34;</span>
</span></span><span class=line><span class=cl><span class=n>lora_checkpoint_path</span> <span class=o>=</span> <span class=s2>&#34;../chapter8/results/final_checkpoint&#34;</span>
</span></span><span class=line><span class=cl><span class=n>merged_model_path</span> <span class=o>=</span> <span class=s2>&#34;./merged_llama3_8b_ai_qa&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 加载基础模型</span>
</span></span><span class=line><span class=cl><span class=n>base_model</span> <span class=o>=</span> <span class=n>AutoModelForCausalLM</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>base_model_name</span><span class=p>,</span> <span class=n>torch_dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>bfloat16</span><span class=p>,</span> <span class=n>device_map</span><span class=o>=</span><span class=s2>&#34;auto&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>base_model_name</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 加载LoRA适配器并合并</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>PeftModel</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>base_model</span><span class=p>,</span> <span class=n>lora_checkpoint_path</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>merge_and_unload</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 保存合并后的完整模型</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>save_pretrained</span><span class=p>(</span><span class=n>merged_model_path</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span><span class=o>.</span><span class=n>save_pretrained</span><span class=p>(</span><span class=n>merged_model_path</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;模型已合并并保存至 </span><span class=si>{</span><span class=n>merged_model_path</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p>第二步：进行GPTQ量化</p><p>需要安装<code>auto-gptq</code>和<code>optimum</code>库：<code>pip install auto-gptq optimum</code></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># quantize_gptq.py</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>AutoModelForCausalLM</span><span class=p>,</span> <span class=n>AutoTokenizer</span><span class=p>,</span> <span class=n>GPTQConfig</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model_path</span> <span class=o>=</span> <span class=s2>&#34;./merged_llama3_8b_ai_qa&#34;</span>
</span></span><span class=line><span class=cl><span class=n>quantized_model_path</span> <span class=o>=</span> <span class=s2>&#34;./gptq_llama3_8b_ai_qa&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 1. 定义GPTQ配置</span>
</span></span><span class=line><span class=cl><span class=n>gptq_config</span> <span class=o>=</span> <span class=n>GPTQConfig</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>bits</span><span class=o>=</span><span class=mi>4</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>dataset</span><span class=o>=</span><span class=s2>&#34;c4&#34;</span><span class=p>,</span> <span class=c1># 使用C4数据集的一个子集作为校准数据</span>
</span></span><span class=line><span class=cl>    <span class=n>tokenizer</span><span class=o>=</span><span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_path</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>desc_act</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=c1># 对于Llama模型，通常设置为False</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 2. 加载模型并进行量化</span>
</span></span><span class=line><span class=cl><span class=n>quantized_model</span> <span class=o>=</span> <span class=n>AutoModelForCausalLM</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model_path</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>quantization_config</span><span class=o>=</span><span class=n>gptq_config</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>device_map</span><span class=o>=</span><span class=s2>&#34;auto&#34;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 3. 保存量化后的模型</span>
</span></span><span class=line><span class=cl><span class=n>quantized_model</span><span class=o>.</span><span class=n>save_pretrained</span><span class=p>(</span><span class=n>quantized_model_path</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_path</span><span class=p>)</span><span class=o>.</span><span class=n>save_pretrained</span><span class=p>(</span><span class=n>quantized_model_path</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;4-bit GPTQ量化模型已保存至 </span><span class=si>{</span><span class=n>quantized_model_path</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p>第三步：部署与压测</p><p>部署方式一：原生<code>transformers</code> (作为性能基线)</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># benchmark_transformers.py</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>AutoModelForCausalLM</span><span class=p>,</span> <span class=n>AutoTokenizer</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>time</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model_path</span> <span class=o>=</span> <span class=s2>&#34;./gptq_llama3_8b_ai_qa&#34;</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModelForCausalLM</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_path</span><span class=p>,</span> <span class=n>device_map</span><span class=o>=</span><span class=s2>&#34;auto&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_path</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>prompts</span> <span class=o>=</span> <span class=p>[</span><span class=s2>&#34;什么是神经网络？&#34;</span><span class=p>]</span> <span class=o>*</span> <span class=mi>8</span> <span class=c1># 模拟一个大小为8的批次</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># --- 压测 ---</span>
</span></span><span class=line><span class=cl><span class=n>start_time</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>inputs</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>(</span><span class=n>prompts</span><span class=p>,</span> <span class=n>return_tensors</span><span class=o>=</span><span class=s2>&#34;pt&#34;</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=s2>&#34;cuda&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>generate</span><span class=p>(</span><span class=n>inputs</span><span class=p>,</span> <span class=n>max_new_tokens</span><span class=o>=</span><span class=mi>100</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>end_time</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>total_time</span> <span class=o>=</span> <span class=n>end_time</span> <span class=o>-</span> <span class=n>start_time</span>
</span></span><span class=line><span class=cl><span class=n>num_requests</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>prompts</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>total_output_tokens</span> <span class=o>=</span> <span class=nb>sum</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>output</span><span class=p>)</span> <span class=k>for</span> <span class=n>output</span> <span class=ow>in</span> <span class=n>outputs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;--- Transformers (Static Batching) ---&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;总耗时: </span><span class=si>{</span><span class=n>total_time</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2> s&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;吞吐量 (RPS): </span><span class=si>{</span><span class=n>num_requests</span> <span class=o>/</span> <span class=n>total_time</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;吞吐量 (Output TPS): </span><span class=si>{</span><span class=n>total_output_tokens</span> <span class=o>/</span> <span class=n>total_time</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p>部署方式二：使用<code>vLLM</code></p><p>需要安装<code>vllm</code>：<code>pip install vllm</code></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># benchmark_vllm.py</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>vllm</span> <span class=kn>import</span> <span class=n>LLM</span><span class=p>,</span> <span class=n>SamplingParams</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>time</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model_path</span> <span class=o>=</span> <span class=s2>&#34;./gptq_llama3_8b_ai_qa&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 1. 初始化vLLM引擎</span>
</span></span><span class=line><span class=cl><span class=c1># vLLM会自动识别GPTQ模型</span>
</span></span><span class=line><span class=cl><span class=n>llm</span> <span class=o>=</span> <span class=n>LLM</span><span class=p>(</span><span class=n>model</span><span class=o>=</span><span class=n>model_path</span><span class=p>,</span> <span class=n>quantization</span><span class=o>=</span><span class=s2>&#34;gptq&#34;</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=s2>&#34;half&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>prompts</span> <span class=o>=</span> <span class=p>[</span><span class=s2>&#34;什么是神经网络？&#34;</span><span class=p>]</span> <span class=o>*</span> <span class=mi>8</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 2. 定义采样参数</span>
</span></span><span class=line><span class=cl><span class=n>sampling_params</span> <span class=o>=</span> <span class=n>SamplingParams</span><span class=p>(</span><span class=n>n</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>temperature</span><span class=o>=</span><span class=mf>0.0</span><span class=p>,</span> <span class=n>max_tokens</span><span class=o>=</span><span class=mi>100</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># --- 压测 ---</span>
</span></span><span class=line><span class=cl><span class=n>start_time</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=c1># vLLM可以一次性接收所有请求</span>
</span></span><span class=line><span class=cl><span class=n>outputs</span> <span class=o>=</span> <span class=n>llm</span><span class=o>.</span><span class=n>generate</span><span class=p>(</span><span class=n>prompts</span><span class=p>,</span> <span class=n>sampling_params</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>end_time</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>total_time</span> <span class=o>=</span> <span class=n>end_time</span> <span class=o>-</span> <span class=n>start_time</span>
</span></span><span class=line><span class=cl><span class=n>num_requests</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>prompts</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>total_output_tokens</span> <span class=o>=</span> <span class=nb>sum</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>output</span><span class=o>.</span><span class=n>outputs</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>token_ids</span><span class=p>)</span> <span class=k>for</span> <span class=n>output</span> <span class=ow>in</span> <span class=n>outputs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;--- vLLM (Continuous Batching) ---&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;总耗时: </span><span class=si>{</span><span class=n>total_time</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2> s&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;吞吐量 (RPS): </span><span class=si>{</span><span class=n>num_requests</span> <span class=o>/</span> <span class=n>total_time</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;吞吐量 (Output TPS): </span><span class=si>{</span><span class=n>total_output_tokens</span> <span class=o>/</span> <span class=n>total_time</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p>预期结果分析：</p><p>当你运行这两个脚本时，你会观察到：</p><ul><li>显存占用：GPTQ量化后的模型，其显存占用会比原始的FP16模型降低约4倍。</li><li>吞吐量：<code>vLLM</code>的吞吐量（无论是RPS还是TPS）将会是原生<code>transformers</code>的数倍甚至一个数量级以上。这是因为vLLM的持续批处理和高效的内存管理，使得GPU几乎没有被浪费。</li><li>延迟：对于单个请求，vLLM的延迟可能与原生实现相当或略低，但其优势在于高并发场景下的总体处理效率。</li></ul><p>这个实战项目直观地向你展示了，通过结合量化和高性能推理框架，我们能将LLM的推理性能提升到一个全新的水平。</p><h2 id=本章小结>本章小结</h2><p>在本章中，我们深入了LLM工程化的“最后一公里”，也是最具挑战性的领域——推理性能优化。</p><p>我们首先建立了衡量性能的“三维坐标系”：延迟、吞吐量和显存占用，理解了它们之间的内在联系与权衡。</p><p>接着，我们学习了“降本”的核心利器——模型量化。我们剖析了GPTQ和AWQ这两种主流的后训练量化技术，理解了它们如何通过巧妙的算法，在大幅压缩模型体积的同时，保持高模型性能。我们还了解了面向CPU的GGUF格式。</p><p>然后，我们转向了“增效”的终极武器——高性能推理框架。我们重点学习了vLLM，理解了其革命性的PagedAttention技术是如何解决KV缓存的内存浪费问题。我们还了解了NVIDIA的官方解决方案TensorRT-LLM，以及它们与传统<code>transformers</code>库在性能上的天壤之-别。</p><p>我们还探讨了持续批处理这一先进的调度策略，理解了它是如何通过动态地管理请求批次，将GPU利用率推向极限，从而实现吞吐量的指数级增长。</p><p>最后，通过一个端到端的实战项目，我们将量化与vLLM部署相结合，用真实的数据和压测结果，亲身体验了性能优化前后的巨大差异。</p><p>完成本章后，你已经掌握了一套完整的LLM推理性能优化工具箱。你不再仅仅满足于让模型“跑起来”，而是有能力让它“跑得快、跑得省”。这项极致的工程能力，将使你在构建和部署大规模、商业化的LLM应用时，具备无可替代的核心价值，真正成为一名从优秀迈向卓越的AI工程师。</p></div><div class=post_footer><div class=meta><div class=info><span class="field tags"><i class=ri-stack-line></i>
<a href=https://zhurongshuo.com/tags/%E4%B9%A6%E7%A8%BF/>书稿</a></span></div></div></div></div><div class=doc_comments></div></div></div></div><a id=back_to_top href=# class=back_to_top><i class=ri-arrow-up-s-line></i></a><footer class=footer><div class=powered_by><a href=https://varkai.com>Designed by VarKai, </a><a href=http://www.gohugo.io/>Proudly published with Hugo,</a></div><div class=footer_slogan><span>法不净空，觉无性也。</span></div><div class=powered_by style=margin-top:10px;font-size:14px><a href=https://zhurongshuo.com/>Copyright © 2010-2025 祝融说 zhurongshuo.com All Rights Reserved.</a></div></footer><script defer src=https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><link href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.css rel=stylesheet integrity="sha256-7qiTu3a8qjjWtcX9w+f2ulVUZSUdCZFEK62eRlmLmCE=" crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin=anonymous></script><script defer src=https://zhurongshuo.com/js/zozo.js></script><script type=text/javascript async src="https://cdn.jsdelivr.net/npm/mathjax@2.7.8/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["[[","]]"]],processEscapes:!0,processEnvironments:!0,skipTags:["script","noscript","style","textarea","pre"],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}}),MathJax.Hub.Queue(function(){var e,t=MathJax.Hub.getAllJax();for(e=0;e<t.length;e+=1)t[e].SourceElement().parentNode.className+=" has-jax"})</script><style>code.has-jax{font:inherit;font-size:100%;background:inherit;border:inherit;color:#515151}</style><script>window.GA_MEASUREMENT_ID="G-KKJ5ZEG1NB",window.GA_CONFIG={enableReadingTime:!0,enableScrollDepth:!0,enableOutboundLinks:!0,enableDownloads:!0,lazyLoadTimeout:3e3}</script><script defer src=https://zhurongshuo.com/js/ga-optimizer.js></script></body></html>