<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><meta name=google-site-verification content="8_xpI-TS3tNV8UPug-Q6Ef3BhKTcy0WOG7dEdAcm2zk"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="祝融"><link rel=dns-prefetch href=//cdn.jsdelivr.net><link rel=preconnect href=https://cdn.jsdelivr.net crossorigin><link rel=dns-prefetch href=//www.googletagmanager.com><link rel=preconnect href=https://www.googletagmanager.com crossorigin><link rel=canonical href=https://zhurongshuo.com/practices/season-4/ai-engineer-in-action/part-02/chapter-09/><title>祝融说。 第九章：释放LLM的潜能：构建RAG与智能体（Agent）</title><meta property="og:title" content="第九章：释放LLM的潜能：构建RAG与智能体（Agent）"><meta property="og:url" content="https://zhurongshuo.com/practices/season-4/ai-engineer-in-action/part-02/chapter-09/"><meta property="og:site_name" content="祝融说。"><meta property="og:locale" content="zh_CN"><meta property="og:type" content="article"><meta property="article:published_time" content="2025-12-09T00:00:00+08:00"><meta property="article:modified_time" content="2025-12-09T00:00:00+08:00"><meta property="article:tag" content="书稿"><meta name=description content="在前面的章节中，我们已经深入探索了大语言模型（LLM）的内部世界。我们学会了如何通过Prompt Engineering引导它，通过高效微调定制它。至此，我们手中的LLM已经像一个知识渊博、训练有素的“超级大脑”。然而，这个大脑在默认状态下，是与世隔绝的。它的知识被“冻结”在训练截止的那一刻，它无法访问最新的信息，也无法与外部工具互动来执行任务。它能“说”，但不能“做”；它能“回忆”，但不能“查询”。
"><meta property="og:description" content="在前面的章节中，我们已经深入探索了大语言模型（LLM）的内部世界。我们学会了如何通过Prompt Engineering引导它，通过高效微调定制它。至此，我们手中的LLM已经像一个知识渊博、训练有素的“超级大脑”。然而，这个大脑在默认状态下，是与世隔绝的。它的知识被“冻结”在训练截止的那一刻，它无法访问最新的信息，也无法与外部工具互动来执行任务。它能“说”，但不能“做”；它能“回忆”，但不能“查询”。
"><meta property="og:image" content="https://zhurongshuo.com/images/favicon.ico"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="第九章：释放LLM的潜能：构建RAG与智能体（Agent）"><meta name=twitter:description content="在前面的章节中，我们已经深入探索了大语言模型（LLM）的内部世界。我们学会了如何通过Prompt Engineering引导它，通过高效微调定制它。至此，我们手中的LLM已经像一个知识渊博、训练有素的“超级大脑”。然而，这个大脑在默认状态下，是与世隔绝的。它的知识被“冻结”在训练截止的那一刻，它无法访问最新的信息，也无法与外部工具互动来执行任务。它能“说”，但不能“做”；它能“回忆”，但不能“查询”。
"><meta name=twitter:image content="https://zhurongshuo.com/images/favicon.ico"><meta name=keywords content="AI工程师实战：从Python基础到LLM应用与性能优化,第九章：释放LLM的潜能：构建RAG与智能体（Agent）"><link rel="shortcut icon" href=https://zhurongshuo.com/images/favicon.ico><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/normalize.css><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/animate-custom.css><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/zozo.css><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/remixicon-custom.css><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/highlight.css><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","headline":"第九章：释放LLM的潜能：构建RAG与智能体（Agent）","description":"在前面的章节中，我们已经深入探索了大语言模型（LLM）的内部世界。我们学会了如何通过Prompt Engineering引导它，通过高效微调定制它。至此，我们手中的LLM已经像一个知识渊博、训练有素的“超级大脑”。然而，这个大脑在默认状态下，是与世隔绝的。它的知识被“冻结”在训练截止的那一刻，它无法访问最新的信息，也无法与外部工具互动来执行任务。它能“说”，但不能“做”；它能“回忆”，但不能“查询”。\n","datePublished":"2025-12-09T00:00:00\u002b08:00","dateModified":"2025-12-09T00:00:00\u002b08:00","author":{"@type":"Person","name":"祝融"},"publisher":{"@type":"Organization","name":"祝融说。","logo":{"@type":"ImageObject","url":"https:\/\/zhurongshuo.com\/images\/favicon.ico"}},"image":"https:\/\/zhurongshuo.com\/images\/favicon.ico","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/zhurongshuo.com\/practices\/season-4\/ai-engineer-in-action\/part-02\/chapter-09\/"},"keywords":"书稿"}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"首页","item":"https:\/\/zhurongshuo.com\/"},{"@type":"ListItem","position":2,"name":"practices","item":"https:\/\/zhurongshuo.com\/practices/"},{"@type":"ListItem","position":3,"name":"第九章：释放LLM的潜能：构建RAG与智能体（Agent）","item":"https:\/\/zhurongshuo.com\/practices\/season-4\/ai-engineer-in-action\/part-02\/chapter-09\/"}]}</script></head><body><div class="main animate__animated animate__fadeInDown"><div class="nav_container animated fadeInDown"><div class=site_nav id=site_nav><ul><li><a href=https://zhurongshuo.com/>首页</a></li><li><a href=https://zhurongshuo.com/start/>开始</a></li><li><a href=https://zhurongshuo.com/advanced/>进阶rc</a></li><li><a href=https://zhurongshuo.com/posts/>归档</a></li><li><a href=https://zhurongshuo.com/tags/>标签</a></li><li><a href=https://zhurongshuo.com/about/>关于</a></li></ul></div><div class=menu_icon><a id=menu_icon><i class=ri-menu-line></i></a></div></div><div class="header animated fadeInDown"><div class=site_title_container><div class=site_title><h1><a href=https://zhurongshuo.com/><span class=web-font>祝融说。</span></a></h1></div><div class=description><p class=sub_title>法不净空，觉无性也。</p><div class=my_socials><a href=https://zhurongshuo.com/books/ title=book-open><i class=ri-book-open-line></i></a>
<a href=https://zhurongshuo.com/practices/ title=trophy><i class=ri-trophy-line></i></a>
<a href=https://zhurongshuo.com/gallery/ title=gallery><i class=ri-gallery-line></i></a>
<a href=https://zhurongshuo.com/about/ title=game><i class=ri-game-line></i></a>
<a href type=application/rss+xml title=rss target=_blank><i class=ri-rss-fill></i></a></div></div></div></div><div class=content><div class=post_page><div class="post animate__animated animate__fadeInDown"><div class="post_title post_detail_title"><h2><a href=https://zhurongshuo.com/practices/season-4/ai-engineer-in-action/part-02/chapter-09/>第九章：释放LLM的潜能：构建RAG与智能体（Agent）</a></h2><span class=date>2025.12.09</span></div><div class="post_content markdown"><p>在前面的章节中，我们已经深入探索了大语言模型（LLM）的内部世界。我们学会了如何通过Prompt Engineering引导它，通过高效微调定制它。至此，我们手中的LLM已经像一个知识渊博、训练有素的“超级大脑”。然而，这个大脑在默认状态下，是与世隔绝的。它的知识被“冻结”在训练截止的那一刻，它无法访问最新的信息，也无法与外部工具互动来执行任务。它能“说”，但不能“做”；它能“回忆”，但不能“查询”。</p><p>要将LLM从一个强大的“语言模型”真正升级为能够解决现实世界问题的“智能助理”，我们必须打破这层壁垒，让它与外部世界连接起来。本章，我们将专注于实现这一目标的两种核心技术范式：检索增强生成（Retrieval-Augmented Generation, RAG）和智能体（Agent）。</p><p>RAG：为LLM装上“外挂知识库”</p><p>我们知道，LLM存在“幻觉”和“知识过时”的问题。RAG架构正是为了解决这一痛点而生。它的核心思想是，在让LLM回答问题之前，先从一个外部的、可信的、可实时更新的知识库（如公司文档、产品手册、数据库）中，检索（Retrieve）出最相关的信息片段，然后将这些信息作为上下文增强（Augment）到Prompt中，最后让LLM基于这些可靠的信息进行生成（Generate）。RAG就像是为LLM配备了一个功能强大的“搜索引擎”和“开放式书架”，使其能够回答基于私有或实时知识的问题，极大地提升了答案的准确性和时效性。</p><p>Agent：赋予LLM“思考”与“行动”的能力</p><p>如果说RAG是让LLM“读万卷书”，那么Agent就是让LLM“行万里路”。一个Agent系统将LLM从一个被动的文本生成器，提升为一个主动的、有目标的任务执行者。它以LLM为核心“大脑”，通过一个“思考-行动-观察”的循环，来决策下一步该做什么。它可以被赋予一系列工具（Tools），如调用计算器、查询天气API、执行代码、搜索网络等。当面对一个复杂任务时，Agent会自主地进行任务分解，选择并使用合适的工具，观察结果，并根据结果进行下一步的思考和行动，直到最终完成任务。</p><p>本章，我们将深入这两种激动人心的技术：</p><ol><li>详解RAG架构：我们将从文本切分、向量化，到向量数据库的应用，再到从检索到生成的完整流程，为你“庖丁解牛”般地解析RAG的每一个环节。</li><li>入门Agent开发：我们将学习Agent的核心思想框架（如ReAct），并借助强大的开源框架（如LangChain或LlamaIndex），快速上手开发自己的智能体，并为其赋予实用的工具。</li><li>双实战驱动：我们将通过两个紧贴实际应用的实战项目——构建一个基于公司文档的智能问答机器人（RAG）和开发一个能查询天气和计算器的简单智能体（Agent）——将理论知识转化为触手可及的工程实践。</li></ol><p>掌握了RAG和Agent，你就掌握了当前LLM应用开发的两大“杀手锏”。你将能够构建出真正有用、可靠、且能与现实世界互动的智能应用。现在，让我们一起，为我们的大模型装上“眼睛”和“双手”，开启它真正的潜能。</p><h2 id=91-检索增强生成rag架构详解>9.1 检索增强生成（RAG）架构详解</h2><p>RAG是一种将信息检索（Information Retrieval）与语言模型生成（Language Model Generation）相结合的架构，旨在通过引入外部知识来增强LLM的回答质量。</p><p>RAG的核心优势：</p><ul><li>缓解幻觉：LLM被强制要求基于提供的上下文来回答，而不是凭空捏造。</li><li>知识实时更新：你无需重新训练昂贵的LLM，只需更新外部知识库，模型就能接触到最新的信息。</li><li>可追溯性与可解释性：可以向用户展示答案是基于哪些源文档生成的，提高了答案的可信度。</li><li>数据安全：私有数据存储在自己的知识库中，无需用其训练模型，降低了数据泄露的风险。</li></ul><p>一个典型的RAG流程包含两个阶段：数据索引（Indexing）和检索与生成（Retrieval & Generation）。</p><p>RAG 流程示意图</p><h3 id=911-数据索引阶段构建你的知识库>9.1.1 数据索引阶段：构建你的知识库</h3><p>这个阶段是离线进行的，目的是将你的原始文档（如PDF, TXT, Markdown, HTML等）处理成一个可供快速检索的格式。</p><p>第一步：加载与切分（Load & Split）</p><p>原始文档通常很长，无法直接放入LLM的上下文窗口。因此，第一步就是将长文档切分成更小的、有意义的文本块（Chunks）。</p><p>加载器（Loaders）：使用如<code>LlamaIndex</code>或<code>LangChain</code>中的文档加载器，可以轻松地读取各种格式的文件。</p><p>切分器（Splitters）：</p><ul><li>固定大小切分（Fixed-size Chunking）：最简单的方法，按固定字符数（如1000个字符）切分，并设置一定的重叠（Overlap，如100个字符），以保证语义的连续性。</li><li>递归字符切分（Recursive Character Text Splitter）：一种更智能的方法。它会尝试按一系列分隔符（如<code>\n\n</code>, <code>\n</code>, <code></code>）来切分，优先保持段落、句子的完整性。</li><li>语义切分（Semantic Chunking）：更高级的方法，通过分析文本块之间的语义相似度来决定切分点，力求每个Chunk都是一个语义完整的单元。</li></ul><p>切分的艺术：</p><ul><li>Chunk的大小是一个关键超参数。</li><li>太小：可能丢失重要的上下文信息，导致检索到的片段过于零散。</li><li>太大：可能包含太多与查询无关的噪声，增加了LLM处理的负担。</li><li>一个常见的起点是512到1024个token。</li></ul><p>第二步：向量化（Embedding）</p><p>切分完成后，我们需要将每个文本块（Chunk）转换为一个向量（Vector），这个过程称为嵌入（Embedding）。这个向量是文本块在多维语义空间中的坐标。</p><p>嵌入模型（Embedding Model）：我们使用一个预训练好的句子转换模型（Sentence Transformer）来完成这个任务。这些模型专门用于将文本映射到能够捕捉其语义的稠密向量空间。</p><p>如何选择嵌入模型？</p><p>MTEB排行榜（Massive Text Embedding Benchmark）：这是评估嵌入模型性能的黄金标准。</p><p>主流选择：</p><ul><li>英文：<code>BAAI/bge-large-en-v1.5</code> (当前性能领先), <code>sentence-transformers/all-MiniLM-L6-v2</code> (轻量高效)。</li><li>中文/多语言：<code>BAAI/bge-m3</code> (强大的多语言模型), <code>infgrad/stella-base-zh-v2</code> (优秀的中文模型)。</li></ul><p>实现：使用<code>sentence-transformers</code>库或Hugging Face的<code>transformers</code>库可以轻松加载和使用这些模型。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sentence_transformers</span> <span class=kn>import</span> <span class=n>SentenceTransformer</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 加载嵌入模型</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>SentenceTransformer</span><span class=p>(</span><span class=s1>&#39;BAAI/bge-base-en-v1.5&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 准备文本块</span>
</span></span><span class=line><span class=cl><span class=n>chunks</span> <span class=o>=</span> <span class=p>[</span><span class=s2>&#34;RAG stands for Retrieval-Augmented Generation.&#34;</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>          <span class=s2>&#34;It enhances LLMs with external knowledge.&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 进行向量化</span>
</span></span><span class=line><span class=cl><span class=n>embeddings</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=n>chunks</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>embeddings</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span> <span class=c1># (2, 768) -&gt; 2个文本块，每个都是768维的向量</span>
</span></span></code></pre></div><h3 id=912-向量数据库选型与应用>9.1.2 向量数据库选型与应用</h3><p>现在我们有了一大堆文本块和它们对应的向量。当用户提出一个问题时，我们需要找到与问题最“相似”的文本块。在一个拥有数百万文本块的知识库中，逐个计算相似度是不可行的。这时，向量数据库（Vector Database）就派上用场了。</p><p>向量数据库专门用于高效地存储和检索高维向量。它的核心技术是近似最近邻搜索（Approximate Nearest Neighbor, ANN）。</p><p>工作原理（简述）：</p><p>ANN算法通过构建特殊的索引结构（如IVF, HNSW），来避免全量搜索。它不能保证100%找到最相似的向量，但在牺牲极小的精度的前提下，将搜索速度提升了几个数量级，这对于实时应用是完全可以接受的。</p><p>主流向量数据库选型：</p><ol><li><p>内存型/本地型库：</p><p>FAISS (Facebook AI Similarity Search)：由Facebook AI开发的高性能向量相似度搜索库。它是一个C++库，有Python接口。功能强大，速度极快，但本身不提供数据库的管理功能，更像一个“搜索引擎库”。
ChromaDB：一个为AI应用设计的开源向量数据库。它非常易于使用，提供了简单的Python API，支持本地持久化存储，非常适合快速原型开发和中小型项目。</p></li><li><p>服务端/分布式数据库：</p><p>Pinecone, Weaviate, Milvus：这些是功能更全面的、可作为独立服务部署的向量数据库。它们支持分布式扩展、元数据过滤、实时索引更新等高级功能，适合大规模生产环境。</p></li></ol><p>使用ChromaDB示例：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>chromadb</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 1. 初始化ChromaDB客户端 (可以存到磁盘)</span>
</span></span><span class=line><span class=cl><span class=n>client</span> <span class=o>=</span> <span class=n>chromadb</span><span class=o>.</span><span class=n>PersistentClient</span><span class=p>(</span><span class=n>path</span><span class=o>=</span><span class=s2>&#34;/path/to/db&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 2. 创建或获取一个集合 (Collection)</span>
</span></span><span class=line><span class=cl><span class=n>collection</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>get_or_create_collection</span><span class=p>(</span><span class=n>name</span><span class=o>=</span><span class=s2>&#34;my_knowledge_base&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 3. 添加数据 (Indexing)</span>
</span></span><span class=line><span class=cl><span class=c1># 假设我们已经有了chunks和embeddings</span>
</span></span><span class=line><span class=cl><span class=n>collection</span><span class=o>.</span><span class=n>add</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>embeddings</span><span class=o>=</span><span class=n>embeddings</span><span class=o>.</span><span class=n>tolist</span><span class=p>(),</span> <span class=c1># 嵌入向量</span>
</span></span><span class=line><span class=cl>    <span class=n>documents</span><span class=o>=</span><span class=n>chunks</span><span class=p>,</span>             <span class=c1># 原始文本块</span>
</span></span><span class=line><span class=cl>    <span class=n>metadatas</span><span class=o>=</span><span class=p>[{</span><span class=s2>&#34;source&#34;</span><span class=p>:</span> <span class=s2>&#34;doc1.pdf&#34;</span><span class=p>},</span> <span class=p>{</span><span class=s2>&#34;source&#34;</span><span class=p>:</span> <span class=s2>&#34;doc2.txt&#34;</span><span class=p>}],</span> <span class=c1># 元数据</span>
</span></span><span class=line><span class=cl>    <span class=n>ids</span><span class=o>=</span><span class=p>[</span><span class=sa>f</span><span class=s2>&#34;chunk_</span><span class=si>{</span><span class=n>i</span><span class=si>}</span><span class=s2>&#34;</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>chunks</span><span class=p>))]</span> <span class=c1># 唯一的ID</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># --- 检索阶段 ---</span>
</span></span><span class=line><span class=cl><span class=c1># 4. 查询 (Query)</span>
</span></span><span class=line><span class=cl><span class=n>query_text</span> <span class=o>=</span> <span class=s2>&#34;What is RAG?&#34;</span>
</span></span><span class=line><span class=cl><span class=n>query_embedding</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>encode</span><span class=p>([</span><span class=n>query_text</span><span class=p>])[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>tolist</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 检索最相似的 top-k 个结果</span>
</span></span><span class=line><span class=cl><span class=n>results</span> <span class=o>=</span> <span class=n>collection</span><span class=o>.</span><span class=n>query</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>query_embeddings</span><span class=o>=</span><span class=p>[</span><span class=n>query_embedding</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=n>n_results</span><span class=o>=</span><span class=mi>2</span> <span class=c1># 返回最相似的2个</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>results</span><span class=p>[</span><span class=s1>&#39;documents&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=c1># [[&#39;RAG stands for Retrieval-Augmented Generation.&#39;, &#39;It enhances LLMs with external knowledge.&#39;]]</span>
</span></span></code></pre></div><h3 id=913-从检索到生成的完整流程>9.1.3 从检索到生成的完整流程</h3><p>现在我们已经打通了索引和检索，可以串联起整个RAG的第二阶段了。</p><p>第三步：检索（Retrieve）</p><ol><li>接收用户问题 <code>query</code>。</li><li>使用与索引时相同的嵌入模型，将 <code>query</code> 转换为 <code>query_embedding</code>。</li><li>在向量数据库中，使用 <code>query_embedding</code> 进行相似度搜索，检索出Top-K个最相关的文本块 <code>retrieved_chunks</code>。</li></ol><p>第四步：增强与生成（Augment & Generate）</p><ol><li><p>构建Prompt：将检索到的文本块 <code>retrieved_chunks</code> 和用户的原始问题 <code>query</code>，一起组合成一个精心设计的Prompt。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>Context information is below.
</span></span><span class=line><span class=cl>---------------------
</span></span><span class=line><span class=cl>{context_str}  &lt;-- 将retrieved_chunks拼接成一个字符串
</span></span><span class=line><span class=cl>---------------------
</span></span><span class=line><span class=cl>Given the context information and not prior knowledge, answer the query.
</span></span><span class=line><span class=cl>Query: {query_str}
</span></span><span class=line><span class=cl>Answer:
</span></span></code></pre></div></li><li><p>调用LLM：将这个增强后的Prompt发送给LLM。</p></li><li><p>获取答案：LLM会基于提供的上下文，生成最终的答案。</p></li></ol><p>这个流程确保了LLM的回答是有据可依的，极大地提升了答案的质量和可靠性。</p><h2 id=92-智能体agent开发入门>9.2 智能体（Agent）开发入门</h2><p>如果说RAG是给LLM一个“只读”的外部大脑，那么Agent就是赋予LLM“思考”和“行动”的能力，让它能与外部世界进行“读写”交互。</p><h3 id=921-agent的核心思想react框架>9.2.1 Agent的核心思想：ReAct框架</h3><p>ReAct (Reasoning and Acting) 是当前Agent系统最核心、最基础的思想框架之一。它将LLM的思考过程显式地分解为一个“Thought -> Action -> Observation”的循环。</p><ul><li>Thought (思考)：LLM分析当前的任务目标和已有的信息，进行推理，并决定下一步应该采取什么行动。这个思考过程是LLM自己生成的、对人类可读的文本。</li><li>Action (行动)：根据思考，LLM决定调用一个工具（Tool），并指定调用该工具所需的输入（Action Input）。例如，<code>Action: Calculator, Action Input: 2+2</code>。</li><li>Observation (观察)：Agent系统执行这个Action（例如，运行计算器得到结果<code>4</code>），并将工具返回的结果作为“观察”信息，反馈给LLM。</li></ul><p>LLM接收到这个Observation后，开始新一轮的循环：根据新的观察结果进行下一步的Thought，决定下一个Action，直到它认为任务已经完成，并生成最终的答案。</p><p>ReAct示例（简化版）：</p><p>任务: "爱因斯坦的年龄的平方是多少？"</p><ul><li>Thought 1: 我需要先知道爱因斯坦的年龄。我没有这个信息，所以我需要搜索。</li><li>Action 1: <code>Search("爱因斯坦的年龄")</code></li><li>Observation 1: "阿尔伯特·爱因斯坦（1879年3月14日—1955年4月18日），享年76岁。"</li><li>Thought 2: 我现在知道爱因斯坦的年龄是76岁。任务要求计算年龄的平方，即76的平方。我需要一个计算器来完成这个计算。</li><li>Action 2: <code>Calculator("76^2")</code></li><li>Observation 2: "5776"</li><li>Thought 3: 我已经得到了计算结果5776。我已经完成了任务的所有步骤，可以给出最终答案了。</li><li>Final Answer: 爱因斯坦年龄的平方是5776。</li></ul><p>通过这个循环，LLM将一个复杂的、需要外部信息的任务，分解成了多个简单的、可执行的步骤，并利用工具来弥补自身能力的不足。</p><h3 id=922-langchainllamaindex框架快速上手>9.2.2 LangChain/LlamaIndex框架快速上手</h3><p>从零实现一个Agent系统非常复杂，需要处理Prompt模板、工具调用、输出解析、循环控制等大量细节。幸运的是，<code>LangChain</code>和<code>LlamaIndex</code>这两个强大的开源框架，极大地简化了Agent的开发。</p><ul><li>LangChain：一个功能全面、非常灵活的LLM应用开发框架。它提供了构建Agent所需的各种组件（LLM接口、Prompt模板、输出解析器、工具等），并允许你像搭积木一样自由组合。它的学习曲线相对陡峭，但自由度高。</li><li>LlamaIndex：最初专注于RAG，但现在也发展出了强大的Agent能力。它的抽象层次更高，通常能用更少的代码实现一个功能完备的RAG或Agent系统，非常适合快速上手。</li></ul><p>使用LangChain创建一个简单Agent的流程：</p><ol><li>定义工具（Tools）：定义Agent可以使用的工具列表。</li><li>初始化LLM：选择并配置一个LLM（如<code>ChatOpenAI</code>或<code>HuggingFaceHub</code>）。</li><li>创建Prompt模板：设计一个符合ReAct框架的Prompt模板，告诉LLM它有哪些工具可用，以及应该如何思考和行动。</li><li>构建Agent：将LLM、工具和Prompt组合起来，创建一个Agent。</li><li>创建Agent执行器（Agent Executor）：这是一个负责运行Agent循环的控制器。</li><li>运行Agent：调用执行器来完成任务。</li></ol><h3 id=923-为agent赋予工具tools>9.2.3 为Agent赋予工具（Tools）</h3><p>工具是Agent与外部世界交互的桥梁。任何可以被程序化调用的功能，都可以被封装成一个工具。</p><p>常见的工具类型：</p><ul><li>计算器：执行数学运算。</li><li>搜索引擎：通过API（如Google Search API, Tavily）进行网络搜索。</li><li>Python REPL：执行Python代码，能力极强但风险也高。</li><li>数据库查询：连接数据库，执行SQL查询。</li><li>API调用：调用任何第三方API（如天气、股票、地图等）。</li><li>RAG检索器：将我们之前构建的RAG检索器本身，也封装成一个工具。当Agent认为需要从私有知识库中查找信息时，就可以调用这个工具。</li></ul><p>在LangChain中，定义一个工具通常需要：</p><ul><li><code>name</code>: 工具的名称，LLM会通过这个名字来决定调用哪个工具。</li><li><code>description</code>: 极其重要。对工具功能的清晰描述。LLM完全依赖这个描述来理解工具的用途和何时使用它。</li><li><code>func</code>: 工具背后实际执行的Python函数。</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>langchain.tools</span> <span class=kn>import</span> <span class=n>tool</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@tool</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>get_weather</span><span class=p>(</span><span class=n>city</span><span class=p>:</span> <span class=nb>str</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>str</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Returns the current weather for a given city.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=c1># 在这里实现调用天气API的真实逻辑</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>city</span> <span class=o>==</span> <span class=s2>&#34;北京&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=s2>&#34;北京今天晴，25摄氏度。&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=sa>f</span><span class=s2>&#34;抱歉，我无法查询</span><span class=si>{</span><span class=n>city</span><span class=si>}</span><span class=s2>的天气。&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Agent就可以通过 &#39;get_weather&#39; 这个名字来使用这个工具了。</span>
</span></span></code></pre></div><h2 id=93-实战项目一构建一个基于公司文档的智能问答机器人rag>9.3 实战项目一：构建一个基于公司文档的智能问答机器人（RAG）</h2><p>项目目标：假设我们有一些关于公司政策的Markdown文档，我们将构建一个RAG系统，让员工可以就这些政策进行提问。</p><p>技术栈：<code>transformers</code> (for embeddings), <code>chromadb</code>, <code>langchain</code></p><p>第一步：准备数据和环境</p><ol><li><p>创建一些<code>.md</code>文件，如<code>policy_leave.md</code>, <code>policy_expense.md</code>。</p><p><code>policy_leave.md</code>: "公司提供每年15天的带薪年假。申请年假需提前两周通过HR系统提交。"
<code>policy_expense.md</code>: "员工的出差交通费可以报销。乘坐飞机需选择经济舱。出租车费用需提供发票。"</p></li><li><p>安装库: <code>pip install langchain chromadb sentence-transformers</code></p></li></ol><p>第二步：索引数据</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># rag_indexing.py</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>langchain.document_loaders</span> <span class=kn>import</span> <span class=n>DirectoryLoader</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>langchain.text_splitter</span> <span class=kn>import</span> <span class=n>RecursiveCharacterTextSplitter</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>langchain.embeddings</span> <span class=kn>import</span> <span class=n>HuggingFaceEmbeddings</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>langchain.vectorstores</span> <span class=kn>import</span> <span class=n>Chroma</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 1. 加载文档</span>
</span></span><span class=line><span class=cl><span class=n>loader</span> <span class=o>=</span> <span class=n>DirectoryLoader</span><span class=p>(</span><span class=s1>&#39;./company_policies/&#39;</span><span class=p>,</span> <span class=n>glob</span><span class=o>=</span><span class=s2>&#34;/*.md&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>documents</span> <span class=o>=</span> <span class=n>loader</span><span class=o>.</span><span class=n>load</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 2. 切分文本</span>
</span></span><span class=line><span class=cl><span class=n>text_splitter</span> <span class=o>=</span> <span class=n>RecursiveCharacterTextSplitter</span><span class=p>(</span><span class=n>chunk_size</span><span class=o>=</span><span class=mi>500</span><span class=p>,</span> <span class=n>chunk_overlap</span><span class=o>=</span><span class=mi>50</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>texts</span> <span class=o>=</span> <span class=n>text_splitter</span><span class=o>.</span><span class=n>split_documents</span><span class=p>(</span><span class=n>documents</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 3. 加载嵌入模型</span>
</span></span><span class=line><span class=cl><span class=n>embeddings</span> <span class=o>=</span> <span class=n>HuggingFaceEmbeddings</span><span class=p>(</span><span class=n>model_name</span><span class=o>=</span><span class=s1>&#39;BAAI/bge-base-en-v1.5&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 4. 创建并持久化向量数据库</span>
</span></span><span class=line><span class=cl><span class=c1># 这会将向量数据存储在 &#39;db&#39; 目录下</span>
</span></span><span class=line><span class=cl><span class=n>vectordb</span> <span class=o>=</span> <span class=n>Chroma</span><span class=o>.</span><span class=n>from_documents</span><span class=p>(</span><span class=n>documents</span><span class=o>=</span><span class=n>texts</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>                                 <span class=n>embedding</span><span class=o>=</span><span class=n>embeddings</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>                                 <span class=n>persist_directory</span><span class=o>=</span><span class=s2>&#34;./db&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>vectordb</span><span class=o>.</span><span class=n>persist</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;索引创建完成。&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p>第三步：构建问答链（QA Chain）</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># rag_qa.py</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>langchain.vectorstores</span> <span class=kn>import</span> <span class=n>Chroma</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>langchain.embeddings</span> <span class=kn>import</span> <span class=n>HuggingFaceEmbeddings</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>langchain.chat_models</span> <span class=kn>import</span> <span class=n>ChatOllama</span> <span class=c1># 使用本地Ollama运行的LLM，也可以换成ChatOpenAI等</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>langchain.chains</span> <span class=kn>import</span> <span class=n>RetrievalQA</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 1. 加载嵌入模型和向量数据库</span>
</span></span><span class=line><span class=cl><span class=n>embeddings</span> <span class=o>=</span> <span class=n>HuggingFaceEmbeddings</span><span class=p>(</span><span class=n>model_name</span><span class=o>=</span><span class=s1>&#39;BAAI/bge-base-en-v1.5&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>vectordb</span> <span class=o>=</span> <span class=n>Chroma</span><span class=p>(</span><span class=n>persist_directory</span><span class=o>=</span><span class=s2>&#34;./db&#34;</span><span class=p>,</span> <span class=n>embedding_function</span><span class=o>=</span><span class=n>embeddings</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 2. 初始化LLM</span>
</span></span><span class=line><span class=cl><span class=c1># 假设你已经通过Ollama在本地运行了Llama 3: ollama run llama3</span>
</span></span><span class=line><span class=cl><span class=n>llm</span> <span class=o>=</span> <span class=n>ChatOllama</span><span class=p>(</span><span class=n>model</span><span class=o>=</span><span class=s2>&#34;llama3&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 3. 创建检索器 (Retriever)</span>
</span></span><span class=line><span class=cl><span class=n>retriever</span> <span class=o>=</span> <span class=n>vectordb</span><span class=o>.</span><span class=n>as_retriever</span><span class=p>(</span><span class=n>search_kwargs</span><span class=o>=</span><span class=p>{</span><span class=s2>&#34;k&#34;</span><span class=p>:</span> <span class=mi>2</span><span class=p>})</span> <span class=c1># 检索最相关的2个块</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 4. 创建RetrievalQA链</span>
</span></span><span class=line><span class=cl><span class=c1># chain_type=&#34;stuff&#34; 是最简单的方式，将所有检索到的文档“塞”进一个Prompt里</span>
</span></span><span class=line><span class=cl><span class=n>qa_chain</span> <span class=o>=</span> <span class=n>RetrievalQA</span><span class=o>.</span><span class=n>from_chain_type</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>llm</span><span class=o>=</span><span class=n>llm</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>chain_type</span><span class=o>=</span><span class=s2>&#34;stuff&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>retriever</span><span class=o>=</span><span class=n>retriever</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>return_source_documents</span><span class=o>=</span><span class=kc>True</span> <span class=c1># 同时返回源文档，便于追溯</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 5. 进行提问</span>
</span></span><span class=line><span class=cl><span class=n>query</span> <span class=o>=</span> <span class=s2>&#34;我每年有多少天年假？&#34;</span>
</span></span><span class=line><span class=cl><span class=n>result</span> <span class=o>=</span> <span class=n>qa_chain</span><span class=p>({</span><span class=s2>&#34;query&#34;</span><span class=p>:</span> <span class=n>query</span><span class=p>})</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;回答:&#34;</span><span class=p>,</span> <span class=n>result</span><span class=p>[</span><span class=s1>&#39;result&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;来源:&#34;</span><span class=p>,</span> <span class=p>[</span><span class=n>doc</span><span class=o>.</span><span class=n>metadata</span><span class=p>[</span><span class=s1>&#39;source&#39;</span><span class=p>]</span> <span class=k>for</span> <span class=n>doc</span> <span class=ow>in</span> <span class=n>result</span><span class=p>[</span><span class=s1>&#39;source_documents&#39;</span><span class=p>]])</span>
</span></span></code></pre></div><p>运行<code>rag_qa.py</code>后，系统会首先从向量数据库中检索到关于年假政策的文本块，然后将其与问题一起发送给LLM，最终得到准确的回答，并能告诉你答案来自哪个文档。</p><h2 id=94-实战项目二开发一个能查询天气和计算器的简单智能体agent>9.4 实战项目二：开发一个能查询天气和计算器的简单智能体（Agent）</h2><p>项目目标：构建一个Agent，它能理解自然语言问题，并自主决定是使用天气查询工具还是计算器工具来回答。</p><p>技术栈：<code>langchain</code>, <code>langchain-openai</code> (或 <code>langchain-community</code> for Ollama)</p><p>第一步：定义工具</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># agent_tools.py</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>langchain.tools</span> <span class=kn>import</span> <span class=n>tool</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@tool</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>get_weather</span><span class=p>(</span><span class=n>city</span><span class=p>:</span> <span class=nb>str</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>str</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Returns the current weather for a given city.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;--- 调用天气工具，城市: </span><span class=si>{</span><span class=n>city</span><span class=si>}</span><span class=s2> ---&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=s2>&#34;北京&#34;</span> <span class=ow>in</span> <span class=n>city</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=s2>&#34;北京今天多云转晴，气温15-28摄氏度。&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>elif</span> <span class=s2>&#34;上海&#34;</span> <span class=ow>in</span> <span class=n>city</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=s2>&#34;上海今天有小雨，气温20-25摄氏度。&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=sa>f</span><span class=s2>&#34;抱歉，我无法查询 </span><span class=si>{</span><span class=n>city</span><span class=si>}</span><span class=s2> 的天气。&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@tool</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>calculator</span><span class=p>(</span><span class=n>expression</span><span class=p>:</span> <span class=nb>str</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>str</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;A simple calculator that evaluates a mathematical expression.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;--- 调用计算器工具，表达式: </span><span class=si>{</span><span class=n>expression</span><span class=si>}</span><span class=s2> ---&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>try</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=c1># 使用eval有安全风险，真实项目中应使用更安全的库</span>
</span></span><span class=line><span class=cl>        <span class=n>result</span> <span class=o>=</span> <span class=nb>eval</span><span class=p>(</span><span class=n>expression</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=nb>str</span><span class=p>(</span><span class=n>result</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>except</span> <span class=ne>Exception</span> <span class=k>as</span> <span class=n>e</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=sa>f</span><span class=s2>&#34;计算错误: </span><span class=si>{</span><span class=n>e</span><span class=si>}</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>tools</span> <span class=o>=</span> <span class=p>[</span><span class=n>get_weather</span><span class=p>,</span> <span class=n>calculator</span><span class=p>]</span>
</span></span></code></pre></div><p>第二步：构建并运行Agent</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># agent_run.py</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>langchain_openai</span> <span class=kn>import</span> <span class=n>ChatOpenAI</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>langchain</span> <span class=kn>import</span> <span class=n>hub</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>langchain.agents</span> <span class=kn>import</span> <span class=n>create_react_agent</span><span class=p>,</span> <span class=n>AgentExecutor</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>agent_tools</span> <span class=kn>import</span> <span class=n>tools</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 1. 初始化LLM</span>
</span></span><span class=line><span class=cl><span class=c1># 需要设置你的OpenAI API Key: os.environ[&#34;OPENAI_API_KEY&#34;] = &#34;...&#34;</span>
</span></span><span class=line><span class=cl><span class=n>llm</span> <span class=o>=</span> <span class=n>ChatOpenAI</span><span class=p>(</span><span class=n>model</span><span class=o>=</span><span class=s2>&#34;gpt-3.5-turbo&#34;</span><span class=p>,</span> <span class=n>temperature</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 2. 获取ReAct框架的Prompt模板</span>
</span></span><span class=line><span class=cl><span class=c1># 这是LangChain提供的一个经过优化的标准ReAct Prompt</span>
</span></span><span class=line><span class=cl><span class=n>prompt</span> <span class=o>=</span> <span class=n>hub</span><span class=o>.</span><span class=n>pull</span><span class=p>(</span><span class=s2>&#34;hwchase17/react&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 3. 创建Agent</span>
</span></span><span class=line><span class=cl><span class=c1># 这个函数会将LLM、工具和Prompt绑定在一起</span>
</span></span><span class=line><span class=cl><span class=n>agent</span> <span class=o>=</span> <span class=n>create_react_agent</span><span class=p>(</span><span class=n>llm</span><span class=p>,</span> <span class=n>tools</span><span class=p>,</span> <span class=n>prompt</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 4. 创建Agent执行器</span>
</span></span><span class=line><span class=cl><span class=n>agent_executor</span> <span class=o>=</span> <span class=n>AgentExecutor</span><span class=p>(</span><span class=n>agent</span><span class=o>=</span><span class=n>agent</span><span class=p>,</span> <span class=n>tools</span><span class=o>=</span><span class=n>tools</span><span class=p>,</span> <span class=n>verbose</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span> <span class=c1># verbose=True会打印出完整的思考链</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 5. 运行Agent</span>
</span></span><span class=line><span class=cl><span class=c1># 测试1: 需要调用天气工具</span>
</span></span><span class=line><span class=cl><span class=n>response1</span> <span class=o>=</span> <span class=n>agent_executor</span><span class=o>.</span><span class=n>invoke</span><span class=p>({</span><span class=s2>&#34;input&#34;</span><span class=p>:</span> <span class=s2>&#34;今天北京的天气怎么样？&#34;</span><span class=p>})</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;最终回答:&#34;</span><span class=p>,</span> <span class=n>response1</span><span class=p>[</span><span class=s2>&#34;output&#34;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>&#34;</span> <span class=o>+</span> <span class=s2>&#34;=&#34;</span><span class=o>*</span><span class=mi>50</span> <span class=o>+</span> <span class=s2>&#34;</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 测试2: 需要调用计算器工具</span>
</span></span><span class=line><span class=cl><span class=n>response2</span> <span class=o>=</span> <span class=n>agent_executor</span><span class=o>.</span><span class=n>invoke</span><span class=p>({</span><span class=s2>&#34;input&#34;</span><span class=p>:</span> <span class=s2>&#34;3的5次方是多少？&#34;</span><span class=p>})</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;最终回答:&#34;</span><span class=p>,</span> <span class=n>response2</span><span class=p>[</span><span class=s2>&#34;output&#34;</span><span class=p>])</span>
</span></span></code></pre></div><p>当你运行<code>agent_run.py</code>时，<code>verbose=True</code>会让你清晰地看到ReAct的每一步：</p><p>对于问题1，LLM会Thought: "我需要查询北京的天气"，然后Action: <code>get_weather("北京")</code>。</p><p>对于问题2，LLM会Thought: "我需要计算3的5次方"，然后Action: <code>calculator("35")</code>。</p><p>这完美地展示了Agent如何根据任务需求，自主地选择和使用正确的工具。</p><h2 id=本章小结>本章小结</h2><p>在本章中，我们迈出了将LLM从一个“封闭大脑”转变为一个能与外部世界互动的“智能体”的关键两步。</p><p>我们首先深入剖析了检索增强生成（RAG）架构。我们学习了其从数据索引（加载、切分、向量化）到检索与生成的完整流程，并掌握了如何使用<code>ChromaDB</code>等向量数据库来构建和查询知识库。通过RAG，我们为LLM装上了一个强大的“外挂知识库”，有效解决了其知识局限和幻觉问题。</p><p>接着，我们探索了更前沿的智能体（Agent）技术。我们理解了其核心的ReAct思想框架，即通过“思考-行动-观察”的循环，让LLM能够进行任务分解、调用外部工具。借助<code>LangChain</code>等框架，我们学会了如何快速地构建一个能够自主决策和行动的Agent。</p><p>最后，通过两个紧密结合实际的实战项目，我们将RAG和Agent的理论知识，转化为了可以运行和体验的代码。我们亲手构建了一个企业级的智能问答机器人雏形，并开发了一个能使用工具的简单智能体。</p><p>完成本章后，你已经掌握了当前LLL应用层开发最核心、最热门的两大范式。你不再仅仅是LLM的使用者或微调者，你已经成为了一名能够设计和构建复杂、实用的AI应用的“架构师”。你所构建的应用，将不再局限于模型自身的知识，而是能够连接无限的外部数据和功能，从而在真实世界中创造出巨大的价值。在本书的最后，我们将展望AI工程的未来，探讨如何将我们构建的应用，通过CI/CD、监控和评估，打造成一个真正稳定、可靠的生产级系统。</p></div><div class=post_footer><div class=meta><div class=info><span class="field tags"><i class=ri-stack-line></i>
<a href=https://zhurongshuo.com/tags/%E4%B9%A6%E7%A8%BF/>书稿</a></span></div></div></div></div><div class=doc_comments></div></div></div></div><a id=back_to_top href=# class=back_to_top><i class=ri-arrow-up-s-line></i></a><footer class=footer><div class=powered_by><a href=https://varkai.com>Designed by VarKai, </a><a href=http://www.gohugo.io/>Proudly published with Hugo,</a></div><div class=footer_slogan><span>法不净空，觉无性也。</span></div><div class=powered_by style=margin-top:10px;font-size:14px><a href=https://zhurongshuo.com/>Copyright © 2010-2025 祝融说 zhurongshuo.com All Rights Reserved.</a></div></footer><script defer src=https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><link href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.css rel=stylesheet integrity="sha256-7qiTu3a8qjjWtcX9w+f2ulVUZSUdCZFEK62eRlmLmCE=" crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin=anonymous></script><script defer src=https://zhurongshuo.com/js/zozo.js></script><script type=text/javascript async src="https://cdn.jsdelivr.net/npm/mathjax@2.7.8/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["[[","]]"]],processEscapes:!0,processEnvironments:!0,skipTags:["script","noscript","style","textarea","pre"],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}}),MathJax.Hub.Queue(function(){var e,t=MathJax.Hub.getAllJax();for(e=0;e<t.length;e+=1)t[e].SourceElement().parentNode.className+=" has-jax"})</script><style>code.has-jax{font:inherit;font-size:100%;background:inherit;border:inherit;color:#515151}</style><script>window.GA_MEASUREMENT_ID="G-KKJ5ZEG1NB",window.GA_CONFIG={enableReadingTime:!0,enableScrollDepth:!0,enableOutboundLinks:!0,enableDownloads:!0,lazyLoadTimeout:3e3}</script><script defer src=https://zhurongshuo.com/js/ga-optimizer.js></script></body></html>