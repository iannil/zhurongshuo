<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><meta name=google-site-verification content="8_xpI-TS3tNV8UPug-Q6Ef3BhKTcy0WOG7dEdAcm2zk"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="祝融"><link rel=dns-prefetch href=//cdn.jsdelivr.net><link rel=preconnect href=https://cdn.jsdelivr.net crossorigin><link rel=dns-prefetch href=//www.googletagmanager.com><link rel=preconnect href=https://www.googletagmanager.com crossorigin><link rel=canonical href=https://zhurongshuo.com/practices/season-4/ai-engineer-in-action/part-02/chapter-06/><title>祝融说。 第六章：自然语言处理的核心：从词嵌入到Transformer</title><meta property="og:title" content="第六章：自然语言处理的核心：从词嵌入到Transformer"><meta property="og:url" content="https://zhurongshuo.com/practices/season-4/ai-engineer-in-action/part-02/chapter-06/"><meta property="og:site_name" content="祝融说。"><meta property="og:locale" content="zh_CN"><meta property="og:type" content="article"><meta property="article:published_time" content="2025-12-09T00:00:00+08:00"><meta property="article:modified_time" content="2025-12-09T00:00:00+08:00"><meta property="article:tag" content="书稿"><meta name=description content="在上一章中，我们已经掌握了深度学习的基本原理和PyTorch的实战技巧，并成功构建了一个基于LSTM的情感分类器。我们已经能够让机器“处理”语言，但我们离真正“理解”语言还有多远？
人类语言，是思想的载体，其复杂、微妙与歧义性，是计算机科学领域最艰巨的挑战之一。一个词语的意义，往往取决于其上下文；一个句子的情感，可能隐藏在精巧的语法结构和微妙的语序之中。要让机器理解语言，首先必须解决一个根本问题：如何将离散、符号化的文本，转化为机器能够计算和学习的数学表示？
"><meta property="og:description" content="在上一章中，我们已经掌握了深度学习的基本原理和PyTorch的实战技巧，并成功构建了一个基于LSTM的情感分类器。我们已经能够让机器“处理”语言，但我们离真正“理解”语言还有多远？
人类语言，是思想的载体，其复杂、微妙与歧义性，是计算机科学领域最艰巨的挑战之一。一个词语的意义，往往取决于其上下文；一个句子的情感，可能隐藏在精巧的语法结构和微妙的语序之中。要让机器理解语言，首先必须解决一个根本问题：如何将离散、符号化的文本，转化为机器能够计算和学习的数学表示？
"><meta property="og:image" content="https://zhurongshuo.com/images/favicon.ico"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="第六章：自然语言处理的核心：从词嵌入到Transformer"><meta name=twitter:description content="在上一章中，我们已经掌握了深度学习的基本原理和PyTorch的实战技巧，并成功构建了一个基于LSTM的情感分类器。我们已经能够让机器“处理”语言，但我们离真正“理解”语言还有多远？
人类语言，是思想的载体，其复杂、微妙与歧义性，是计算机科学领域最艰巨的挑战之一。一个词语的意义，往往取决于其上下文；一个句子的情感，可能隐藏在精巧的语法结构和微妙的语序之中。要让机器理解语言，首先必须解决一个根本问题：如何将离散、符号化的文本，转化为机器能够计算和学习的数学表示？
"><meta name=twitter:image content="https://zhurongshuo.com/images/favicon.ico"><meta name=keywords content="AI工程师实战：从Python基础到LLM应用与性能优化,第六章：自然语言处理的核心：从词嵌入到Transformer"><link rel="shortcut icon" href=https://zhurongshuo.com/images/favicon.ico><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/normalize.css><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/animate-custom.css><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/zozo.css><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/remixicon-custom.css><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/highlight.css><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","headline":"第六章：自然语言处理的核心：从词嵌入到Transformer","description":"在上一章中，我们已经掌握了深度学习的基本原理和PyTorch的实战技巧，并成功构建了一个基于LSTM的情感分类器。我们已经能够让机器“处理”语言，但我们离真正“理解”语言还有多远？\n人类语言，是思想的载体，其复杂、微妙与歧义性，是计算机科学领域最艰巨的挑战之一。一个词语的意义，往往取决于其上下文；一个句子的情感，可能隐藏在精巧的语法结构和微妙的语序之中。要让机器理解语言，首先必须解决一个根本问题：如何将离散、符号化的文本，转化为机器能够计算和学习的数学表示？\n","datePublished":"2025-12-09T00:00:00\u002b08:00","dateModified":"2025-12-09T00:00:00\u002b08:00","author":{"@type":"Person","name":"祝融"},"publisher":{"@type":"Organization","name":"祝融说。","logo":{"@type":"ImageObject","url":"https:\/\/zhurongshuo.com\/images\/favicon.ico"}},"image":"https:\/\/zhurongshuo.com\/images\/favicon.ico","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/zhurongshuo.com\/practices\/season-4\/ai-engineer-in-action\/part-02\/chapter-06\/"},"keywords":"书稿"}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"首页","item":"https:\/\/zhurongshuo.com\/"},{"@type":"ListItem","position":2,"name":"practices","item":"https:\/\/zhurongshuo.com\/practices/"},{"@type":"ListItem","position":3,"name":"第六章：自然语言处理的核心：从词嵌入到Transformer","item":"https:\/\/zhurongshuo.com\/practices\/season-4\/ai-engineer-in-action\/part-02\/chapter-06\/"}]}</script></head><body><div class="main animate__animated animate__fadeInDown"><div class="nav_container animated fadeInDown"><div class=site_nav id=site_nav><ul><li><a href=https://zhurongshuo.com/>首页</a></li><li><a href=https://zhurongshuo.com/start/>开始</a></li><li><a href=https://zhurongshuo.com/advanced/>进阶rc</a></li><li><a href=https://zhurongshuo.com/posts/>归档</a></li><li><a href=https://zhurongshuo.com/tags/>标签</a></li><li><a href=https://zhurongshuo.com/about/>关于</a></li></ul></div><div class=menu_icon><a id=menu_icon><i class=ri-menu-line></i></a></div></div><div class="header animated fadeInDown"><div class=site_title_container><div class=site_title><h1><a href=https://zhurongshuo.com/><span class=web-font>祝融说。</span></a></h1></div><div class=description><p class=sub_title>法不净空，觉无性也。</p><div class=my_socials><a href=https://zhurongshuo.com/books/ title=book-open><i class=ri-book-open-line></i></a>
<a href=https://zhurongshuo.com/practices/ title=trophy><i class=ri-trophy-line></i></a>
<a href=https://zhurongshuo.com/gallery/ title=gallery><i class=ri-gallery-line></i></a>
<a href=https://zhurongshuo.com/about/ title=game><i class=ri-game-line></i></a>
<a href type=application/rss+xml title=rss target=_blank><i class=ri-rss-fill></i></a></div></div></div></div><div class=content><div class=post_page><div class="post animate__animated animate__fadeInDown"><div class="post_title post_detail_title"><h2><a href=https://zhurongshuo.com/practices/season-4/ai-engineer-in-action/part-02/chapter-06/>第六章：自然语言处理的核心：从词嵌入到Transformer</a></h2><span class=date>2025.12.09</span></div><div class="post_content markdown"><p>在上一章中，我们已经掌握了深度学习的基本原理和PyTorch的实战技巧，并成功构建了一个基于LSTM的情感分类器。我们已经能够让机器“处理”语言，但我们离真正“理解”语言还有多远？</p><p>人类语言，是思想的载体，其复杂、微妙与歧义性，是计算机科学领域最艰巨的挑战之一。一个词语的意义，往往取决于其上下文；一个句子的情感，可能隐藏在精巧的语法结构和微妙的语序之中。要让机器理解语言，首先必须解决一个根本问题：如何将离散、符号化的文本，转化为机器能够计算和学习的数学表示？</p><p>本章，我们将踏上一段解码语言奥秘的旅程。这段旅程，将直接通往我们最终的目标——大语言模型（LLM）的殿堂。因为LLM之所以强大，正是因为它建立在一系列革命性的文本表示和架构创新之上。</p><p>我们将从这里开始：</p><ul><li>文本表示法：我们将回顾文本表示的演进之路，从简单却稀疏的One-Hot编码，到能够捕捉词汇语义关系的词嵌入（Word Embedding）技术，如Word2Vec。你将理解，为什么“国王 - 男性 + 女性 ≈ 女王”这个著名的类比，成为了现代NLP的起点。</li><li>注意力机制（Attention）：这是深度学习领域最具影响力的思想之一。它模仿了人类视觉的注意力机制，允许模型在处理信息时，动态地将“焦点”放在输入序列中最相关的部分。我们将揭示其工作原理，并理解它是如何解决LSTM等传统序列模型的瓶颈的。</li><li>Transformer架构全解析：2017年，一篇名为《Attention Is All You Need》的论文横空出世，提出了完全基于注意力机制的Transformer模型。它抛弃了RNN的循环结构，实现了真正意义上的并行计算，极大地提升了训练效率和模型性能。Transformer不仅统一了NLP领域，其影响力更辐射至CV、语音等多个领域，是所有现代大语言模型（从BERT到GPT系列）的共同基石。我们将以前所未有的深度，逐层剖析其内部的每一个组件。</li><li>Hugging Face生态入门：理论的最终目的是为了应用。我们将介绍当今NLP领域事实上的“标准”——Hugging Face生态系统。你将学会使用其核心的<code>transformers</code>库，轻松地加载和使用数以万计的预训练模型，站在巨人的肩膀上解决实际问题。</li></ul><p>最后，为了让你对Transformer的理解达到“知其然，更知其所以然”的境界，我们将通过一个极具挑战性的实战项目——从零开始，使用PyTorch构建一个简化版的Transformer模型。你将亲手实现Self-Attention、多头注意力、位置编码等核心组件，这个过程将彻底巩固你对Transformer架构的理解，其价值无可估量。</p><p>本章是本书技术深度的高峰。掌握了Transformer，你就掌握了理解和应用所有现代LLM的“万能钥匙”。现在，让我们集中全部的注意力，开始这场NLP领域最核心、最深刻的探索之旅。</p><h2 id=61-文本表示法从one-hot到word2vec>6.1 文本表示法：从One-Hot到Word2Vec</h2><h3 id=611-离散表示法one-hot编码>6.1.1 离散表示法：One-Hot编码</h3><p>要让计算机处理文本，第一步就是将词语数值化。最直观的方法是构建一个词汇表（Vocabulary），然后为每个词语分配一个唯一的ID。</p><p>假设我们的词汇表是：<code>{"我": 0, "爱": 1, "北京": 2, "天安门": 3}</code>。</p><p>One-Hot编码将每个词语表示为一个非常长的向量，这个向量的维度等于词汇表的大小。向量中，只有在对应词语ID的位置上为1，其余所有位置都为0。</p><ul><li><code>我</code> -> <code>[1, 0, 0, 0]</code></li><li><code>爱</code> -> <code>[0, 1, 0, 0]</code></li><li><code>北京</code> -> <code>[0, 0, 1, 0]</code></li></ul><p>One-Hot编码的致命缺陷：</p><ol><li>维度灾难（Curse of Dimensionality）：真实世界的词汇表非常巨大（几十万甚至上百万），导致每个词的向量维度极高，且极其稀疏（绝大部分都是0），这在计算和存储上都是巨大的浪费。</li><li>语义鸿沟：One-Hot向量是相互正交的。这意味着，从数学上看，任意两个词之间的距离都是一样的。<code>dist("北京", "天安门")</code> 和 <code>dist("北京", "爱")</code> 没有任何区别。它完全无法表达“北京”和“天安门”在语义上更接近这一事实。</li></ol><p>我们需要一种更高级的表示法，它应该是低维、稠密的，并且能够蕴含语义信息。</p><h3 id=612-分布式表示法词嵌入word-embedding>6.1.2 分布式表示法：词嵌入（Word Embedding）</h3><p>核心思想（分布式假设）：一个词的意义，由它周围的词来定义（"You shall know a word by the company it keeps"）。例如，经常出现在“银行”、“存款”、“利率”周围的词，很可能与“金融”相关。</p><p>词嵌入不再使用稀疏的高维向量，而是将每个词映射到一个低维（例如100、300维）、稠密、连续的向量空间中。在这个空间里：</p><p>语义上相近的词，其对应的向量在空间中的距离也更近。</p><p>向量之间的方向关系可以表示词与词之间的类比关系。这就是著名的 <code>vector("国王") - vector("男性") + vector("女性") ≈ vector("女王")</code> 的来源。</p><h3 id=613-word2vec训练你自己的词向量>6.1.3 Word2Vec：训练你自己的词向量</h3><p>Word2Vec是Google在2013年提出的一个里程碑式的工作，它提供了一种高效训练词嵌入的方法。Word2Vec包含两种模型架构：</p><ol><li>CBOW (Continuous Bag-of-Words)：根据上下文词来预测中心词。例如，对于句子“我 爱 北京 天安门”，当中心词是“北京”时，上下文词是“我”、“爱”、“天安门”。CBOW模型会接收上下文词的词向量，并将它们（例如通过求平均）合并起来，去预测中心词“北京”。</li><li>Skip-gram：根据中心词来预测上下文词。还是上面的例子，Skip-gram模型会接收中心词“北京”的词向量，然后去分别预测它周围的词“我”、“爱”、“天安门”。Skip-gram在处理稀有词（低频词）时效果更好，但训练速度比CBOW慢。</li></ol><p>训练过程（简述）：</p><p>Word2Vec的巧妙之处在于，它将无监督的词向量学习问题，转化为了一个有监督的“伪任务”（predicting surrounding words）。</p><ol><li>初始化一个巨大的词向量矩阵 <code>E</code>，其中每一行都是一个词的随机初始化的词向量。</li><li>通过一个巨大的语料库，创建大量的（中心词，上下文词）训练样本。</li><li>使用一个简单的神经网络（通常只有一个隐藏层），根据Skip-gram或CBOW任务进行训练。</li><li>在训练过程中，我们真正关心的不是这个伪任务的预测结果，而是作为副产品被不断优化的词向量矩阵 <code>E</code>。</li><li>训练结束后，这个矩阵 <code>E</code> 就是我们得到的词嵌入。</li></ol><p>在PyTorch中，词嵌入通常通过一个<code>nn.Embedding</code>层来实现。这个层本质上就是一个可学习的查找表（Lookup Table）。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 假设词汇表大小为10000，我们想将每个词映射为300维的向量</span>
</span></span><span class=line><span class=cl><span class=n>vocab_size</span> <span class=o>=</span> <span class=mi>10000</span>
</span></span><span class=line><span class=cl><span class=n>embedding_dim</span> <span class=o>=</span> <span class=mi>300</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>embedding_layer</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>vocab_size</span><span class=p>,</span> <span class=n>embedding_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 输入是词的ID序列，形状为 (batch_size, sequence_length)</span>
</span></span><span class=line><span class=cl><span class=n>input_ids</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>LongTensor</span><span class=p>([[</span><span class=mi>10</span><span class=p>,</span> <span class=mi>25</span><span class=p>,</span> <span class=mi>5</span><span class=p>,</span> <span class=mi>2</span><span class=p>],</span> <span class=p>[</span><span class=mi>100</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>]])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 输出是对应的词向量序列，形状为 (batch_size, sequence_length, embedding_dim)</span>
</span></span><span class=line><span class=cl><span class=n>embedded_vectors</span> <span class=o>=</span> <span class=n>embedding_layer</span><span class=p>(</span><span class=n>input_ids</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>embedded_vectors</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span> <span class=c1># torch.Size([2, 4, 300])</span>
</span></span></code></pre></div><p>在神经网络的训练过程中，这个<code>embedding_layer</code>的权重（即词向量矩阵）会随着反向传播一起被优化，从而学习到适合当前任务的词表示。我们也可以加载像Word2Vec或GloVe这样预训练好的词向量来初始化这个层，这称为迁移学习。</p><h2 id=62-注意力机制attention的革命>6.2 注意力机制（Attention）的革命</h2><h3 id=621-encoder-decoder架构的瓶颈>6.2.1 Encoder-Decoder架构的瓶颈</h3><p>在注意力机制出现之前，处理序列到序列（Seq2Seq）任务（如机器翻译）的主流架构是基于RNN的Encoder-Decoder模型。</p><p>Encoder（编码器）：一个RNN（如LSTM），负责读取源语言句子（例如，一个英文句子），并将整个句子的信息压缩成一个固定长度的向量，称为上下文向量（Context Vector） <code>C</code>。这个向量 <code>C</code> 就是Encoder最后一个时间步的隐藏状态。</p><p>Decoder（解码器）：另一个RNN，它接收上下文向量 <code>C</code> 作为其初始隐藏状态，然后一个词一个词地生成目标语言句子（例如，法文句子）。</p><p>瓶颈在哪里？</p><p>整个源句子的所有信息，无论长短，都必须被硬生生地压缩进一个固定长度的上下文向量 <code>C</code> 中。这就像让你用一句话总结一部长篇小说，信息的损失是巨大的。对于长句子，模型很难记住开头的细节。这个固定长度的向量 <code>C</code> 成为了整个模型的性能瓶颈。</p><h3 id=622-注意力机制的诞生>6.2.2 注意力机制的诞生</h3><p>Bahdanau等人在2014年提出的注意力机制，正是为了打破这个瓶颈。其核心思想是：在Decoder生成每个词时，不应该只依赖于一个固定的上下文向量，而应该允许Decoder“回头看”Encoder的所有隐藏状态，并动态地决定在当前时间步，源句子的哪个部分最值得“关注”。</p><p>工作流程（以机器翻译为例）：</p><p>假设Decoder正在准备生成第 <code>t</code> 个目标词。</p><ol><li><p>计算对齐分数（Alignment Score）：Decoder当前的隐藏状态 <code>s_{t-1}</code> 会与Encoder的每一个隐藏状态 <code>h_1, h_2, ..., h_n</code> 进行比较，计算一个“对齐分数”或“相关性分数” <code>e_tj = score(s_{t-1}, h_j)</code>。这个<code>score</code>函数可以是一个简单的前馈网络。这个分数衡量了要生成的目标词与源句子中第 <code>j</code> 个词的关联程度。</p></li><li><p>计算注意力权重（Attention Weights）：将所有对齐分数 <code>e_t1, e_t2, ..., e_tn</code> 通过一个<code>Softmax</code>函数进行归一化，得到一组注意力权重 <code>α_t1, α_t2, ..., α_tn</code>。这组权重的和为1，可以看作是一个概率分布，表示在当前时间步，注意力应该如何分配到源句子的各个词上。</p></li><li><p>计算上下文向量（Context Vector）：将注意力权重 <code>α_tj</code> 作为加权系数，对Encoder的所有隐藏状态 <code>h_j</code> 进行加权求和，得到一个为当前时间步 <code>t</code> 量身定制的上下文向量 <code>C_t = Σ α_tj * h_j</code>。如果某个源词的注意力权重高，那么它的信息在 <code>C_t</code> 中所占的比重就大。</p></li><li><p>生成目标词：将这个动态的上下文向量 <code>C_t</code> 与Decoder上一步的输出、以及当前的隐藏状态 <code>s_{t-1}</code> 结合起来，共同预测当前的目标词 <code>y_t</code>。</p></li></ol><p>革命性意义：</p><ul><li>打破信息瓶颈：不再依赖于单一的固定长度向量，而是为每个解码步骤动态生成上下文向量。</li><li>可解释性：通过可视化注意力权重矩阵，我们可以直观地看到在生成某个目标词时，模型主要“看”了源句子的哪些部分，这为我们理解和调试模型提供了窗口。</li><li>解决了长距离依赖：由于可以直接关注到源序列的任意位置，模型处理长距离依赖的能力大大增强。</li></ul><p>注意力机制的思想是如此强大和通用，它很快就不再局限于Encoder-Decoder架构，而是演变成了一种更普适的机制，并最终催生了Transformer。</p><h2 id=63-transformer架构全解析从encoder-decoder到self-attention>6.3 Transformer架构全解析：从Encoder-Decoder到Self-Attention</h2><p>2017年，Google的论文《Attention Is All You Need》提出了Transformer模型，彻底颠覆了NLP领域。它的核心论点是：我们不再需要RNN的循环结构来处理序列，仅仅依靠注意力机制就足够了。</p><h3 id=631-整体架构基于attention的encoder-decoder>6.3.1 整体架构：基于Attention的Encoder-Decoder</h3><p>Transformer的宏观结构仍然是一个Encoder-Decoder模型，但其内部的实现被完全重构了。</p><p>Encoder：由N个相同的Encoder Layer堆叠而成。负责将输入的ID序列（例如，英文句子）转换为一系列上下文感知的词表示。</p><p>Decoder：由N个相同的Decoder Layer堆叠而成。负责接收Encoder的输出和已经生成的目标序列，来预测下一个目标词。</p><p>Transformer 整体架构图 (图片来源: The Annotated Transformer)</p><h3 id=632-核心组件之一自注意力机制self-attention>6.3.2 核心组件之一：自注意力机制（Self-Attention）</h3><p>这是Transformer的灵魂。传统的注意力机制用于连接Encoder和Decoder，而自注意力（Self-Attention）则是在同一个序列内部计算注意力。它的目的是让序列中的每个词，都能“看到”并衡量序列中所有其他词对自己的重要性，从而捕捉句子内部的依赖关系（如语法结构、指代关系等）。</p><p>Query, Key, Value (Q, K, V) 的抽象</p><p>为了实现Self-Attention，Transformer为输入序列中的每个词向量，都创建了三个新的向量：</p><ul><li>Query (查询向量) <code>q</code>：代表当前词，它要去“查询”其他词。</li><li>Key (键向量) <code>k</code>：代表被查询的词，它像一个“标签”，用来和Query进行匹配。</li><li>Value (值向量) <code>v</code>：代表被查询的词的实际内容。</li></ul><p>这三个向量都是通过将原始词向量乘以三个可学习的权重矩阵 <code>W_Q</code>, <code>W_K</code>, <code>W_V</code> 得到的。</p><p>计算过程：</p><p>假设我们要计算句子 "Thinking Machines" 中 "Thinking" 这个词的Self-Attention输出。</p><ol><li>计算分数：将 "Thinking" 的Query向量 <code>q1</code>，与句子中所有词（包括它自己）的Key向量 <code>k1, k2</code> 进行点积（Dot-Product），得到分数。<code>score1 = q1 · k1</code>, <code>score2 = q1 · k2</code>。这个分数衡量了其他词对于理解 "Thinking" 的重要性。</li><li>缩放（Scale）：将分数除以一个缩放因子，通常是Key向量维度的平方根 <code>sqrt(d_k)</code>。这可以防止在维度很高时点积结果过大，导致Softmax进入梯度很小的区域。</li><li>Softmax：将缩放后的分数通过Softmax函数，得到注意力权重。</li><li>加权求和：将得到的注意力权重，与所有词的Value向量 <code>v1, v2</code> 进行加权求和，得到最终的输出向量 <code>z1</code>。这个 <code>z1</code> 就是 "Thinking" 这个词经过Self-Attention之后，融合了整个句子上下文信息的新表示。</li></ol><p>这个过程对句子中的每个词都并行地进行，最终得到一系列上下文感知的输出向量。由于整个过程只涉及矩阵乘法，因此可以高度并行化，这是它相比RNN的巨大优势。</p><h3 id=633-核心组件之二多头注意力multi-head-attention>6.3.3 核心组件之二：多头注意力（Multi-Head Attention）</h3><p>一次Self-Attention只让一个词从一个“角度”或“子空间”去关注其他词。但一个词的依赖关系可能是多方面的（例如，既有语法上的主谓关系，又有语义上的指代关系）。</p><p>多头注意力机制通过并行地运行多个独立的Self-Attention“头”（Head）来解决这个问题。</p><ol><li>将原始的Q, K, V通过多组不同的权重矩阵 <code>W_Q^i, W_K^i, W_V^i</code>，投影到多个不同的、低维的表示子空间中。</li><li>在每个子空间中，独立地执行Self-Attention计算，得到一个输出向量 <code>z_i</code>。</li><li>将所有头的输出向量 <code>z_1, z_2, ..., z_h</code> 拼接（Concatenate）起来。</li><li>将拼接后的向量通过一个额外的线性层 <code>W_O</code> 进行变换，得到最终的输出。</li></ol><p>这使得模型能够同时关注来自不同表示子空间的信息，从而更全面地捕捉复杂的依赖关系。</p><h3 id=634-核心组件之三位置编码positional-encoding>6.3.4 核心组件之三：位置编码（Positional Encoding）</h3><p>Self-Attention本身并不包含任何关于词序的信息。如果我们打乱一个句子的顺序，Self-Attention的输出是完全一样的。这显然是不行的，因为语序在语言中至关重要。</p><p>位置编码就是为了向模型注入位置信息。Transformer的作者没有使用可学习的位置嵌入，而是采用了一种巧妙的、固定的数学方法：</p><p>为每个位置 <code>pos</code> 和向量的每个维度 <code>i</code>，使用不同频率的<code>sin</code>和<code>cos</code>函数来生成一个位置编码向量 <code>PE</code>。</p><ul><li><code>PE(pos, 2i) = sin(pos / 10000^(2i/d_model))</code></li><li><code>PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))</code></li></ul><p>将这个位置编码向量，直接加到原始的词嵌入向量上。</p><p>这种方法的优点是：</p><p>它可以推广到比训练时遇到的更长的序列。</p><p>由于<code>sin</code>和<code>cos</code>的周期性，模型可以很容易地学习到相对位置关系。</p><h3 id=635-串联一切encoder和decoder-layer的内部结构>6.3.5 串联一切：Encoder和Decoder Layer的内部结构</h3><p>Encoder Layer：</p><ol><li>一个多头自注意力层（Multi-Head Self-Attention）。</li><li>一个残差连接（Residual Connection）和层归一化（Layer Normalization）。即 <code>LayerNorm(x + Sublayer(x))</code>。残差连接有助于缓解梯度消失，让深层网络更容易训练。</li><li>一个简单的前馈神经网络（Feed-Forward Network），通常由两个线性层和一个ReLU激活函数组成。</li><li>另一个残差连接和层归一化。</li></ol><p>Decoder Layer：比Encoder Layer多了一个组件。</p><ol><li>一个带掩码的多头自注意力层（Masked Multi-Head Self-Attention）。在解码时，为了防止模型“偷看”未来的词，需要将当前位置之后的所有词的注意力权重设置为0。这个“掩码”操作确保了模型的自回归（auto-regressive）特性。</li><li>残差连接和层归一化。</li><li>一个多头注意力层，它的Q来自上一个Decoder层的输出，而K和V则来自Encoder的最终输出。这是连接Encoder和Decoder的桥梁，与传统的Attention机制作用相同。</li><li>残差连接和层归一化。</li><li>一个前馈神经网络。</li><li>残差连接和层归一化。</li></ol><p>最后，Decoder的输出会经过一个线性层和Softmax，来预测词汇表中每个词的概率。</p><h2 id=64-hugging-face生态入门transformers库的妙用>6.4 Hugging Face生态入门：<code>transformers</code>库的妙用</h2><p>手动实现和训练一个Transformer模型非常复杂。幸运的是，Hugging Face公司为我们提供了一套无与伦比的开源工具，使得使用最先进的NLP模型变得前所未有的简单。</p><p>Hugging Face生态的核心：</p><p>Model Hub：一个巨大的模型仓库，托管了数以万计的、由社区和企业贡献的预训练模型（如BERT, GPT-2, T5等），涵盖了100多种语言和各种任务。</p><p><code>transformers</code>库：一个Python库，提供了加载、训练和使用Model Hub中所有模型的统一API。</p><p><code>datasets</code>库：提供了对数千个常用数据集的便捷访问和处理工具。</p><p><code>tokenizers</code>库：提供了高效、可定制的文本分词器。</p><h3 id=641-pipeline最简单的入门方式>6.4.1 <code>pipeline</code>：最简单的入门方式</h3><p><code>pipeline</code>是<code>transformers</code>库中最高级的抽象，能让你用几行代码就完成一个端到端的NLP任务。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>pipeline</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 情感分析</span>
</span></span><span class=line><span class=cl><span class=n>classifier</span> <span class=o>=</span> <span class=n>pipeline</span><span class=p>(</span><span class=s2>&#34;sentiment-analysis&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>result</span> <span class=o>=</span> <span class=n>classifier</span><span class=p>(</span><span class=s2>&#34;I love using Hugging Face, it&#39;s so easy!&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>result</span><span class=p>)</span> <span class=c1># [{&#39;label&#39;: &#39;POSITIVE&#39;, &#39;score&#39;: 0.99...}]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 文本生成 (使用GPT-2)</span>
</span></span><span class=line><span class=cl><span class=n>generator</span> <span class=o>=</span> <span class=n>pipeline</span><span class=p>(</span><span class=s2>&#34;text-generation&#34;</span><span class=p>,</span> <span class=n>model</span><span class=o>=</span><span class=s2>&#34;gpt2&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>text</span> <span class=o>=</span> <span class=n>generator</span><span class=p>(</span><span class=s2>&#34;In a world where AI is becoming more powerful,&#34;</span><span class=p>,</span> <span class=n>max_length</span><span class=o>=</span><span class=mi>30</span><span class=p>,</span> <span class=n>num_return_sequences</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>text</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 填空 (使用BERT)</span>
</span></span><span class=line><span class=cl><span class=n>unmasker</span> <span class=o>=</span> <span class=n>pipeline</span><span class=p>(</span><span class=s2>&#34;fill-mask&#34;</span><span class=p>,</span> <span class=n>model</span><span class=o>=</span><span class=s2>&#34;bert-base-uncased&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>result</span> <span class=o>=</span> <span class=n>unmasker</span><span class=p>(</span><span class=s2>&#34;The capital of France is [MASK].&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>result</span><span class=p>)</span> <span class=c1># [{&#39;token_str&#39;: &#39;paris&#39;, ...}]</span>
</span></span></code></pre></div><h3 id=642-autoclass加载任意模型和分词器>6.4.2 AutoClass：加载任意模型和分词器</h3><p>当你需要更多控制时，可以使用<code>AutoModel</code>和<code>AutoTokenizer</code>。它们可以根据你提供的模型名称（例如<code>"bert-base-uncased"</code>），自动从Model Hub下载并加载对应的模型类和分词器。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>AutoTokenizer</span><span class=p>,</span> <span class=n>AutoModelForSequenceClassification</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 模型名称</span>
</span></span><span class=line><span class=cl><span class=n>model_name</span> <span class=o>=</span> <span class=s2>&#34;distilbert-base-uncased-finetuned-sst-2-english&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 1. 加载分词器</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_name</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 2. 加载模型</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModelForSequenceClassification</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_name</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 准备输入文本</span>
</span></span><span class=line><span class=cl><span class=n>texts</span> <span class=o>=</span> <span class=p>[</span><span class=s2>&#34;This movie was great!&#34;</span><span class=p>,</span> <span class=s2>&#34;This movie was terrible.&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 3. 分词和编码</span>
</span></span><span class=line><span class=cl><span class=c1># padding=True: 填充到批次中最长句子的长度</span>
</span></span><span class=line><span class=cl><span class=c1># truncation=True: 如果句子超过模型最大长度，则截断</span>
</span></span><span class=line><span class=cl><span class=c1># return_tensors=&#34;pt&#34;: 返回PyTorch Tensor</span>
</span></span><span class=line><span class=cl><span class=n>inputs</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>(</span><span class=n>texts</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>truncation</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>return_tensors</span><span class=o>=</span><span class=s2>&#34;pt&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>inputs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 4. 模型推理</span>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>inputs</span><span class=p>)</span> <span class=c1># 使用解包字典作为参数</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 5. 解析输出</span>
</span></span><span class=line><span class=cl><span class=n>logits</span> <span class=o>=</span> <span class=n>outputs</span><span class=o>.</span><span class=n>logits</span>
</span></span><span class=line><span class=cl><span class=n>predictions</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>predictions</span><span class=p>)</span> <span class=c1># tensor([1, 0]) (1: positive, 0: negative)</span>
</span></span></code></pre></div><p>这个“加载Tokenizer -> 加载Model -> 编码文本 -> 模型推理 -> 解析输出”的流程，是使用<code>transformers</code>库解决几乎所有问题的标准范式。</p><h2 id=65-实战项目从零构建一个简化版的transformer模型>6.5 实战项目：从零构建一个简化版的Transformer模型</h2><p>这个项目极具挑战性，但完成后你对Transformer的理解将达到一个全新的高度。我们将实现一个用于机器翻译的、简化版的Encoder-Decoder Transformer。</p><p>我们将实现的关键组件：</p><ul><li>位置编码（Positional Encoding）</li><li>缩放点积注意力（Scaled Dot-Product Attention）</li><li>多头注意力（Multi-Head Attention）</li><li>位置前馈网络（Position-wise Feed-Forward Network）</li><li>Encoder Layer 和 Decoder Layer</li><li>完整的Encoder, Decoder和Transformer模型</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 这是一个高度浓缩和简化的实现，旨在展示核心逻辑</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>math</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># --- 组件 1: 位置编码 ---</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>PositionalEncoding</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=n>max_len</span><span class=o>=</span><span class=mi>5000</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>PositionalEncoding</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>pe</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>max_len</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>position</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>max_len</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float</span><span class=p>)</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>div_term</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>float</span><span class=p>()</span> <span class=o>*</span> <span class=p>(</span><span class=o>-</span><span class=n>math</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=mf>10000.0</span><span class=p>)</span> <span class=o>/</span> <span class=n>d_model</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>pe</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>::</span><span class=mi>2</span><span class=p>]</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>sin</span><span class=p>(</span><span class=n>position</span> <span class=o>*</span> <span class=n>div_term</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>pe</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>::</span><span class=mi>2</span><span class=p>]</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cos</span><span class=p>(</span><span class=n>position</span> <span class=o>*</span> <span class=n>div_term</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>pe</span> <span class=o>=</span> <span class=n>pe</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>register_buffer</span><span class=p>(</span><span class=s1>&#39;pe&#39;</span><span class=p>,</span> <span class=n>pe</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># x: [seq_len, batch_size, d_model]</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>pe</span><span class=p>[:</span><span class=n>x</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>),</span> <span class=p>:]</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># --- 组件 2: 多头注意力 (包含缩放点积注意力) ---</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>MultiHeadAttention</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=n>nhead</span><span class=p>,</span> <span class=n>dropout</span><span class=o>=</span><span class=mf>0.1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>nhead</span> <span class=o>=</span> <span class=n>nhead</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>d_model</span> <span class=o>=</span> <span class=n>d_model</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span> <span class=o>=</span> <span class=n>d_model</span> <span class=o>//</span> <span class=n>nhead</span>
</span></span><span class=line><span class=cl>      
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>q_linear</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>k_linear</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>v_linear</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>out_linear</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>dropout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>      
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>query</span><span class=p>,</span> <span class=n>key</span><span class=p>,</span> <span class=n>value</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># query, key, value: [seq_len, batch_size, d_model]</span>
</span></span><span class=line><span class=cl>        <span class=n>batch_size</span> <span class=o>=</span> <span class=n>query</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>      
</span></span><span class=line><span class=cl>        <span class=c1># 1. 线性投影</span>
</span></span><span class=line><span class=cl>        <span class=n>q</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>q_linear</span><span class=p>(</span><span class=n>query</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>k</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>k_linear</span><span class=p>(</span><span class=n>key</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>v</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>v_linear</span><span class=p>(</span><span class=n>value</span><span class=p>)</span>
</span></span><span class=line><span class=cl>      
</span></span><span class=line><span class=cl>        <span class=c1># 2. 重塑以进行多头计算</span>
</span></span><span class=line><span class=cl>        <span class=c1># [seq_len, batch_size, d_model] -&gt; [seq_len, batch_size * nhead, head_dim] -&gt; [batch_size * nhead, seq_len, head_dim]</span>
</span></span><span class=line><span class=cl>        <span class=n>q</span> <span class=o>=</span> <span class=n>q</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>batch_size</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>nhead</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>k</span> <span class=o>=</span> <span class=n>k</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>batch_size</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>nhead</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>v</span> <span class=o>=</span> <span class=n>v</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>batch_size</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>nhead</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>      
</span></span><span class=line><span class=cl>        <span class=c1># 3. 缩放点积注意力</span>
</span></span><span class=line><span class=cl>        <span class=n>scores</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>bmm</span><span class=p>(</span><span class=n>q</span><span class=p>,</span> <span class=n>k</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>))</span> <span class=o>/</span> <span class=n>math</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>      
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>mask</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>scores</span> <span class=o>=</span> <span class=n>scores</span><span class=o>.</span><span class=n>masked_fill</span><span class=p>(</span><span class=n>mask</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=o>-</span><span class=mf>1e9</span><span class=p>)</span>
</span></span><span class=line><span class=cl>          
</span></span><span class=line><span class=cl>        <span class=n>attention_weights</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>scores</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>attention_weights</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>attention_weights</span><span class=p>)</span>
</span></span><span class=line><span class=cl>      
</span></span><span class=line><span class=cl>        <span class=n>output</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>bmm</span><span class=p>(</span><span class=n>attention_weights</span><span class=p>,</span> <span class=n>v</span><span class=p>)</span>
</span></span><span class=line><span class=cl>      
</span></span><span class=line><span class=cl>        <span class=c1># 4. 重塑和最后线性层</span>
</span></span><span class=line><span class=cl>        <span class=c1># [batch_size * nhead, seq_len, head_dim] -&gt; [seq_len, batch_size, d_model]</span>
</span></span><span class=line><span class=cl>        <span class=n>output</span> <span class=o>=</span> <span class=n>output</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span><span class=o>.</span><span class=n>contiguous</span><span class=p>()</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>batch_size</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>output</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>out_linear</span><span class=p>(</span><span class=n>output</span><span class=p>)</span>
</span></span><span class=line><span class=cl>      
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>output</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># --- 组件 3: Encoder Layer ---</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>TransformerEncoderLayer</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=n>nhead</span><span class=p>,</span> <span class=n>dim_feedforward</span><span class=o>=</span><span class=mi>2048</span><span class=p>,</span> <span class=n>dropout</span><span class=o>=</span><span class=mf>0.1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>self_attn</span> <span class=o>=</span> <span class=n>MultiHeadAttention</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>nhead</span><span class=p>,</span> <span class=n>dropout</span><span class=o>=</span><span class=n>dropout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>linear1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>dim_feedforward</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>dropout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>linear2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>dim_feedforward</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>      
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>norm1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>norm2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>dropout1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>dropout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>dropout2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>dropout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>src</span><span class=p>,</span> <span class=n>src_mask</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># Self-Attention</span>
</span></span><span class=line><span class=cl>        <span class=n>src2</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>self_attn</span><span class=p>(</span><span class=n>src</span><span class=p>,</span> <span class=n>src</span><span class=p>,</span> <span class=n>src</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=n>src_mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># Add &amp; Norm</span>
</span></span><span class=line><span class=cl>        <span class=n>src</span> <span class=o>=</span> <span class=n>src</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout1</span><span class=p>(</span><span class=n>src2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>src</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>norm1</span><span class=p>(</span><span class=n>src</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># Feed Forward</span>
</span></span><span class=line><span class=cl>        <span class=n>src2</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>linear2</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>F</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>linear1</span><span class=p>(</span><span class=n>src</span><span class=p>))))</span>
</span></span><span class=line><span class=cl>        <span class=c1># Add &amp; Norm</span>
</span></span><span class=line><span class=cl>        <span class=n>src</span> <span class=o>=</span> <span class=n>src</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout2</span><span class=p>(</span><span class=n>src2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>src</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>norm2</span><span class=p>(</span><span class=n>src</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>src</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># --- 完整的Transformer模型 (简化版) ---</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>MyTransformer</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>src_vocab_size</span><span class=p>,</span> <span class=n>tgt_vocab_size</span><span class=p>,</span> <span class=n>d_model</span><span class=o>=</span><span class=mi>512</span><span class=p>,</span> <span class=n>nhead</span><span class=o>=</span><span class=mi>8</span><span class=p>,</span> <span class=n>num_encoder_layers</span><span class=o>=</span><span class=mi>6</span><span class=p>,</span> <span class=n>num_decoder_layers</span><span class=o>=</span><span class=mi>6</span><span class=p>,</span> <span class=n>dim_feedforward</span><span class=o>=</span><span class=mi>2048</span><span class=p>,</span> <span class=n>dropout</span><span class=o>=</span><span class=mf>0.1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>src_embedding</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>src_vocab_size</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>tgt_embedding</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>tgt_vocab_size</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>pos_encoder</span> <span class=o>=</span> <span class=n>PositionalEncoding</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>dropout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>      
</span></span><span class=line><span class=cl>        <span class=c1># 使用PyTorch内置的TransformerEncoder和DecoderLayer</span>
</span></span><span class=line><span class=cl>        <span class=n>encoder_layer</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>TransformerEncoderLayer</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>nhead</span><span class=p>,</span> <span class=n>dim_feedforward</span><span class=p>,</span> <span class=n>dropout</span><span class=p>,</span> <span class=n>batch_first</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>transformer_encoder</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>TransformerEncoder</span><span class=p>(</span><span class=n>encoder_layer</span><span class=p>,</span> <span class=n>num_layers</span><span class=o>=</span><span class=n>num_encoder_layers</span><span class=p>)</span>
</span></span><span class=line><span class=cl>      
</span></span><span class=line><span class=cl>        <span class=n>decoder_layer</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>TransformerDecoderLayer</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>nhead</span><span class=p>,</span> <span class=n>dim_feedforward</span><span class=p>,</span> <span class=n>dropout</span><span class=p>,</span> <span class=n>batch_first</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>transformer_decoder</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>TransformerDecoder</span><span class=p>(</span><span class=n>decoder_layer</span><span class=p>,</span> <span class=n>num_layers</span><span class=o>=</span><span class=n>num_decoder_layers</span><span class=p>)</span>
</span></span><span class=line><span class=cl>      
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>fc_out</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>tgt_vocab_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>      
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>src</span><span class=p>,</span> <span class=n>tgt</span><span class=p>,</span> <span class=n>src_padding_mask</span><span class=p>,</span> <span class=n>tgt_padding_mask</span><span class=p>,</span> <span class=n>memory_key_padding_mask</span><span class=p>,</span> <span class=n>tgt_mask</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># src: [src_len, batch_size]</span>
</span></span><span class=line><span class=cl>        <span class=c1># tgt: [tgt_len, batch_size]</span>
</span></span><span class=line><span class=cl>        <span class=n>src_emb</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>pos_encoder</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>src_embedding</span><span class=p>(</span><span class=n>src</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>tgt_emb</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>pos_encoder</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>tgt_embedding</span><span class=p>(</span><span class=n>tgt</span><span class=p>))</span>
</span></span><span class=line><span class=cl>      
</span></span><span class=line><span class=cl>        <span class=n>memory</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>transformer_encoder</span><span class=p>(</span><span class=n>src_emb</span><span class=p>,</span> <span class=n>src_key_padding_mask</span><span class=o>=</span><span class=n>src_padding_mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>output</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>transformer_decoder</span><span class=p>(</span><span class=n>tgt_emb</span><span class=p>,</span> <span class=n>memory</span><span class=p>,</span> <span class=n>tgt_mask</span><span class=o>=</span><span class=n>tgt_mask</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>                                          <span class=n>tgt_key_padding_mask</span><span class=o>=</span><span class=n>tgt_padding_mask</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                          <span class=n>memory_key_padding_mask</span><span class=o>=</span><span class=n>memory_key_padding_mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>      
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc_out</span><span class=p>(</span><span class=n>output</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># ... 后续还需要编写生成掩码的函数、训练循环等 ...</span>
</span></span></code></pre></div><p>这个实战项目非常复杂，需要对PyTorch和Transformer的每个细节都有深入的理解。完成它，将是你技术能力的一次巨大飞跃。</p><h2 id=本章小结>本章小结</h2><p>在本章中，我们完成了一次穿越现代自然语言处理核心地带的深度旅行。</p><p>我们从最基本的文本表示法出发，理解了从One-Hot到词嵌入（Word2Vec）的演进，解决了如何让机器捕捉词汇语义的关键问题。</p><p>接着，我们学习了具有革命性意义的注意力机制，它打破了传统RNN模型的瓶颈，实现了对输入序列相关部分的动态聚焦。</p><p>在此基础上，我们对Transformer架构进行了“庖丁解牛”式的全解析。我们深入了其内部的每一个核心组件：赋予模型上下文理解能力的自注意力，增强模型表示能力的多头注意力，以及解决语序问题的位置编码。我们清晰地看到了这些组件是如何在Encoder和Decoder中协同工作，最终构建起这个强大的模型。</p><p>为了将理论付诸实践，我们入门了Hugging Face生态，学会了使用<code>transformers</code>库来轻松调用最先进的预训练模型，这让我们能够快速地解决实际问题。</p><p>最后，通过从零构建一个简化版Transformer的极具挑战性的实战项目，我们将所有理论知识内化为了深刻的实践能力。</p><p>完成本章后，你已经掌握了通往大语言模型世界的所有前置知识。你不再会对BERT、GPT这些如雷贯耳的名字感到神秘，因为你知道，它们的核心，正是你已经烂熟于心的Transformer架构。在下一章，我们将正式进入LLM的实战领域，学习如何对这些庞大的模型进行微调和应用，真正释放它们的巨大潜能。</p></div><div class=post_footer><div class=meta><div class=info><span class="field tags"><i class=ri-stack-line></i>
<a href=https://zhurongshuo.com/tags/%E4%B9%A6%E7%A8%BF/>书稿</a></span></div></div></div></div><div class=doc_comments></div></div></div></div><a id=back_to_top href=# class=back_to_top><i class=ri-arrow-up-s-line></i></a><footer class=footer><div class=powered_by><a href=https://varkai.com>Designed by VarKai, </a><a href=http://www.gohugo.io/>Proudly published with Hugo,</a></div><div class=footer_slogan><span>法不净空，觉无性也。</span></div><div class=powered_by style=margin-top:10px;font-size:14px><a href=https://zhurongshuo.com/>Copyright © 2010-2025 祝融说 zhurongshuo.com All Rights Reserved.</a></div></footer><script defer src=https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><link href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.css rel=stylesheet integrity="sha256-7qiTu3a8qjjWtcX9w+f2ulVUZSUdCZFEK62eRlmLmCE=" crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin=anonymous></script><script defer src=https://zhurongshuo.com/js/zozo.js></script><script type=text/javascript async src="https://cdn.jsdelivr.net/npm/mathjax@2.7.8/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["[[","]]"]],processEscapes:!0,processEnvironments:!0,skipTags:["script","noscript","style","textarea","pre"],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}}),MathJax.Hub.Queue(function(){var e,t=MathJax.Hub.getAllJax();for(e=0;e<t.length;e+=1)t[e].SourceElement().parentNode.className+=" has-jax"})</script><style>code.has-jax{font:inherit;font-size:100%;background:inherit;border:inherit;color:#515151}</style><script async src="https://www.googletagmanager.com/gtag/js?id=G-KKJ5ZEG1NB"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KKJ5ZEG1NB")</script></body></html>