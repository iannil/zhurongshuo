<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><meta name=google-site-verification content="8_xpI-TS3tNV8UPug-Q6Ef3BhKTcy0WOG7dEdAcm2zk"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="祝融"><link rel=dns-prefetch href=//cdn.jsdelivr.net><link rel=preconnect href=https://cdn.jsdelivr.net crossorigin><link rel=dns-prefetch href=//www.googletagmanager.com><link rel=preconnect href=https://www.googletagmanager.com crossorigin><link rel=canonical href=https://zhurongshuo.com/practices/season-4/ai-engineer-in-action/part-02/chapter-05/><title>祝融说。 第五章：深度学习之门：从神经网络到PyTorch实战</title><meta property="og:title" content="第五章：深度学习之门：从神经网络到PyTorch实战"><meta property="og:url" content="https://zhurongshuo.com/practices/season-4/ai-engineer-in-action/part-02/chapter-05/"><meta property="og:site_name" content="祝融说。"><meta property="og:locale" content="zh_CN"><meta property="og:type" content="article"><meta property="article:published_time" content="2025-12-09T00:00:00+08:00"><meta property="article:modified_time" content="2025-12-09T00:00:00+08:00"><meta property="article:tag" content="书稿"><meta name=description content="欢迎来到本书的第二篇章。在“基础内功篇”中，我们已经为自己锻造了一身坚实的铠甲：精通了Python编程，掌握了数据科学的利器，并具备了将应用工程化部署的能力。现在，装备齐全的我们，即将踏上一段更为激动人心的征程——深入探索驱动现代人工智能革命的核心引擎：深度学习。
"><meta property="og:description" content="欢迎来到本书的第二篇章。在“基础内功篇”中，我们已经为自己锻造了一身坚实的铠甲：精通了Python编程，掌握了数据科学的利器，并具备了将应用工程化部署的能力。现在，装备齐全的我们，即将踏上一段更为激动人心的征程——深入探索驱动现代人工智能革命的核心引擎：深度学习。
"><meta property="og:image" content="https://zhurongshuo.com/images/favicon.ico"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="第五章：深度学习之门：从神经网络到PyTorch实战"><meta name=twitter:description content="欢迎来到本书的第二篇章。在“基础内功篇”中，我们已经为自己锻造了一身坚实的铠甲：精通了Python编程，掌握了数据科学的利器，并具备了将应用工程化部署的能力。现在，装备齐全的我们，即将踏上一段更为激动人心的征程——深入探索驱动现代人工智能革命的核心引擎：深度学习。
"><meta name=twitter:image content="https://zhurongshuo.com/images/favicon.ico"><meta name=keywords content="AI工程师实战：从Python基础到LLM应用与性能优化,第五章：深度学习之门：从神经网络到PyTorch实战"><link rel="shortcut icon" href=https://zhurongshuo.com/images/favicon.ico><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/normalize.css><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/animate-custom.css><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/zozo.css><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/remixicon-custom.css><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/highlight.css><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","headline":"第五章：深度学习之门：从神经网络到PyTorch实战","description":"欢迎来到本书的第二篇章。在“基础内功篇”中，我们已经为自己锻造了一身坚实的铠甲：精通了Python编程，掌握了数据科学的利器，并具备了将应用工程化部署的能力。现在，装备齐全的我们，即将踏上一段更为激动人心的征程——深入探索驱动现代人工智能革命的核心引擎：深度学习。\n","datePublished":"2025-12-09T00:00:00\u002b08:00","dateModified":"2025-12-09T00:00:00\u002b08:00","author":{"@type":"Person","name":"祝融"},"publisher":{"@type":"Organization","name":"祝融说。","logo":{"@type":"ImageObject","url":"https:\/\/zhurongshuo.com\/images\/favicon.ico"}},"image":"https:\/\/zhurongshuo.com\/images\/favicon.ico","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/zhurongshuo.com\/practices\/season-4\/ai-engineer-in-action\/part-02\/chapter-05\/"},"keywords":"书稿"}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"首页","item":"https:\/\/zhurongshuo.com\/"},{"@type":"ListItem","position":2,"name":"practices","item":"https:\/\/zhurongshuo.com\/practices/"},{"@type":"ListItem","position":3,"name":"第五章：深度学习之门：从神经网络到PyTorch实战","item":"https:\/\/zhurongshuo.com\/practices\/season-4\/ai-engineer-in-action\/part-02\/chapter-05\/"}]}</script></head><body><div class="main animate__animated animate__fadeInDown"><div class="nav_container animated fadeInDown"><div class=site_nav id=site_nav><ul><li><a href=https://zhurongshuo.com/>首页</a></li><li><a href=https://zhurongshuo.com/start/>开始</a></li><li><a href=https://zhurongshuo.com/advanced/>进阶rc</a></li><li><a href=https://zhurongshuo.com/posts/>归档</a></li><li><a href=https://zhurongshuo.com/tags/>标签</a></li><li><a href=https://zhurongshuo.com/about/>关于</a></li></ul></div><div class=menu_icon><a id=menu_icon><i class=ri-menu-line></i></a></div></div><div class="header animated fadeInDown"><div class=site_title_container><div class=site_title><h1><a href=https://zhurongshuo.com/><span class=web-font>祝融说。</span></a></h1></div><div class=description><p class=sub_title>法不净空，觉无性也。</p><div class=my_socials><a href=https://zhurongshuo.com/books/ title=book-open><i class=ri-book-open-line></i></a>
<a href=https://zhurongshuo.com/practices/ title=trophy><i class=ri-trophy-line></i></a>
<a href=https://zhurongshuo.com/gallery/ title=gallery><i class=ri-gallery-line></i></a>
<a href=https://zhurongshuo.com/about/ title=game><i class=ri-game-line></i></a>
<a href type=application/rss+xml title=rss target=_blank><i class=ri-rss-fill></i></a></div></div></div></div><div class=content><div class=post_page><div class="post animate__animated animate__fadeInDown"><div class="post_title post_detail_title"><h2><a href=https://zhurongshuo.com/practices/season-4/ai-engineer-in-action/part-02/chapter-05/>第五章：深度学习之门：从神经网络到PyTorch实战</a></h2><span class=date>2025.12.09</span></div><div class="post_content markdown"><p>欢迎来到本书的第二篇章。在“基础内功篇”中，我们已经为自己锻造了一身坚实的铠甲：精通了Python编程，掌握了数据科学的利器，并具备了将应用工程化部署的能力。现在，装备齐全的我们，即将踏上一段更为激动人心的征程——深入探索驱动现代人工智能革命的核心引擎：深度学习。</p><p>如果说Scikit-learn代表的传统机器学习是基于精巧的数学和统计学构建的“手工作坊”，那么深度学习则更像是一座能够自动学习和提炼特征的“现代化工厂”。它通过模拟人脑中神经网络的结构，构建出由成千上万甚至数十亿个“神经元”连接而成的深度神经网络（Deep Neural Networks, DNNs），从而在图像识别、语音识别、自然语言处理等领域取得了前所未有的突破。</p><p>我们今天所惊叹的大语言模型（LLM），正是深度学习发展至今的巅峰之作。要真正理解并驾驭LLM，我们必须回到它的源头，理解其最基本的构成单元和工作原理。本章，就是我们开启这扇“深度学习之门”的钥匙。</p><p>我们将从深度学习的“第一性原理”出发：</p><ul><li>神经网络与反向传播：我们将用最直观的方式，为你揭示神经网络是如何进行信息传递和学习的。你将理解，看似神秘的“学习”过程，其本质是一场基于微积分的、名为“反向传播”的、优雅的“功劳分配”游戏。</li><li>PyTorch框架精讲：理论是枯燥的，而代码是鲜活的。我们将深入学习当今学术界和工业界最受欢迎的深度学习框架——PyTorch。你将掌握其三大核心支柱：<code>Tensor</code>（多维数组）、<code>Autograd</code>（自动求导引擎）和<code>nn.Module</code>（神经网络的构建基石）。</li><li>经典网络结构解析：在LLM的Transformer架构一统天下之前，CNN（卷积神经网络）和RNN（循环神经网络）曾是各自领域的王者。理解它们的设计思想，对于我们理解特征提取、序列建模等核心概念，乃至理解Transformer的演进背景，都至关重要。</li><li>训练的艺术：构建一个网络只是第一步，如何让它“学得好”则是一门艺术。我们将探讨损失函数（告诉模型错在哪）、优化器（指导模型如何改进）和正则化（防止模型“死记硬背”）这三大核心要素。</li></ul><p>最后，我们将通过一个经典的NLP实战项目——使用PyTorch实现一个文本情感分类器——来将本章所有理论和技术点串联起来。我们将从零开始，定义数据集、构建模型、编写训练循环，并最终得到一个能够判断电影评论是正面还是负面的AI模型。</p><p>本章是你从“AI应用者”向“AI构建者”转变的关键一步。它将为你后续学习Transformer架构和驾驭大语言模型打下最坚实、最深刻的理论与实践基础。现在，让我们点燃“计算”的熔炉，开启这场充满挑战与创造的“炼丹”之旅。</p><h2 id=51-神经网络与反向传播的核心思想>5.1 神经网络与反向传播的核心思想</h2><h3 id=511-从生物到数学神经元的抽象>5.1.1 从生物到数学：神经元的抽象</h3><p>深度学习的灵感来源于生物大脑。一个生物神经元接收来自其他神经元的电信号（输入），当这些信号的累积强度超过某个阈值时，它就会被“激活”，并向其他神经元发送信号（输出）。</p><p>数学家和计算机科学家将这个过程抽象成了一个简单的数学模型——人工神经元（或称感知机）：</p><ol><li>输入与权重：神经元接收多个输入值 <code>x1, x2, ..., xn</code>。每个输入都与一个权重（weight） <code>w1, w2, ..., wn</code> 相关联。权重代表了该输入的重要性。</li><li>加权求和：神经元将所有输入与对应的权重相乘，然后求和，并加上一个偏置（bias） <code>b</code>。这个结果 <code>z = (w1*x1 + w2*x2 + ... + wn*xn) + b</code>，在数学上可以简洁地表示为向量点积：<code>z = w · x + b</code>。</li><li>激活函数：将加权和 <code>z</code> 传入一个非线性的激活函数（Activation Function） <code>f</code>，得到最终的输出 <code>a = f(z)</code>。</li></ol><p>为什么需要激活函数？</p><p>激活函数的非线性是整个神经网络能够学习复杂模式的关键。如果没有激活函数（或者说激活函数是线性的），那么无论你将多少层神经元堆叠在一起，整个网络本质上都只是一个简单的线性模型，无法学习像图像识别、语言理解这样复杂的非线性关系。</p><p>常见的激活函数：</p><p>Sigmoid: <code>f(z) = 1 / (1 + e^(-z))</code>。将输入压缩到(0, 1)之间，常用于二分类问题的输出层，表示概率。</p><p>ReLU (Rectified Linear Unit): <code>f(z) = max(0, z)</code>。计算简单，能有效缓解梯度消失问题，是目前最常用的激活函数之一。</p><h3 id=512-神经网络从单个神经元到层层相连>5.1.2 神经网络：从单个神经元到层层相连</h3><p>一个神经元的能力是有限的。但当我们将大量的神经元组织成层（Layer），并将这些层前后连接起来，就构成了神经网络。</p><p>输入层（Input Layer）：接收最原始的数据，例如一张图片的像素值，或一个句子的词向量。</p><p>隐藏层（Hidden Layers）：位于输入层和输出层之间，负责进行大部分的计算和特征提取。一个神经网络可以有零个或多个隐藏层。当隐藏层数量大于等于一时，我们称之为深度神经网络。</p><p>输出层（Output Layer）：产生最终的预测结果。例如，在猫狗分类任务中，输出层可能有两个神经元，分别代表“是猫的概率”和“是狗的概率”。</p><p>信息在网络中从输入层流向输出层的过程，我们称之为前向传播（Forward Propagation）。每一层的输出，都作为下一层的输入。</p><h3 id=513-学习的本质反向传播与梯度下降>5.1.3 “学习”的本质：反向传播与梯度下降</h3><p>我们构建了一个网络，但它的权重 <code>w</code> 和偏置 <code>b</code> 最初都是随机初始化的。这样一个未经训练的网络，对于任何输入，给出的都是随机的、错误的预测。那么，网络是如何“学习”的呢？</p><p>学习的过程，本质上就是一个参数优化的过程。我们希望找到一组最佳的 <code>w</code> 和 <code>b</code>，使得网络对于给定的输入，能产生最接近真实标签的输出。</p><p>这个过程可以分为三步：</p><p>第一步：定义损失（Loss）</p><p>我们需要一个量化的指标来衡量模型的预测结果“有多差”。这个指标就是损失函数（Loss Function）。例如，在分类任务中，常用的损失函数是交叉熵损失（Cross-Entropy Loss）。损失值越大，说明模型预测得越离谱。我们的目标，就是通过调整参数，让损失值变得尽可能小。</p><p>第二步：计算梯度（Gradient）</p><p>损失 <code>L</code> 是一个关于所有权重 <code>w</code> 和偏置 <code>b</code> 的函数。微积分告诉我们，函数在某一点的梯度，指向了该函数值增长最快的方向。那么，梯度的反方向，就是函数值下降最快的方向。</p><p>我们想让损失 <code>L</code> 变小，就需要计算出 <code>L</code> 对每一个参数（如 <code>w_ij</code>，表示第i层第j个神经元的某个权重）的偏导数 <code>∂L/∂w_ij</code>。这个包含了所有偏导数的向量，就是损失函数关于参数的梯度。</p><p>如何高效地计算这个梯度？ 这就是反向传播（Backpropagation）算法的核心所在。它利用微积分中的链式法则，从输出层开始，逐层向后计算梯度。</p><ol><li>首先计算损失对输出层激活值的梯度。</li><li>然后利用这个梯度，计算对输出层加权和的梯度。</li><li>再利用这个梯度，计算对连接到输出层的权重和偏置的梯度，以及对前一层激活值的梯度。</li><li>……如此循环，直到计算出所有参数的梯度。</li></ol><p>反向传播是一个极其巧妙的算法，它避免了大量的重复计算，使得在深层网络中计算梯度成为可能。幸运的是，在现代深度学习框架中，我们无需手动实现它。</p><p>第三步：更新参数（Gradient Descent）</p><p>有了梯度之后，我们就可以更新参数了。最简单的更新规则是梯度下降（Gradient Descent）：</p><p><code>new_w = old_w - learning_rate * ∂L/∂w</code></p><p><code>learning_rate</code>（学习率）是一个超参数，它控制了我们每次更新参数的“步长”。</p><p>我们从梯度的反方向（<code>-</code>号）迈出一步，来更新权重。</p><p>这个“前向传播 -> 计算损失 -> 反向传播 -> 更新参数”的循环，就是神经网络训练的核心。我们会将整个数据集（或一小批数据）反复地送入这个循环中进行迭代，每一次迭代，模型的参数都会向着让损失更小的方向进行微调。经过成千上万次的迭代后，模型就逐渐“学会”了如何进行准确的预测。</p><h2 id=52-pytorch框架精讲tensorautograd与nnmodule>5.2 PyTorch框架精讲：Tensor、Autograd与<code>nn.Module</code></h2><p>PyTorch是由Facebook AI研究院（FAIR）推出的一个开源深度学习框架。它以其Pythonic的设计、灵活性和强大的动态计算图机制，深受学术界和研究人员的喜爱，并逐渐在工业界占据主导地位。</p><h3 id=521-tensor带gpu加速的多维数组>5.2.1 <code>Tensor</code>：带GPU加速的多维数组</h3><p>PyTorch的<code>Tensor</code>，与Numpy的<code>ndarray</code>在概念上极其相似，可以看作是<code>ndarray</code>的“超级加强版”。</p><p>它是一个多维数组，是PyTorch中数据流动的基本单位。</p><p>核心优势：<code>Tensor</code>可以被无缝地移动到GPU上进行计算，从而利用GPU强大的并行计算能力来加速训练。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># --- 创建Tensor ---</span>
</span></span><span class=line><span class=cl><span class=c1># 从列表创建</span>
</span></span><span class=line><span class=cl><span class=n>x_list</span> <span class=o>=</span> <span class=p>[[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>],</span> <span class=p>[</span><span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>]]</span>
</span></span><span class=line><span class=cl><span class=n>x_tensor</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=n>x_list</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>x_tensor</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 类似Numpy的创建方式</span>
</span></span><span class=line><span class=cl><span class=n>x_zeros</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>x_rand</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>rand</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>x_zeros</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>x_rand</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># --- Tensor的属性 ---</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Shape of tensor: </span><span class=si>{</span><span class=n>x_tensor</span><span class=o>.</span><span class=n>shape</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Datatype of tensor: </span><span class=si>{</span><span class=n>x_tensor</span><span class=o>.</span><span class=n>dtype</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Device tensor is stored on: </span><span class=si>{</span><span class=n>x_tensor</span><span class=o>.</span><span class=n>device</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># --- GPU加速 ---</span>
</span></span><span class=line><span class=cl><span class=c1># 检查是否有可用的GPU</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>is_available</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=n>device</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>device</span><span class=p>(</span><span class=s2>&#34;cuda&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;GPU is available! Using device: </span><span class=si>{</span><span class=n>device</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>    <span class=c1># 将Tensor移动到GPU</span>
</span></span><span class=line><span class=cl>    <span class=n>x_gpu</span> <span class=o>=</span> <span class=n>x_tensor</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;x_gpu is on device: </span><span class=si>{</span><span class=n>x_gpu</span><span class=o>.</span><span class=n>device</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>    <span class=c1># 在GPU上进行计算</span>
</span></span><span class=line><span class=cl>    <span class=n>y_gpu</span> <span class=o>=</span> <span class=n>x_gpu</span> <span class=o>+</span> <span class=n>x_gpu</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>    <span class=c1># 将结果移回CPU（例如，用于打印或与Numpy交互）</span>
</span></span><span class=line><span class=cl>    <span class=n>y_cpu</span> <span class=o>=</span> <span class=n>y_gpu</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=s2>&#34;cpu&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>device</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>device</span><span class=p>(</span><span class=s2>&#34;cpu&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;GPU not available, using CPU.&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># --- 与Numpy的互操作 ---</span>
</span></span><span class=line><span class=cl><span class=c1># Tensor -&gt; Numpy</span>
</span></span><span class=line><span class=cl><span class=n>np_array</span> <span class=o>=</span> <span class=n>x_tensor</span><span class=o>.</span><span class=n>numpy</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Numpy array: </span><span class=se>\n</span><span class=s2> </span><span class=si>{</span><span class=n>np_array</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Numpy -&gt; Tensor</span>
</span></span><span class=line><span class=cl><span class=n>np_array_new</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>ones</span><span class=p>((</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>tensor_from_np</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>from_numpy</span><span class=p>(</span><span class=n>np_array_new</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Tensor from numpy: </span><span class=se>\n</span><span class=s2> </span><span class=si>{</span><span class=n>tensor_from_np</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p>这种与Numpy的无缝转换，使得我们可以轻松地在数据预处理（常用Numpy/Pandas）和模型计算（使用PyTorch Tensor）之间切换。</p><h3 id=522-autograd神奇的自动求导引擎>5.2.2 <code>Autograd</code>：神奇的自动求导引擎</h3><p>这是PyTorch最核心、最神奇的功能。它为我们免去了手动实现反向传播的痛苦。<code>Autograd</code>会悄悄地记录下所有在<code>Tensor</code>上的操作，构建一个动态计算图（Dynamic Computational Graph）。</p><p>当一个<code>Tensor</code>的属性<code>.requires_grad</code>被设置为<code>True</code>时，<code>Autograd</code>就会开始跟踪它。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 创建需要计算梯度的Tensor</span>
</span></span><span class=line><span class=cl><span class=n>w</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([[</span><span class=mf>2.0</span><span class=p>],</span> <span class=p>[</span><span class=mf>3.0</span><span class=p>]],</span> <span class=n>requires_grad</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>b</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=mf>1.0</span><span class=p>,</span> <span class=n>requires_grad</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 定义输入</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([[</span><span class=mf>1.0</span><span class=p>,</span> <span class=mf>2.0</span><span class=p>]])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 前向传播</span>
</span></span><span class=line><span class=cl><span class=c1># y = x @ w + b  (@是矩阵乘法的简写)</span>
</span></span><span class=line><span class=cl><span class=n>z</span> <span class=o>=</span> <span class=n>x</span> <span class=o>@</span> <span class=n>w</span> <span class=o>+</span> <span class=n>b</span>
</span></span><span class=line><span class=cl><span class=n>loss</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>z</span><span class=p>)</span> <span class=c1># 假设损失就是z的和</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;z: </span><span class=si>{</span><span class=n>z</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;loss: </span><span class=si>{</span><span class=n>loss</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># --- 反向传播 ---</span>
</span></span><span class=line><span class=cl><span class=c1># 调用 .backward()，Autograd会自动计算loss对所有requires_grad=True的Tensor的梯度</span>
</span></span><span class=line><span class=cl><span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># --- 查看梯度 ---</span>
</span></span><span class=line><span class=cl><span class=c1># 梯度值会累积在 .grad 属性中</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Gradient of w: </span><span class=se>\n</span><span class=s2> </span><span class=si>{</span><span class=n>w</span><span class=o>.</span><span class=n>grad</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Gradient of b: </span><span class=si>{</span><span class=n>b</span><span class=o>.</span><span class=n>grad</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p>发生了什么？</p><ol><li>前向传播 <code>z = x @ w + b</code>，其中 <code>x</code> 是 <code>[[1, 2]]</code>，<code>w</code> 是 <code>[[2], [3]]</code>。
<code>x @ w</code> = <code>1*2 + 2*3</code> = <code>8</code>。
<code>z</code> = <code>8 + 1</code> = <code>9</code>。
<code>loss</code> = <code>9</code>。</li><li><code>loss.backward()</code> 被调用。</li><li><code>Autograd</code>开始反向计算梯度：
<code>∂loss/∂z</code> = <code>1</code>
<code>∂z/∂w</code> = <code>x^T</code> = <code>[[1], [2]]</code> (链式法则)
<code>∂loss/∂w</code> = <code>∂loss/∂z * ∂z/∂w</code> = <code>1 * [[1], [2]]</code> = <code>[[1], [2]]</code>
<code>∂z/∂b</code> = <code>1</code>
<code>∂loss/∂b</code> = <code>∂loss/∂z * ∂z/∂b</code> = <code>1 * 1</code> = <code>1</code></li><li>计算出的梯度与我们打印的 <code>w.grad</code> 和 <code>b.grad</code> 完全一致！</li></ol><p>重要提示：</p><p>梯度是会累积的。在每次进行参数更新前，都需要手动将梯度清零：<code>optimizer.zero_grad()</code>。</p><p>只有浮点类型的Tensor才能计算梯度。</p><p>在模型评估（inference）阶段，我们不需要计算梯度，应该使用<code>with torch.no_grad():</code>上下文管理器来关闭<code>Autograd</code>，这样可以节省内存并加速计算。</p><h3 id=523-nnmodule构建神经网络的乐高积木>5.2.3 <code>nn.Module</code>：构建神经网络的“乐高积木”</h3><p><code>torch.nn</code>是PyTorch专门用于构建神经网络的模块。所有的网络层、损失函数、激活函数都在这里。而<code>nn.Module</code>是所有神经网络模块的基类。</p><p>要构建自己的神经网络，我们通常需要：</p><ol><li>创建一个继承自<code>nn.Module</code>的类。</li><li>在<code>__init__</code>方法中，定义网络所需的各个层（如卷积层、线性层）。这些层本身也是<code>nn.Module</code>的子类。</li><li>在<code>forward</code>方法中，定义数据在前向传播过程中是如何流经这些层的。</li></ol><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn.functional</span> <span class=k>as</span> <span class=nn>F</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>SimpleNet</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>input_size</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>,</span> <span class=n>output_size</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># 必须调用父类的__init__方法</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>SimpleNet</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>      
</span></span><span class=line><span class=cl>        <span class=c1># --- 定义网络层 ---</span>
</span></span><span class=line><span class=cl>        <span class=c1># 线性层 (y = Wx + b)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>fc1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>input_size</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>relu</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>fc2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_size</span><span class=p>,</span> <span class=n>output_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># --- 定义前向传播逻辑 ---</span>
</span></span><span class=line><span class=cl>        <span class=c1># x -&gt; fc1 -&gt; relu -&gt; fc2 -&gt; output</span>
</span></span><span class=line><span class=cl>        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc1</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=n>out</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc2</span><span class=p>(</span><span class=n>out</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>out</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># --- 使用我们定义的网络 ---</span>
</span></span><span class=line><span class=cl><span class=n>input_dim</span> <span class=o>=</span> <span class=mi>784</span> <span class=c1># 例如，一个28x28的展平图像</span>
</span></span><span class=line><span class=cl><span class=n>hidden_dim</span> <span class=o>=</span> <span class=mi>128</span>
</span></span><span class=line><span class=cl><span class=n>output_dim</span> <span class=o>=</span> <span class=mi>10</span> <span class=c1># 例如，0-9的10个类别</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 实例化模型</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>SimpleNet</span><span class=p>(</span><span class=n>input_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>,</span> <span class=n>output_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 我们可以像函数一样调用模型实例，它会自动执行forward方法</span>
</span></span><span class=line><span class=cl><span class=n>dummy_input</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>64</span><span class=p>,</span> <span class=n>input_dim</span><span class=p>)</span> <span class=c1># 模拟一个batch的数据</span>
</span></span><span class=line><span class=cl><span class=n>output</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>dummy_input</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Output shape: </span><span class=si>{</span><span class=n>output</span><span class=o>.</span><span class=n>shape</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span> <span class=c1># torch.Size([64, 10])</span>
</span></span></code></pre></div><p>通过继承<code>nn.Module</code>，我们的<code>SimpleNet</code>类自动获得了许多强大的功能，例如：</p><p><code>model.parameters()</code>：可以方便地获取模型中所有可训练的参数（权重和偏置）。</p><p><code>model.to(device)</code>：可以将整个模型及其所有参数一键移动到GPU。</p><p><code>model.train()</code> / <code>model.eval()</code>：切换训练和评估模式（对于像Dropout和BatchNorm这样的层很重要）。</p><h2 id=53-经典网络结构解析cnn与rnnlstm>5.3 经典网络结构解析：CNN与RNN/LSTM</h2><p>在Transformer出现之前，CNN和RNN是深度学习在计算机视觉（CV）和自然语言处理（NLP）领域取得成功的两大基石。</p><h3 id=531-卷积神经网络cnn图像特征的捕获者>5.3.1 卷积神经网络（CNN）：图像特征的捕获者</h3><p>核心思想：</p><p>传统的全连接网络处理图像时，会将图像展平成一个长向量，这会丢失像素点的空间结构信息。CNN通过卷积（Convolution）和池化（Pooling）操作，专门设计用来处理具有网格结构的数据（如图像）。</p><ol><li><p>卷积层 (<code>nn.Conv2d</code>)：它使用一个小的卷积核（Kernel）或滤波器（Filter）（例如3x3或5x5），在输入图像上滑动。在每个位置，卷积核与其覆盖的图像区域进行逐元素相乘再求和，得到一个输出值。这个过程可以看作是在提取局部特征。例如，一个卷积核可能对图像中的垂直边缘敏感，另一个可能对某种颜色或纹理敏感。</p><p>参数共享：同一个卷积核在整张图上共享同一套权重，这极大地减少了模型的参数量，并使得网络具有平移不变性（无论猫在图的左上角还是右下角，都能被识别）。</p></li><li><p>池化层 (<code>nn.MaxPool2d</code>)：也叫下采样层。它在一个区域内（例如2x2），取最大值（Max Pooling）或平均值（Average Pooling）作为输出。</p><p>作用：减小特征图的尺寸，从而减少后续层的计算量和参数。提供一定程度的平移、旋转不变性，使模型更鲁棒。</p></li></ol><p>一个典型的CNN结构通常是卷积层 -> 激活函数 -> 池化层的重复堆叠，最后接上几个全连接层进行分类。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>SimpleCNN</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>SimpleCNN</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=c1># 输入: 1个通道 (灰度图), 输出: 16个通道, 卷积核: 3x3, padding=1</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>conv1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>16</span><span class=p>,</span> <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>pool</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>MaxPool2d</span><span class=p>(</span><span class=n>kernel_size</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>conv2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>16</span><span class=p>,</span> <span class=mi>32</span><span class=p>,</span> <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># 假设输入是28x28的图像，经过两次pooling后，尺寸变为 7x7</span>
</span></span><span class=line><span class=cl>        <span class=c1># 32个通道 * 7 * 7 = 1568</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>fc1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>32</span> <span class=o>*</span> <span class=mi>7</span> <span class=o>*</span> <span class=mi>7</span><span class=p>,</span> <span class=mi>10</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>pool</span><span class=p>(</span><span class=n>F</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>conv1</span><span class=p>(</span><span class=n>x</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>pool</span><span class=p>(</span><span class=n>F</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>conv2</span><span class=p>(</span><span class=n>x</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>        <span class=c1># 展平特征图</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>32</span> <span class=o>*</span> <span class=mi>7</span> <span class=o>*</span> <span class=mi>7</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc1</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>
</span></span></code></pre></div><h3 id=532-循环神经网络rnnlstm序列数据的记忆者>5.3.2 循环神经网络（RNN/LSTM）：序列数据的记忆者</h3><p>核心思想：</p><p>对于文本、语音、时间序列这类具有先后顺序的数据，CNN和普通全连接网络都无法处理。RNN通过引入一个“记忆”单元——隐藏状态（Hidden State）——来解决这个问题。</p><p>循环结构：</p><p>在处理序列的每个时间步 <code>t</code> 时，RNN不仅接收当前的输入 <code>x_t</code>，还接收来自上一个时间步的隐藏状态 <code>h_{t-1}</code>。</p><p>它将 <code>x_t</code> 和 <code>h_{t-1}</code> 一起计算，生成当前的输出 <code>y_t</code> 和新的隐藏状态 <code>h_t</code>。</p><p>这个新的 <code>h_t</code> 会被传递到下一个时间步 <code>t+1</code>。</p><p>通过这种方式，<code>h_t</code> 就像一个动态的记忆，它编码了从序列开始到当前位置的所有历史信息。</p><p>RNN的困境：长期依赖问题</p><p>标准的RNN在处理长序列时，会遇到梯度消失/爆炸的问题。这意味着在反向传播时，梯度会随着时间步的增加而指数级地减小或增大，导致模型难以学习到序列中相距较远元素之间的依赖关系（例如，一个长段落开头的主语和结尾的谓语之间的关系）。</p><p>LSTM（Long Short-Term Memory）</p><p>LSTM是一种特殊的RNN，它通过引入一个更复杂的内部结构——细胞状态（Cell State）和三个门（Gate）——来解决长期依赖问题。</p><p>细胞状态 <code>C_t</code>：像一条传送带，信息可以在上面直流，只进行少量的线性交互。这使得梯度可以很容易地在长序列中传递。</p><p>遗忘门（Forget Gate）：决定从上一个细胞状态 <code>C_{t-1}</code> 中丢弃哪些信息。</p><p>输入门（Input Gate）：决定将哪些新的信息存入当前的细胞状态 <code>C_t</code>。</p><p>输出门（Output Gate）：决定从当前的细胞状态 <code>C_t</code> 中输出哪些信息作为隐藏状态 <code>h_t</code>。</p><p>这些门都是由Sigmoid激活函数控制的小型神经网络，它们可以学会何时遗忘、何时记忆、何时输出，从而更有效地捕捉长期依赖。在Transformer出现之前，LSTM及其变体GRU是处理NLP任务的绝对主力。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 在PyTorch中使用LSTM</span>
</span></span><span class=line><span class=cl><span class=c1># input_size: 每个时间步输入的特征维度 (如词向量维度)</span>
</span></span><span class=line><span class=cl><span class=c1># hidden_size: 隐藏状态的维度</span>
</span></span><span class=line><span class=cl><span class=n>lstm_layer</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LSTM</span><span class=p>(</span><span class=n>input_size</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span> <span class=n>hidden_size</span><span class=o>=</span><span class=mi>256</span><span class=p>,</span> <span class=n>num_layers</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>batch_first</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 输入的shape: (batch_size, sequence_length, input_size)</span>
</span></span><span class=line><span class=cl><span class=n>dummy_input</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>32</span><span class=p>,</span> <span class=mi>50</span><span class=p>,</span> <span class=mi>100</span><span class=p>)</span> 
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 输出: output, (h_n, c_n)</span>
</span></span><span class=line><span class=cl><span class=c1># output: 每个时间步的隐藏状态 (32, 50, 256)</span>
</span></span><span class=line><span class=cl><span class=c1># h_n: 最后一个时间步的隐藏状态 (num_layers, 32, 256)</span>
</span></span><span class=line><span class=cl><span class=c1># c_n: 最后一个时间步的细胞状态 (num_layers, 32, 256)</span>
</span></span><span class=line><span class=cl><span class=n>output</span><span class=p>,</span> <span class=p>(</span><span class=n>hidden_state</span><span class=p>,</span> <span class=n>cell_state</span><span class=p>)</span> <span class=o>=</span> <span class=n>lstm_layer</span><span class=p>(</span><span class=n>dummy_input</span><span class=p>)</span>
</span></span></code></pre></div><h2 id=54-训练的艺术损失函数优化器与正则化>5.4 训练的艺术：损失函数、优化器与正则化</h2><h3 id=541-损失函数loss-functions指引方向的灯塔>5.4.1 损失函数（Loss Functions）：指引方向的灯塔</h3><p>损失函数告诉我们模型离目标还有多远。选择正确的损失函数至关重要。</p><p>回归任务：</p><p><code>nn.MSELoss</code> (均方误差损失)：<code>L = (y_pred - y_true)^2</code>。最常用的回归损失，对异常值敏感。</p><p><code>nn.L1Loss</code> (平均绝对误差损失)：<code>L = |y_pred - y_true|</code>。对异常值更鲁棒。</p><p>二分类任务：</p><p><code>nn.BCELoss</code> (二元交叉熵损失)：需要模型的输出经过Sigmoid激活，表示概率。</p><p><code>nn.BCEWithLogitsLoss</code>：将Sigmoid和BCELoss合并，数值上更稳定，是二分类的首选。</p><p>多分类任务：</p><p><code>nn.CrossEntropyLoss</code>：这是多分类任务中最常用的损失函数。它内部自动包含了Softmax操作和负对数似然损失。因此，模型的原始输出（logits）可以直接传入，无需手动进行Softmax。</p><h3 id=542-优化器optimizers驱动学习的引擎>5.4.2 优化器（Optimizers）：驱动学习的引擎</h3><p>优化器实现了梯度下降算法，根据计算出的梯度来更新模型的参数。</p><p><code>torch.optim.SGD</code> (随机梯度下降)：最基本的优化器。</p><p><code>optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)</code></p><p><code>momentum</code>（动量）是一个改进项，它引入了过去梯度的累积，有助于加速收敛并越过局部最优点。</p><p><code>torch.optim.Adam</code> (Adaptive Moment Estimation)：目前最常用、最通用的优化器之一。</p><p><code>optimizer = torch.optim.Adam(model.parameters(), lr=0.001)</code></p><p>它结合了动量和RMSProp的思想，能为每个参数自适应地计算学习率。在大多数情况下，Adam都能取得良好且快速的收-敛效果，是入门的首选。</p><p>一个标准的训练循环（Training Loop）</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 假设 model, train_loader, loss_fn, optimizer 已定义</span>
</span></span><span class=line><span class=cl><span class=n>num_epochs</span> <span class=o>=</span> <span class=mi>10</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_epochs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>inputs</span><span class=p>,</span> <span class=n>labels</span> <span class=ow>in</span> <span class=n>train_loader</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=c1># 1. 将数据移动到GPU</span>
</span></span><span class=line><span class=cl>        <span class=n>inputs</span><span class=p>,</span> <span class=n>labels</span> <span class=o>=</span> <span class=n>inputs</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>),</span> <span class=n>labels</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>      
</span></span><span class=line><span class=cl>        <span class=c1># 2. 前向传播</span>
</span></span><span class=line><span class=cl>        <span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>inputs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>      
</span></span><span class=line><span class=cl>        <span class=c1># 3. 计算损失</span>
</span></span><span class=line><span class=cl>        <span class=n>loss</span> <span class=o>=</span> <span class=n>loss_fn</span><span class=p>(</span><span class=n>outputs</span><span class=p>,</span> <span class=n>labels</span><span class=p>)</span>
</span></span><span class=line><span class=cl>      
</span></span><span class=line><span class=cl>        <span class=c1># 4. 反向传播</span>
</span></span><span class=line><span class=cl>        <span class=c1># a. 清空旧梯度</span>
</span></span><span class=line><span class=cl>        <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=c1># b. 计算新梯度</span>
</span></span><span class=line><span class=cl>        <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>      
</span></span><span class=line><span class=cl>        <span class=c1># 5. 更新参数</span>
</span></span><span class=line><span class=cl>        <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span></span><span class=line><span class=cl>      
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Epoch [</span><span class=si>{</span><span class=n>epoch</span><span class=o>+</span><span class=mi>1</span><span class=si>}</span><span class=s2>/</span><span class=si>{</span><span class=n>num_epochs</span><span class=si>}</span><span class=s2>], Loss: </span><span class=si>{</span><span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>()</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></div><h3 id=543-正则化regularization防止死记硬背的良方>5.4.3 正则化（Regularization）：防止“死记硬背”的良方</h3><p>当模型的容量（复杂度）远超训练数据的复杂性时，它可能会“记住”训练集中的每一个样本，包括噪声。这会导致模型在训练集上表现极好，但在未见过的测试集上表现很差。这种现象称为过拟合（Overfitting）。正则化就是一系列用于对抗过拟合的技术。</p><p>L1/L2 正则化：</p><p>在损失函数上增加一个与模型权重大小相关的惩罚项。</p><p>L2正则化（权重衰减, Weight Decay）倾向于让权重变得更小、更分散。在PyTorch的优化器中通过<code>weight_decay</code>参数实现：<code>torch.optim.Adam(..., weight_decay=1e-4)</code>。
Dropout (<code>nn.Dropout</code>)：</p><p>在训练过程中的每次前向传播时，以一定的概率 <code>p</code> 随机地将一部分神经元的输出设置为0。</p><p>这强迫网络不能过度依赖于任何一个神经元，而是要学习到更鲁棒、更冗余的特征表示。</p><p>重要：Dropout只在训练时生效。调用<code>model.eval()</code>会自动关闭Dropout。</p><p>早停（Early Stopping）：</p><p>在训练过程中，持续监控模型在验证集上的性能。</p><p>如果验证集上的损失连续多个epoch不再下降，甚至开始上升，就提前终止训练，并保存性能最好的那个模型。</p><h2 id=55-实战项目使用pytorch实现一个文本情感分类器>5.5 实战项目：使用PyTorch实现一个文本情感分类器</h2><p>现在，我们将运用本章所学的所有知识，从零开始构建一个能够判断IMDB电影评论是正面还是负面的情感分类器。</p><p>项目流程：</p><ol><li>数据准备：加载IMDB数据集，进行文本预处理（分词、构建词汇表、数值化）。</li><li>定义<code>Dataset</code>和<code>DataLoader</code>：使用PyTorch的工具来高效地加载和批处理数据。</li><li>构建模型：我们将构建一个基于LSTM的模型。</li><li>定义训练和评估函数：编写标准的训练循环和评估逻辑。</li><li>执行训练：运行训练过程，并观察损失和准确率的变化。</li></ol><p>执行步骤：</p><ol><li><p>数据准备</p><p>我们将使用<code>torchtext</code>库来方便地处理数据。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>pip install torchtext spacy
</span></span><span class=line><span class=cl>python -m spacy download en_core_web_sm
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 1_data_preparation.py</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torchtext.datasets</span> <span class=kn>import</span> <span class=n>IMDB</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torchtext.data.utils</span> <span class=kn>import</span> <span class=n>get_tokenizer</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torchtext.vocab</span> <span class=kn>import</span> <span class=n>build_vocab_from_iterator</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>spacy</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 加载英文分词器</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>get_tokenizer</span><span class=p>(</span><span class=s1>&#39;spacy&#39;</span><span class=p>,</span> <span class=n>language</span><span class=o>=</span><span class=s1>&#39;en_core_web_sm&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 加载IMDB数据集</span>
</span></span><span class=line><span class=cl><span class=n>train_iter</span><span class=p>,</span> <span class=n>test_iter</span> <span class=o>=</span> <span class=n>IMDB</span><span class=p>(</span><span class=n>split</span><span class=o>=</span><span class=p>(</span><span class=s1>&#39;train&#39;</span><span class=p>,</span> <span class=s1>&#39;test&#39;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>yield_tokens</span><span class=p>(</span><span class=n>data_iter</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>_</span><span class=p>,</span> <span class=n>text</span> <span class=ow>in</span> <span class=n>data_iter</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>yield</span> <span class=n>tokenizer</span><span class=p>(</span><span class=n>text</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 构建词汇表</span>
</span></span><span class=line><span class=cl><span class=n>vocab</span> <span class=o>=</span> <span class=n>build_vocab_from_iterator</span><span class=p>(</span><span class=n>yield_tokens</span><span class=p>(</span><span class=n>train_iter</span><span class=p>),</span> <span class=n>specials</span><span class=o>=</span><span class=p>[</span><span class=s2>&#34;&lt;unk&gt;&#34;</span><span class=p>,</span> <span class=s2>&#34;&lt;pad&gt;&#34;</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>vocab</span><span class=o>.</span><span class=n>set_default_index</span><span class=p>(</span><span class=n>vocab</span><span class=p>[</span><span class=s2>&#34;&lt;unk&gt;&#34;</span><span class=p>])</span> <span class=c1># 设置未知词的默认索引</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 定义文本和标签的处理管道</span>
</span></span><span class=line><span class=cl><span class=n>text_pipeline</span> <span class=o>=</span> <span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=n>vocab</span><span class=p>(</span><span class=n>tokenizer</span><span class=p>(</span><span class=n>x</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>label_pipeline</span> <span class=o>=</span> <span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=mi>1</span> <span class=k>if</span> <span class=n>x</span> <span class=o>==</span> <span class=s1>&#39;pos&#39;</span> <span class=k>else</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 封装处理逻辑</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>process_data</span><span class=p>(</span><span class=n>data_iter</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>processed_data</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>label</span><span class=p>,</span> <span class=n>text</span> <span class=ow>in</span> <span class=n>data_iter</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>processed_text</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=n>text_pipeline</span><span class=p>(</span><span class=n>text</span><span class=p>),</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>int64</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>processed_label</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=n>label_pipeline</span><span class=p>(</span><span class=n>label</span><span class=p>),</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>int64</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>processed_data</span><span class=o>.</span><span class=n>append</span><span class=p>((</span><span class=n>processed_label</span><span class=p>,</span> <span class=n>processed_text</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>processed_data</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>train_data</span> <span class=o>=</span> <span class=n>process_data</span><span class=p>(</span><span class=n>IMDB</span><span class=p>(</span><span class=n>split</span><span class=o>=</span><span class=s1>&#39;train&#39;</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>test_data</span> <span class=o>=</span> <span class=n>process_data</span><span class=p>(</span><span class=n>IMDB</span><span class=p>(</span><span class=n>split</span><span class=o>=</span><span class=s1>&#39;test&#39;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;数据准备完成。&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># 可以保存 vocab 和处理好的数据，以便后续使用</span>
</span></span><span class=line><span class=cl><span class=c1># torch.save(vocab, &#39;vocab.pth&#39;)</span>
</span></span><span class=line><span class=cl><span class=c1># torch.save(train_data, &#39;train_data.pth&#39;)</span>
</span></span><span class=line><span class=cl><span class=c1># torch.save(test_data, &#39;test_data.pth&#39;)</span>
</span></span></code></pre></div></li><li><p><code>Dataset</code> 和 <code>DataLoader</code></p><p>PyTorch的<code>DataLoader</code>需要一个<code>collate_fn</code>来处理变长的文本序列，将它们填充（padding）到同一批次中最长序列的长度。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 2_dataloader.py</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torch.utils.data</span> <span class=kn>import</span> <span class=n>DataLoader</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torch.nn.utils.rnn</span> <span class=kn>import</span> <span class=n>pad_sequence</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>device</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>device</span><span class=p>(</span><span class=s2>&#34;cuda&#34;</span> <span class=k>if</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>is_available</span><span class=p>()</span> <span class=k>else</span> <span class=s2>&#34;cpu&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 加载之前处理好的数据</span>
</span></span><span class=line><span class=cl><span class=c1># vocab = torch.load(&#39;vocab.pth&#39;)</span>
</span></span><span class=line><span class=cl><span class=c1># train_data = torch.load(&#39;train_data.pth&#39;)</span>
</span></span><span class=line><span class=cl><span class=c1># test_data = torch.load(&#39;test_data.pth&#39;)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>PAD_IDX</span> <span class=o>=</span> <span class=n>vocab</span><span class=p>[</span><span class=s1>&#39;&lt;pad&gt;&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>collate_batch</span><span class=p>(</span><span class=n>batch</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>label_list</span><span class=p>,</span> <span class=n>text_list</span><span class=p>,</span> <span class=n>lengths</span> <span class=o>=</span> <span class=p>[],</span> <span class=p>[],</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=n>_label</span><span class=p>,</span> <span class=n>_text</span><span class=p>)</span> <span class=ow>in</span> <span class=n>batch</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>label_list</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>_label</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>text_list</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>_text</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>lengths</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>_text</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>labels</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=n>label_list</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>texts</span> <span class=o>=</span> <span class=n>pad_sequence</span><span class=p>(</span><span class=n>text_list</span><span class=p>,</span> <span class=n>batch_first</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>padding_value</span><span class=o>=</span><span class=n>PAD_IDX</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>lengths</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=n>lengths</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>int64</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>labels</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>),</span> <span class=n>texts</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>),</span> <span class=n>lengths</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>BATCH_SIZE</span> <span class=o>=</span> <span class=mi>64</span>
</span></span><span class=line><span class=cl><span class=n>train_dataloader</span> <span class=o>=</span> <span class=n>DataLoader</span><span class=p>(</span><span class=n>train_data</span><span class=p>,</span> <span class=n>batch_size</span><span class=o>=</span><span class=n>BATCH_SIZE</span><span class=p>,</span> <span class=n>shuffle</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>collate_fn</span><span class=o>=</span><span class=n>collate_batch</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>test_dataloader</span> <span class=o>=</span> <span class=n>DataLoader</span><span class=p>(</span><span class=n>test_data</span><span class=p>,</span> <span class=n>batch_size</span><span class=o>=</span><span class=n>BATCH_SIZE</span><span class=p>,</span> <span class=n>shuffle</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>collate_fn</span><span class=o>=</span><span class=n>collate_batch</span><span class=p>)</span>
</span></span></code></pre></div></li><li><p>构建模型</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 3_model.py</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>SentimentLSTM</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>vocab_size</span><span class=p>,</span> <span class=n>embedding_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>,</span> <span class=n>output_dim</span><span class=p>,</span> <span class=n>n_layers</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>                <span class=n>bidirectional</span><span class=p>,</span> <span class=n>dropout</span><span class=p>,</span> <span class=n>pad_idx</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>embedding</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>vocab_size</span><span class=p>,</span> <span class=n>embedding_dim</span><span class=p>,</span> <span class=n>padding_idx</span><span class=o>=</span><span class=n>pad_idx</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>lstm</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LSTM</span><span class=p>(</span><span class=n>embedding_dim</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>                            <span class=n>hidden_dim</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>                            <span class=n>num_layers</span><span class=o>=</span><span class=n>n_layers</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>                            <span class=n>bidirectional</span><span class=o>=</span><span class=n>bidirectional</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>                            <span class=n>dropout</span><span class=o>=</span><span class=n>dropout</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>                            <span class=n>batch_first</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>fc</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_dim</span> <span class=o>*</span> <span class=mi>2</span> <span class=k>if</span> <span class=n>bidirectional</span> <span class=k>else</span> <span class=n>hidden_dim</span><span class=p>,</span> <span class=n>output_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>dropout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>text</span><span class=p>,</span> <span class=n>text_lengths</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># text = [batch size, sent len]</span>
</span></span><span class=line><span class=cl>        <span class=n>embedded</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>embedding</span><span class=p>(</span><span class=n>text</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=c1># embedded = [batch size, sent len, emb dim]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Pack sequence</span>
</span></span><span class=line><span class=cl>        <span class=n>packed_embedded</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>rnn</span><span class=o>.</span><span class=n>pack_padded_sequence</span><span class=p>(</span><span class=n>embedded</span><span class=p>,</span> <span class=n>text_lengths</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=s1>&#39;cpu&#39;</span><span class=p>),</span> <span class=n>batch_first</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>enforce_sorted</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>packed_output</span><span class=p>,</span> <span class=p>(</span><span class=n>hidden</span><span class=p>,</span> <span class=n>cell</span><span class=p>)</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>lstm</span><span class=p>(</span><span class=n>packed_embedded</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Unpack sequence</span>
</span></span><span class=line><span class=cl>        <span class=c1># output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>lstm</span><span class=o>.</span><span class=n>bidirectional</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>hidden</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>((</span><span class=n>hidden</span><span class=p>[</span><span class=o>-</span><span class=mi>2</span><span class=p>,:,:],</span> <span class=n>hidden</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>,:,:]),</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>hidden</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>hidden</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>,:,:])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># hidden = [batch size, hid dim * num directions]</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc</span><span class=p>(</span><span class=n>hidden</span><span class=p>)</span>
</span></span></code></pre></div></li><li><p>训练与评估</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 4_train.py</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.optim</span> <span class=k>as</span> <span class=nn>optim</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># --- 模型超参数 ---</span>
</span></span><span class=line><span class=cl><span class=n>VOCAB_SIZE</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>vocab</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>EMBEDDING_DIM</span> <span class=o>=</span> <span class=mi>100</span>
</span></span><span class=line><span class=cl><span class=n>HIDDEN_DIM</span> <span class=o>=</span> <span class=mi>256</span>
</span></span><span class=line><span class=cl><span class=n>OUTPUT_DIM</span> <span class=o>=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl><span class=n>N_LAYERS</span> <span class=o>=</span> <span class=mi>2</span>
</span></span><span class=line><span class=cl><span class=n>BIDIRECTIONAL</span> <span class=o>=</span> <span class=kc>True</span>
</span></span><span class=line><span class=cl><span class=n>DROPOUT</span> <span class=o>=</span> <span class=mf>0.5</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>SentimentLSTM</span><span class=p>(</span><span class=n>VOCAB_SIZE</span><span class=p>,</span> <span class=n>EMBEDDING_DIM</span><span class=p>,</span> <span class=n>HIDDEN_DIM</span><span class=p>,</span> <span class=n>OUTPUT_DIM</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>                    <span class=n>N_LAYERS</span><span class=p>,</span> <span class=n>BIDIRECTIONAL</span><span class=p>,</span> <span class=n>DROPOUT</span><span class=p>,</span> <span class=n>PAD_IDX</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>optimizer</span> <span class=o>=</span> <span class=n>optim</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>())</span>
</span></span><span class=line><span class=cl><span class=n>criterion</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>BCEWithLogitsLoss</span><span class=p>()</span> <span class=c1># 适用于二分类</span>
</span></span><span class=line><span class=cl><span class=n>criterion</span> <span class=o>=</span> <span class=n>criterion</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>binary_accuracy</span><span class=p>(</span><span class=n>preds</span><span class=p>,</span> <span class=n>y</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;返回批次的准确率&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>rounded_preds</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>round</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>sigmoid</span><span class=p>(</span><span class=n>preds</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>correct</span> <span class=o>=</span> <span class=p>(</span><span class=n>rounded_preds</span> <span class=o>==</span> <span class=n>y</span><span class=p>)</span><span class=o>.</span><span class=n>float</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>acc</span> <span class=o>=</span> <span class=n>correct</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span> <span class=o>/</span> <span class=nb>len</span><span class=p>(</span><span class=n>correct</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>acc</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>train</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>iterator</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>,</span> <span class=n>criterion</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>epoch_loss</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>    <span class=n>epoch_acc</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>labels</span><span class=p>,</span> <span class=n>text</span><span class=p>,</span> <span class=n>lengths</span> <span class=ow>in</span> <span class=n>iterator</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>predictions</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>text</span><span class=p>,</span> <span class=n>lengths</span><span class=p>)</span><span class=o>.</span><span class=n>squeeze</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>loss</span> <span class=o>=</span> <span class=n>criterion</span><span class=p>(</span><span class=n>predictions</span><span class=p>,</span> <span class=n>labels</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>acc</span> <span class=o>=</span> <span class=n>binary_accuracy</span><span class=p>(</span><span class=n>predictions</span><span class=p>,</span> <span class=n>labels</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>epoch_loss</span> <span class=o>+=</span> <span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>epoch_acc</span> <span class=o>+=</span> <span class=n>acc</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>epoch_loss</span> <span class=o>/</span> <span class=nb>len</span><span class=p>(</span><span class=n>iterator</span><span class=p>),</span> <span class=n>epoch_acc</span> <span class=o>/</span> <span class=nb>len</span><span class=p>(</span><span class=n>iterator</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>evaluate</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>iterator</span><span class=p>,</span> <span class=n>criterion</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>epoch_loss</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>    <span class=n>epoch_acc</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>.</span><span class=n>eval</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>labels</span><span class=p>,</span> <span class=n>text</span><span class=p>,</span> <span class=n>lengths</span> <span class=ow>in</span> <span class=n>iterator</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>predictions</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>text</span><span class=p>,</span> <span class=n>lengths</span><span class=p>)</span><span class=o>.</span><span class=n>squeeze</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>loss</span> <span class=o>=</span> <span class=n>criterion</span><span class=p>(</span><span class=n>predictions</span><span class=p>,</span> <span class=n>labels</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>acc</span> <span class=o>=</span> <span class=n>binary_accuracy</span><span class=p>(</span><span class=n>predictions</span><span class=p>,</span> <span class=n>labels</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=n>epoch_loss</span> <span class=o>+=</span> <span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
</span></span><span class=line><span class=cl>            <span class=n>epoch_acc</span> <span class=o>+=</span> <span class=n>acc</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>epoch_loss</span> <span class=o>/</span> <span class=nb>len</span><span class=p>(</span><span class=n>iterator</span><span class=p>),</span> <span class=n>epoch_acc</span> <span class=o>/</span> <span class=nb>len</span><span class=p>(</span><span class=n>iterator</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># --- 执行训练 ---</span>
</span></span><span class=line><span class=cl><span class=n>N_EPOCHS</span> <span class=o>=</span> <span class=mi>5</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>N_EPOCHS</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>train_loss</span><span class=p>,</span> <span class=n>train_acc</span> <span class=o>=</span> <span class=n>train</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>train_dataloader</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>,</span> <span class=n>criterion</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>valid_loss</span><span class=p>,</span> <span class=n>valid_acc</span> <span class=o>=</span> <span class=n>evaluate</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>test_dataloader</span><span class=p>,</span> <span class=n>criterion</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;Epoch: </span><span class=si>{</span><span class=n>epoch</span><span class=o>+</span><span class=mi>1</span><span class=si>:</span><span class=s1>02</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;</span><span class=se>\t</span><span class=s1>Train Loss: </span><span class=si>{</span><span class=n>train_loss</span><span class=si>:</span><span class=s1>.3f</span><span class=si>}</span><span class=s1> | Train Acc: </span><span class=si>{</span><span class=n>train_acc</span><span class=o>*</span><span class=mi>100</span><span class=si>:</span><span class=s1>.2f</span><span class=si>}</span><span class=s1>%&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;</span><span class=se>\t</span><span class=s1> Val. Loss: </span><span class=si>{</span><span class=n>valid_loss</span><span class=si>:</span><span class=s1>.3f</span><span class=si>}</span><span class=s1> |  Val. Acc: </span><span class=si>{</span><span class=n>valid_acc</span><span class=o>*</span><span class=mi>100</span><span class=si>:</span><span class=s1>.2f</span><span class=si>}</span><span class=s1>%&#39;</span><span class=p>)</span>
</span></span></code></pre></div></li></ol><h2 id=本章小结>本章小结</h2><p>在本章中，我们成功地推开了深度学习这扇厚重而又充满魅力的大门。</p><p>我们从最基本的神经元模型出发，理解了神经网络如何通过前向传播进行预测，以及如何通过反向传播和梯度下降这一核心机制来进行“学习”。这为我们后续理解一切复杂的深度学习模型奠定了坚实的理论基础。</p><p>接着，我们深入掌握了PyTorch框架。我们学会了使用<code>Tensor</code>进行GPU加速计算，领略了<code>Autograd</code>自动求导的魔力，并掌握了使用<code>nn.Module</code>来像搭积木一样构建我们自己的神经网络。</p><p>我们还回顾了CNN和RNN/LSTM这两种经典的神经网络结构，理解了它们分别在处理空间特征和序列数据上的独特设计思想，这对于我们拓宽视野、理解后续更高级架构的演进至关重要。</p><p>最后，我们探讨了“训练的艺术”，学习了如何选择合适的损失函数和优化器，以及如何使用正则化技术来防止模型过拟合。并通过一个完整的文本情感分类实战项目，将本章所有零散的知识点，凝聚成了一套行之有效的深度学习项目开发流程。</p><p>完成本章后，你已经不再是一个深度学习的门外汉。你具备了理解、构建和训练一个中等复杂度深度学习模型的能力。这块坚实的“基石”，将有力地支撑你向本书的下一个，也是最核心的目标迈进——深入理解并驾驭那颗“皇冠上的明珠”：Transformer架构与大语言模型。</p></div><div class=post_footer><div class=meta><div class=info><span class="field tags"><i class=ri-stack-line></i>
<a href=https://zhurongshuo.com/tags/%E4%B9%A6%E7%A8%BF/>书稿</a></span></div></div></div></div><div class=doc_comments></div></div></div></div><a id=back_to_top href=# class=back_to_top><i class=ri-arrow-up-s-line></i></a><footer class=footer><div class=powered_by><a href=https://varkai.com>Designed by VarKai, </a><a href=http://www.gohugo.io/>Proudly published with Hugo,</a></div><div class=footer_slogan><span>法不净空，觉无性也。</span></div><div class=powered_by style=margin-top:10px;font-size:14px><a href=https://zhurongshuo.com/>Copyright © 2010-2025 祝融说 zhurongshuo.com All Rights Reserved.</a></div></footer><script defer src=https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><link href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.css rel=stylesheet integrity="sha256-7qiTu3a8qjjWtcX9w+f2ulVUZSUdCZFEK62eRlmLmCE=" crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin=anonymous></script><script defer src=https://zhurongshuo.com/js/zozo.js></script><script type=text/javascript async src="https://cdn.jsdelivr.net/npm/mathjax@2.7.8/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["[[","]]"]],processEscapes:!0,processEnvironments:!0,skipTags:["script","noscript","style","textarea","pre"],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}}),MathJax.Hub.Queue(function(){var e,t=MathJax.Hub.getAllJax();for(e=0;e<t.length;e+=1)t[e].SourceElement().parentNode.className+=" has-jax"})</script><style>code.has-jax{font:inherit;font-size:100%;background:inherit;border:inherit;color:#515151}</style><script async src="https://www.googletagmanager.com/gtag/js?id=G-KKJ5ZEG1NB"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KKJ5ZEG1NB")</script></body></html>