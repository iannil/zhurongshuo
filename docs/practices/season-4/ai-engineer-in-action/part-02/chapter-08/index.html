<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><meta name=google-site-verification content="8_xpI-TS3tNV8UPug-Q6Ef3BhKTcy0WOG7dEdAcm2zk"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="祝融"><link rel=dns-prefetch href=//cdn.jsdelivr.net><link rel=preconnect href=https://cdn.jsdelivr.net crossorigin><link rel=dns-prefetch href=//www.googletagmanager.com><link rel=preconnect href=https://www.googletagmanager.com crossorigin><link rel=canonical href=https://zhurongshuo.com/practices/season-4/ai-engineer-in-action/part-02/chapter-08/><title>祝融说。 第八章：定制你的专属大模型：高效微调技术实战</title><meta property="og:title" content="第八章：定制你的专属大模型：高效微调技术实战"><meta property="og:url" content="https://zhurongshuo.com/practices/season-4/ai-engineer-in-action/part-02/chapter-08/"><meta property="og:site_name" content="祝融说。"><meta property="og:locale" content="zh_CN"><meta property="og:type" content="article"><meta property="article:published_time" content="2025-12-09T00:00:00+08:00"><meta property="article:modified_time" content="2025-12-09T00:00:00+08:00"><meta property="article:tag" content="书稿"><meta name=description content="在上一章中，我们已经领略了大语言模型（LLM）的宏大图景。我们知道，像Llama、Qwen这样的开源LLM，是在海量的通用文本上预训练出来的“通用大脑”，它们知识渊博，能力强大。通过精巧的Prompt Engineering，我们已经可以在许多任务上引导它们给出令人满意的结果。
"><meta property="og:description" content="在上一章中，我们已经领略了大语言模型（LLM）的宏大图景。我们知道，像Llama、Qwen这样的开源LLM，是在海量的通用文本上预训练出来的“通用大脑”，它们知识渊博，能力强大。通过精巧的Prompt Engineering，我们已经可以在许多任务上引导它们给出令人满意的结果。
"><meta property="og:image" content="https://zhurongshuo.com/images/favicon.ico"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="第八章：定制你的专属大模型：高效微调技术实战"><meta name=twitter:description content="在上一章中，我们已经领略了大语言模型（LLM）的宏大图景。我们知道，像Llama、Qwen这样的开源LLM，是在海量的通用文本上预训练出来的“通用大脑”，它们知识渊博，能力强大。通过精巧的Prompt Engineering，我们已经可以在许多任务上引导它们给出令人满意的结果。
"><meta name=twitter:image content="https://zhurongshuo.com/images/favicon.ico"><meta name=keywords content="AI工程师实战：从Python基础到LLM应用与性能优化,第八章：定制你的专属大模型：高效微调技术实战"><link rel="shortcut icon" href=https://zhurongshuo.com/images/favicon.ico><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/normalize.css><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/animate-custom.css><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/zozo.css><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/remixicon-custom.css><link rel=stylesheet type=text/css media=screen href=https://zhurongshuo.com/css/highlight.css><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","headline":"第八章：定制你的专属大模型：高效微调技术实战","description":"在上一章中，我们已经领略了大语言模型（LLM）的宏大图景。我们知道，像Llama、Qwen这样的开源LLM，是在海量的通用文本上预训练出来的“通用大脑”，它们知识渊博，能力强大。通过精巧的Prompt Engineering，我们已经可以在许多任务上引导它们给出令人满意的结果。\n","datePublished":"2025-12-09T00:00:00\u002b08:00","dateModified":"2025-12-09T00:00:00\u002b08:00","author":{"@type":"Person","name":"祝融"},"publisher":{"@type":"Organization","name":"祝融说。","logo":{"@type":"ImageObject","url":"https:\/\/zhurongshuo.com\/images\/favicon.ico"}},"image":"https:\/\/zhurongshuo.com\/images\/favicon.ico","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/zhurongshuo.com\/practices\/season-4\/ai-engineer-in-action\/part-02\/chapter-08\/"},"keywords":"书稿"}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"首页","item":"https:\/\/zhurongshuo.com\/"},{"@type":"ListItem","position":2,"name":"practices","item":"https:\/\/zhurongshuo.com\/practices/"},{"@type":"ListItem","position":3,"name":"第八章：定制你的专属大模型：高效微调技术实战","item":"https:\/\/zhurongshuo.com\/practices\/season-4\/ai-engineer-in-action\/part-02\/chapter-08\/"}]}</script></head><body><div class="main animate__animated animate__fadeInDown"><div class="nav_container animated fadeInDown"><div class=site_nav id=site_nav><ul><li><a href=https://zhurongshuo.com/>首页</a></li><li><a href=https://zhurongshuo.com/start/>开始</a></li><li><a href=https://zhurongshuo.com/advanced/>进阶rc</a></li><li><a href=https://zhurongshuo.com/posts/>归档</a></li><li><a href=https://zhurongshuo.com/tags/>标签</a></li><li><a href=https://zhurongshuo.com/about/>关于</a></li></ul></div><div class=menu_icon><a id=menu_icon><i class=ri-menu-line></i></a></div></div><div class="header animated fadeInDown"><div class=site_title_container><div class=site_title><h1><a href=https://zhurongshuo.com/><span class=web-font>祝融说。</span></a></h1></div><div class=description><p class=sub_title>法不净空，觉无性也。</p><div class=my_socials><a href=https://zhurongshuo.com/books/ title=book-open><i class=ri-book-open-line></i></a>
<a href=https://zhurongshuo.com/practices/ title=trophy><i class=ri-trophy-line></i></a>
<a href=https://zhurongshuo.com/gallery/ title=gallery><i class=ri-gallery-line></i></a>
<a href=https://zhurongshuo.com/about/ title=game><i class=ri-game-line></i></a>
<a href type=application/rss+xml title=rss target=_blank><i class=ri-rss-fill></i></a></div></div></div></div><div class=content><div class=post_page><div class="post animate__animated animate__fadeInDown"><div class="post_title post_detail_title"><h2><a href=https://zhurongshuo.com/practices/season-4/ai-engineer-in-action/part-02/chapter-08/>第八章：定制你的专属大模型：高效微调技术实战</a></h2><span class=date>2025.12.09</span></div><div class="post_content markdown"><p>在上一章中，我们已经领略了大语言模型（LLM）的宏大图景。我们知道，像Llama、Qwen这样的开源LLM，是在海量的通用文本上预训练出来的“通用大脑”，它们知识渊博，能力强大。通过精巧的Prompt Engineering，我们已经可以在许多任务上引导它们给出令人满意的结果。</p><p>然而，在许多真实的商业场景中，我们面临的挑战更为具体和深入：</p><ul><li>我们需要一个能理解并使用公司内部术语和黑话的客服机器人。</li><li>我们需要一个能模仿特定作家风格，进行创意写作的助手。</li><li>我们需要一个能精准地将自然语言查询，转换为特定数据库查询语言（如SQL）的代码生成器。</li></ul><p>在这些场景下，仅仅依靠Prompt Engineering可能力不从心。通用的LLM或许能理解“什么是数据库”，但它不知道你公司内部那张错综复杂的“用户行为表”的具体结构。这时，我们就需要一种更强大的技术，来将这个“通用大脑”改造为我们需要的“领域专家”。这项技术，就是微调（Fine-tuning）。</p><p>微调，顾名思义，就是在预训练好的大模型基础上，使用我们自己准备的、与特定任务或领域相关的少量数据，对模型的参数进行“微小”的调整。这个过程，就像是给一位博学的通才，进行一次针对性的岗前培训，使其快速掌握特定岗位所需的专业知识和技能。</p><p>但是，传统的全量微调（Full Fine-tuning）——即更新模型的所有参数——对于动辄数十亿甚至上百亿参数的LLM来说，是一场极其昂贵的“豪赌”。它不仅需要海量的GPU显存（例如，微调一个70B模型可能需要8张A100 80G显卡），还会产生一个与原始模型同样大小的、全新的模型副本，这在存储和部署上都是巨大的负担。</p><p>幸运的是，研究者们提出了一系列参数高效微调（Parameter-Efficient Fine-Tuning, PEFT）技术，彻底改变了这一局面。PEFT的核心思想是：在微调时，冻结原始LLM的大部分参数，只引入或修改一小部分（通常不到1%）的新参数。这使得我们能够以极低的资源成本（甚至在单张消费级显卡上），实现与全量微调相媲美甚至更好的效果。</p><p>本章，我们将深入探索这个激动人心的领域。我们将学习：</p><ul><li>微调的原理：我们将明确为什么要微调，以及在什么时机选择微调，而不是依赖Prompt或RAG。</li><li>PEFT技术概览：我们将鸟瞰Prefix-Tuning, P-Tuning, Prompt Tuning等主流PEFT方法，理解它们的核心思想。</li><li>LoRA与QLoRA：我们将重点剖析当前最流行、最实用的PEFT方法——LoRA（Low-Rank Adaptation），并学习其进一步降低资源消耗的变体QLoRA。我们将深入其数学原理，并给出清晰的代码实现。</li><li>数据工程：高质量的数据是微调成功的关键。我们将学习如何构建符合特定格式的指令微调数据集，这是“喂给”模型进行学习的“精神食粮”。</li><li>实战项目：我们将通过一个完整的实战项目——使用LoRA技术，在单张GPU上微调一个7B规模的开源LLM，使其能够完成特定领域的问答任务——将本章所有知识点融会贯通。</li></ul><p>掌握了高效微调技术，你就拥有了将通用AI转化为专用AI的“点金石”。你将能够以可控的成本，为你的业务场景打造出独一无二的、具有强大竞争力的专属大模型。现在，让我们开始这场“驯服”巨兽的精彩旅程。</p><h2 id=81-微调的原理为什么要微调何时微调>8.1 微调的原理：为什么要微调，何时微调？</h2><h3 id=811-什么是微调>8.1.1 什么是微调？</h3><p>微调（Fine-tuning）是在一个已经过大规模数据预训练的模型（Pre-trained Model）的基础上，使用一个更小、更具针对性的数据集，继续进行训练，以使模型适应特定任务或领域的过程。</p><p>这个过程可以类比于人类学习：</p><ul><li>预训练：相当于一个人接受了从小学到大学的通识教育，学习了语言、数学、历史、科学等海量通用知识。这个阶段塑造了他的世界观和基础认知能力。LLM的预训练就是在互联网级别的文本上完成的。</li><li>微调：相当于这个人大学毕业后，进入一家律师事务所工作。他需要学习法律术语、案例分析方法、法庭辩论技巧等专业知识。这个“岗前培训”就是微调。他利用自己强大的通用语言和逻辑能力，快速地吸收了法律领域的专业知识，成为了一名律师。</li></ul><p>在微调过程中，模型的权重不再是随机初始化的，而是继承自预训练阶段。我们使用特定任务的数据（例如，法律问答对），以一个较小的学习率继续进行梯度下降，对这些权重进行“微调”，使其更符合新任务的分布。</p><h3 id=812-为什么要微调-微调的收益>8.1.2 为什么要微调？—— 微调的收益</h3><p>既然我们已经有了Prompt Engineering和RAG（检索增强生成），为什么还需要微调这么“重”的操作？微调能带来一些独特且不可替代的价值：</p><ol><li><p>注入领域知识（Domain Knowledge）：这是微调最核心的价值之一。虽然RAG可以为模型提供外部知识，但微调能将知识内化（Internalize）到模型的参数中。这使得模型能够更自然、更流畅地使用专业术语，理解领域内的微妙关系。例如，通过在医学文献上微调，模型能学会像医生一样思考和表达。</p></li><li><p>学习特定风格或格式（Style/Format Adaptation）：如果你希望模型生成的文本具有某种特定的风格（如莎士比亚风格、公司官方口吻）或遵循某种复杂的输出格式（如生成特定的JSON或XML结构），微调是比在Prompt中反复描述更可靠、更高效的方法。模型会从数据中“学会”这种模式。</p></li><li><p>掌握新能力（Capability Acquisition）：有些能力很难通过Prompt来教会。例如，将自然语言转换为一种公司自研的、从未在互联网上出现过的DSL（领域特定语言）。通过提供大量的（自然语言，DSL）配对数据进行微调，模型可以学会这种新的“翻译”能力。</p></li><li><p>提升可靠性与一致性：对于需要大规模、重复执行的任务，依赖复杂的Few-shot Prompt可能导致输出不稳定。微调后的模型，其行为更加可预测和一致，减少了对超长、精巧Prompt的依赖。</p></li><li><p>优化推理成本：一个经过微调的、更小的模型（如7B），在特定任务上的表现可能超过一个需要复杂Few-shot Prompt的、更大的通用模型（如70B）。在生产环境中，使用更小的模型意味着更低的推理延迟和成本。同时，微调后，你的Prompt可以变得更短、更简单，这也减少了每次API调用的token消耗。</p></li></ol><h3 id=813-何时微调-技术选型决策>8.1.3 何时微调？—— 技术选型决策</h3><p>微调并非万能药，它需要成本（数据、计算、时间）。在决定是否微调前，应该先尝试更轻量级的方法。一个典型的技术选型路径如下：</p><p>第一步：Prompt Engineering (零成本)</p><p>对于简单的、一次性的任务，或者当你刚开始探索一个新场景时，首先尝试通过精心设计的Prompt（包括Zero-shot, Few-shot, CoT等技巧）来解决问题。</p><p>适用场景：通用问答、简单摘要、文本润色、创意生成等。</p><p>如果效果满足要求，就此打住。</p><p>第二步：检索增强生成 (RAG) (中等成本)</p><p>如果模型的主要问题是“知识不足”或“产生幻觉”（例如，回答关于公司内部产品的问题，或最新的新闻事件），那么RAG是首选。</p><p>适用场景：构建基于私有知识库的问答系统、需要引用信源的报告生成、需要实时信息的任务。</p><p>成本：需要构建和维护一个知识库及检索系统。</p><p>如果RAG能解决问题，通常也无需微调。 RAG和微调并不互斥，可以结合使用（先微调模型以适应领域语言风格，再通过RAG提供实时知识）。</p><p>第三步：高效参数微调 (PEFT) (中高成本)</p><p>当你尝试了前两步，但模型仍然无法满足以下需求时，就应该考虑微调了：</p><ul><li>需要深度适应领域术语和语言风格。</li><li>需要学习一种新的、复杂的输出格式或能力。</li><li>在特定任务上的性能、可靠性要求极高。</li><li>需要通过压缩Prompt或使用更小模型来优化推理成本。</li></ul><p>成本：需要收集和标注高质量的微调数据集，并投入一定的计算资源。</p><p>第四步：全量微调 (Full Fine-tuning) (极高成本)</p><p>在PEFT技术成熟的今天，全量微调的需求场景已经大大减少。通常只在以下情况考虑：</p><ul><li>你有海量的、高质量的领域数据。</li><li>你的任务与模型的原始预训练任务差异巨大。</li><li>你追求极致的性能，并且不计成本。</li><li>在大多数情况下，PEFT都是比全量微调更具性价比的选择。</li></ul><p>决策流程图：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>graph TD
</span></span><span class=line><span class=cl>    A[开始: 定义任务需求] --&gt; B{尝试Prompt Engineering};
</span></span><span class=line><span class=cl>    B -- 效果满足? --&gt; C[完成];
</span></span><span class=line><span class=cl>    B -- 效果不满足 --&gt; D{问题是知识不足/幻觉?};
</span></span><span class=line><span class=cl>    D -- 是 --&gt; E[实施RAG];
</span></span><span class=line><span class=cl>    E -- 效果满足? --&gt; C;
</span></span><span class=line><span class=cl>    E -- 效果不满足/需要结合 --&gt; F{需要学习风格/格式/新能力?};
</span></span><span class=line><span class=cl>    D -- 否 --&gt; F;
</span></span><span class=line><span class=cl>    F -- 是 --&gt; G[实施PEFT微调];
</span></span><span class=line><span class=cl>    G -- 效果满足? --&gt; C;
</span></span><span class=line><span class=cl>    G -- 效果不满足/追求极致性能 --&gt; H[考虑全量微调];
</span></span></code></pre></div><h2 id=82-参数高效微调peft概览>8.2 参数高效微调（PEFT）概览</h2><p>PEFT的核心思想是：在微调过程中，保持预训练模型的主体参数<code>Φ</code>不变（冻结），只调整一小部分额外添加的、或者选择性解冻的参数<code>Δθ</code>。由于<code>|Δθ| &lt;&lt; |Φ|</code>，这极大地降低了计算和存储的开销。</p><p>根据修改参数的位置和方式，PEFT方法可以分为三大类：</p><h3 id=821-adapter-based方法在模型中插入新模块>8.2.1 Adapter-based方法：在模型中“插入”新模块</h3><p>代表方法：Adapter Tuning</p><p>思想：在Transformer的每个Layer内部，插入两个小型的、瓶颈结构的前馈神经网络模块（称为Adapter）。在微调时，只训练这些新插入的Adapter模块的参数，而原始的自注意力层和前馈网络层都被冻结。</p><p>优点：实现简单，效果稳定。</p><p>缺点：插入的Adapter模块会增加模型的推理延迟。</p><h3 id=822-prompt-based方法在输入端添加可学习的提示>8.2.2 Prompt-based方法：在输入端“添加”可学习的提示</h3><p>这类方法不在模型结构上动刀，而是在输入层做文章。</p><p>代表方法：Prefix-Tuning, P-Tuning, Prompt Tuning</p><p>思想：</p><ul><li>Prompt Tuning：在输入的词嵌入序列前面，拼接上一些可学习的、连续的“虚拟token”（Soft Prompt）。在微调时，只更新这些虚拟token的嵌入向量，而模型的其他所有部分（包括词嵌入层）都保持冻结。</li><li>Prefix-Tuning / P-Tuning：与Prompt Tuning类似，但它们将这些可学习的“前缀”添加到了Transformer的每一层，而不仅仅是输入层，从而给予模型更大的调整自由度。</li></ul><p>优点：不改变模型结构，不增加推理延迟。</p><p>缺点：性能有时不如Adapter方法稳定，且可学习的Prompt长度是一个难以调整的超参数。</p><h3 id=823-reparameterization-based方法对权重矩阵进行低秩修改>8.2.3 Reparameterization-based方法：对权重矩阵进行“低秩”修改</h3><p>这类方法是目前最主流、效果最好的PEFT范式。</p><p>代表方法：LoRA (Low-Rank Adaptation)</p><p>思想：它基于一个假设——在微调过程中，模型权重的变化是“低秩”的。也就是说，巨大的权重更新矩阵<code>ΔW</code>，可以用两个更小的、低秩的矩阵<code>A</code>和<code>B</code>的乘积来近似：<code>ΔW ≈ B * A</code>。</p><p>实现：在微调时，LoRA冻结原始的权重矩阵<code>W</code>，并在其旁边并联一个由矩阵<code>A</code>和<code>B</code>组成的“旁路”。我们只训练<code>A</code>和<code>B</code>的参数。在推理时，可以将训练好的<code>B*A</code>与原始的<code>W</code>合并（<code>W' = W + B*A</code>），从而完全不增加任何推理延迟。</p><p>优点：性能非常接近全量微调，且推理时无额外开销。实现灵活，可以应用到任意线性层。</p><p>缺点：暂无明显缺点，已成为PEFT的事实标准。</p><p>我们将在下一节详细剖析LoRA的原理与实现。</p><h2 id=83-lora与qlora原理与代码实现>8.3 LoRA与QLoRA：原理与代码实现</h2><h3 id=831-lora-low-rank-adaptation-的核心原理>8.3.1 LoRA (Low-Rank Adaptation) 的核心原理</h3><p>假设我们有一个预训练的权重矩阵 <code>W₀ ∈ R^(d×k)</code>（例如，Transformer中Q, K, V投影的线性层权重）。在全量微调中，我们会更新这个矩阵，得到 <code>W₀ + ΔW</code>。LoRA的核心洞察是，这个更新矩阵 <code>ΔW</code> 具有很低的“内在秩”（intrinsic rank），即它可以用更少的信息来表示。</p><p>LoRA的做法是，使用两个低秩矩阵 <code>B ∈ R^(d×r)</code> 和 <code>A ∈ R^(r×k)</code> 来表示 <code>ΔW</code>，其中秩 <code>r</code> 是一个远小于 <code>d</code> 和 <code>k</code> 的超参数（例如 <code>r=8, 16, 64</code>）。</p><p>前向传播过程的变化：</p><ul><li>原始路径：<code>h = W₀ * x</code></li><li>LoRA路径：<code>h = W₀ * x + B * A * x</code></li></ul><p>为了进一步降低计算量，LoRA还引入了一个缩放因子 <code>α</code>：<code>h = W₀ * x + (α/r) * B * A * x</code></p><p>训练过程：</p><ol><li>冻结原始权重 <code>W₀</code>。</li><li>随机初始化矩阵 <code>A</code>（例如，高斯分布）和矩阵 <code>B</code>（初始化为0）。</li><li>在微调过程中，只更新 <code>A</code> 和 <code>B</code> 的参数。</li></ol><p>可训练参数量对比：</p><ul><li>全量微调：<code>d * k</code></li><li>LoRA：<code>r * d + r * k = r * (d + k)</code></li></ul><p>由于 <code>r &lt;&lt; d</code> 且 <code>r &lt;&lt; k</code>，LoRA的可训练参数量极大地减少了。例如，对于一个<code>4096×4096</code>的矩阵，全量微调需要更新约16.7M参数。如果使用<code>r=8</code>的LoRA，只需要更新<code>8 * (4096 + 4096) ≈ 65k</code>参数，减少了超过250倍！</p><p>推理过程（合并权重）：</p><p>训练完成后，我们可以将LoRA的旁路合并回主干，实现零额外推理延迟。</p><ol><li>计算训练好的 <code>ΔW = (α/r) * B * A</code>。</li><li>计算新的权重矩阵 <code>W' = W₀ + ΔW</code>。</li><li>在部署时，直接使用这个合并后的权重矩阵 <code>W'</code>，完全丢弃 <code>A</code> 和 <code>B</code>。</li></ol><h3 id=832-qlora在消费级gpu上微调巨型模型>8.3.2 QLoRA：在消费级GPU上微调巨型模型</h3><p>LoRA已经极大地降低了微调的门槛，但对于非常大的模型（如70B），加载模型本身就需要巨大的显存。QLoRA (Quantized LoRA) 进一步将资源消耗推向了极致。</p><p>QLoRA结合了两种技术：</p><ol><li>4-bit NormalFloat (NF4) 量化：这是QLoRA的核心创新。它将预训练模型的冻结权重（即<code>W₀</code>）从标准的16位浮点数（FP16）或32位浮点数（FP32），量化为一种新的4位数据类型（NF4）。这使得加载模型所需的显存减少了约4倍。例如，一个7B模型，FP16需要约14GB显存，而4-bit量化后只需要约4-5GB。</li><li>双重量化（Double Quantization）：为了进一步节省内存，QLoRA对量化过程中产生的“量化常数”本身，也进行了二次量化。</li><li>Paged Optimizers：利用NVIDIA统一内存的特性，防止在处理长序列导致梯度检查点（gradient checkpointing）内存激增时，出现显存不足（OOM）的错误。</li></ol><p>QLoRA的训练过程：</p><ol><li>将预训练模型的权重以4-bit NF4格式加载到GPU。</li><li>插入LoRA适配器（矩阵A和B）。</li><li>在训练时，只有LoRA的参数（A和B）是以高精度（如FP16）进行计算和更新的。</li><li>当进行前向和反向传播，需要用到原始权重<code>W₀</code>时，系统会动态地将4-bit的权重反量化（de-quantize）为FP16，进行计算，计算完毕后立即丢弃，内存中始终只保留4-bit的权重。</li></ol><p>通过这种方式，QLoRA在保持了与16-bit LoRA微调几乎相同性能的同时，极大地降低了显存的峰值占用，使得在单张24GB的消费级显卡（如RTX 3090/4090）上微调65B模型成为可能。</p><h3 id=833-使用peft库实现loraqlora>8.3.3 使用<code>peft</code>库实现LoRA/QLoRA</h3><p>Hugging Face的<code>peft</code>库极大地简化了PEFT方法的实现。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>AutoModelForCausalLM</span><span class=p>,</span> <span class=n>AutoTokenizer</span><span class=p>,</span> <span class=n>BitsAndBytesConfig</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>peft</span> <span class=kn>import</span> <span class=n>get_peft_model</span><span class=p>,</span> <span class=n>LoraConfig</span><span class=p>,</span> <span class=n>TaskType</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 模型名称</span>
</span></span><span class=line><span class=cl><span class=n>model_name</span> <span class=o>=</span> <span class=s2>&#34;meta-llama/Llama-2-7b-hf&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># --- QLoRA 配置 ---</span>
</span></span><span class=line><span class=cl><span class=c1># 1. BitsAndBytesConfig 用于4-bit量化</span>
</span></span><span class=line><span class=cl><span class=n>bnb_config</span> <span class=o>=</span> <span class=n>BitsAndBytesConfig</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>load_in_4bit</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>bnb_4bit_use_double_quant</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>bnb_4bit_quant_type</span><span class=o>=</span><span class=s2>&#34;nf4&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>bnb_4bit_compute_dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>bfloat16</span> <span class=c1># 计算时使用的类型</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 2. 加载量化后的模型</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModelForCausalLM</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model_name</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>quantization_config</span><span class=o>=</span><span class=n>bnb_config</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>device_map</span><span class=o>=</span><span class=s2>&#34;auto&#34;</span> <span class=c1># 自动将模型分配到可用设备</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 加载分词器</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_name</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span><span class=o>.</span><span class=n>pad_token</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>eos_token</span> <span class=c1># 设置pad token</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># --- LoRA 配置 ---</span>
</span></span><span class=line><span class=cl><span class=c1># 3. LoraConfig</span>
</span></span><span class=line><span class=cl><span class=n>peft_config</span> <span class=o>=</span> <span class=n>LoraConfig</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>task_type</span><span class=o>=</span><span class=n>TaskType</span><span class=o>.</span><span class=n>CAUSAL_LM</span><span class=p>,</span> <span class=c1># 任务类型：因果语言模型</span>
</span></span><span class=line><span class=cl>    <span class=n>r</span><span class=o>=</span><span class=mi>8</span><span class=p>,</span>                          <span class=c1># LoRA的秩</span>
</span></span><span class=line><span class=cl>    <span class=n>lora_alpha</span><span class=o>=</span><span class=mi>32</span><span class=p>,</span>                <span class=c1># LoRA的alpha参数</span>
</span></span><span class=line><span class=cl>    <span class=n>lora_dropout</span><span class=o>=</span><span class=mf>0.1</span><span class=p>,</span>             <span class=c1># LoRA层的dropout率</span>
</span></span><span class=line><span class=cl>    <span class=c1># 指定要应用LoRA的模块名称，通常是Q和V的投影层</span>
</span></span><span class=line><span class=cl>    <span class=n>target_modules</span><span class=o>=</span><span class=p>[</span><span class=s2>&#34;q_proj&#34;</span><span class=p>,</span> <span class=s2>&#34;v_proj&#34;</span><span class=p>],</span> 
</span></span><span class=line><span class=cl>    <span class=n>bias</span><span class=o>=</span><span class=s2>&#34;none&#34;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 4. 使用get_peft_model将LoRA应用到模型上</span>
</span></span><span class=line><span class=cl><span class=n>peft_model</span> <span class=o>=</span> <span class=n>get_peft_model</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>peft_config</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 打印可训练参数</span>
</span></span><span class=line><span class=cl><span class=n>peft_model</span><span class=o>.</span><span class=n>print_trainable_parameters</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=c1># 输出: trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.0622...</span>
</span></span></code></pre></div><p>只需短短几行代码，我们就将一个巨大的LLM转换为了一个轻量的、可微调的PEFT模型。接下来，我们就可以像训练普通PyTorch模型一样，使用<code>transformers.Trainer</code>或自定义训练循环来训练这个<code>peft_model</code>了。</p><h2 id=84-数据工程构建高质量的指令微调数据集>8.4 数据工程：构建高质量的指令微调数据集</h2><p>“Garbage in, garbage out.”（垃圾进，垃圾出。）这句话在LLM微调中体现得淋漓尽致。数据集的质量，是决定微调成败的最关键因素。</p><p>指令微调（Instruction-Tuning）是当前最主流的微调范式。其核心思想是，将各种任务都转化为“指令-响应”的格式，让模型学会遵循人类的指令。</p><h3 id=841-数据集格式>8.4.1 数据集格式</h3><p>一个典型的指令微调数据集，通常是一个JSONL文件（每行一个JSON对象），每个JSON对象包含以下字段：
<code>instruction</code>：对任务的清晰描述。
<code>input</code>（可选）：任务的上下文或输入。
<code>output</code>：期望模型生成的标准答案。</p><p>示例：</p><ol><li><p>无<code>input</code>的问答任务</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>{</span><span class=nt>&#34;instruction&#34;</span><span class=p>:</span> <span class=s2>&#34;中国的首都是哪里？&#34;</span><span class=p>,</span> <span class=nt>&#34;input&#34;</span><span class=p>:</span> <span class=s2>&#34;&#34;</span><span class=p>,</span> <span class=nt>&#34;output&#34;</span><span class=p>:</span> <span class=s2>&#34;中国的首都是北京。&#34;</span><span class=p>}</span>
</span></span></code></pre></div></li><li><p>有<code>input</code>的分类任务</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>{</span><span class=nt>&#34;instruction&#34;</span><span class=p>:</span> <span class=s2>&#34;请判断以下句子的情感是正面、负面还是中性。&#34;</span><span class=p>,</span> <span class=nt>&#34;input&#34;</span><span class=p>:</span> <span class=s2>&#34;这家餐厅的食物味道很棒，但服务太慢了。&#34;</span><span class=p>,</span> <span class=nt>&#34;output&#34;</span><span class=p>:</span> <span class=s2>&#34;中性&#34;</span><span class=p>}</span>
</span></span></code></pre></div></li><li><p>代码生成任务</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>{</span><span class=nt>&#34;instruction&#34;</span><span class=p>:</span> <span class=s2>&#34;编写一个Python函数，计算斐波那契数列的第n项。&#34;</span><span class=p>,</span> <span class=nt>&#34;input&#34;</span><span class=p>:</span> <span class=s2>&#34;n = 10&#34;</span><span class=p>,</span> <span class=nt>&#34;output&#34;</span><span class=p>:</span> <span class=s2>&#34;def fibonacci(n):\n  if n &lt;= 1:\n    return n\n  else:\n    return fibonacci(n-1) + fibonacci(n-2)&#34;</span><span class=p>}</span>
</span></span></code></pre></div></li></ol><h3 id=842-prompt模板化>8.4.2 Prompt模板化</h3><p>在将数据喂给模型前，我们需要将这些结构化的字段，组合成一个单一的文本字符串，这个过程称为Prompt模板化。一个好的模板，能够清晰地向模型展示任务的结构。</p><p>一个常用的模板（Alpaca格式）：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>Below is an instruction that describes a task. Write a response that appropriately completes the request.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>### Instruction:
</span></span><span class=line><span class=cl>{instruction}
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>### Input:
</span></span><span class=line><span class=cl>{input}
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>### Response:
</span></span><span class=line><span class=cl>{output}
</span></span></code></pre></div><p>如果<code>input</code>为空，则<code>### Input:</code>部分可以省略。在训练时，模型的目标就是根据<code>Instruction</code>和<code>Input</code>，去生成<code>Response</code>部分的内容。</p><h3 id=843-构建高质量数据集的原则>8.4.3 构建高质量数据集的原则</h3><ol><li>质量远比数量重要：几百条高质量、精心设计的数据，其效果可能远超数万条低质量、有噪声的数据。</li><li>多样性：
指令多样性：同一个任务，可以用多种不同的方式来提问。例如，“总结这篇文章”、“给这篇文章写个摘要”、“这篇文章的核心观点是什么？”。
输入多样性：覆盖各种可能的输入场景、边缘情况。
响应多样性：对于开放式问题，答案也应该是多样的。</li><li>清晰无歧义：指令应该清晰、明确，避免含糊不清的描述。</li><li>正确性：<code>output</code>必须是绝对正确的标准答案。</li><li>简洁性：在保证清晰的前提下，指令和输入应尽可能简洁。</li></ol><h3 id=844-数据来源>8.4.4 数据来源</h3><p>使用现有开源数据集：</p><ul><li>Alpaca：斯坦福大学发布的52k条指令数据，由GPT-3.5生成，是指令微调的开山之作。</li><li>Dolly：由Databricks员工众包生成的15k条高质量指令数据。</li><li>Open-Orca：一个包含数百万条由更强模型（如GPT-4）生成的指令数据的大型数据集。</li></ul><p>自己生成数据：</p><ul><li>人工标注：成本最高，但质量最好。由领域专家亲自编写指令和答案。</li><li>使用强模型（如GPT-4）生成：设计一些高质量的“种子Prompt”，然后让GPT-4围绕你的领域，生成大量的问答对、分类、摘要等数据。这是目前最常用、性价比最高的方法。但需要注意，生成的数据必须经过人工的严格筛选和修正。</li></ul><p>从现有文档中提取：将公司的FAQ、产品手册、API文档等，转化为问答对的形式。</p><h2 id=85-实战项目使用lora微调一个7b模型以完成特定领域的问答任务>8.5 实战项目：使用LoRA微调一个7B模型以完成特定领域的问答任务</h2><p>项目目标：我们将使用一个关于“人工智能基础知识”的自定义小型数据集，通过QLoRA技术，微调一个7B的开源模型（如Llama 3 8B或Qwen 7B），使其能够准确地回答相关的AI概念问题。</p><p>技术栈：</p><ul><li><code>transformers</code></li><li><code>peft</code></li><li><code>accelerate</code> (用于分布式训练和设备管理)</li><li><code>bitsandbytes</code> (用于QLoRA)</li><li><code>datasets</code> (用于加载和处理数据)</li></ul><p>第一步：准备数据集 (<code>ai_knowledge_qa.jsonl</code>)</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>{</span><span class=nt>&#34;instruction&#34;</span><span class=p>:</span> <span class=s2>&#34;什么是神经网络？&#34;</span><span class=p>,</span> <span class=nt>&#34;input&#34;</span><span class=p>:</span> <span class=s2>&#34;&#34;</span><span class=p>,</span> <span class=nt>&#34;output&#34;</span><span class=p>:</span> <span class=s2>&#34;神经网络是一种模仿生物大脑结构和功能的计算模型，由大量的、称为神经元的人工节点组成，这些节点按层次结构组织。它通过在大量数据上进行训练来学习和识别模式。&#34;</span><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>{</span><span class=nt>&#34;instruction&#34;</span><span class=p>:</span> <span class=s2>&#34;请解释什么是反向传播。&#34;</span><span class=p>,</span> <span class=nt>&#34;input&#34;</span><span class=p>:</span> <span class=s2>&#34;&#34;</span><span class=p>,</span> <span class=nt>&#34;output&#34;</span><span class=p>:</span> <span class=s2>&#34;反向传播是一种在神经网络中训练模型的核心算法。它通过计算损失函数相对于网络各参数的梯度，然后使用梯度下降等优化算法来更新权重，从而最小化预测误差。&#34;</span><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>{</span><span class=nt>&#34;instruction&#34;</span><span class=p>:</span> <span class=s2>&#34;Transformer模型的核心思想是什么？&#34;</span><span class=p>,</span> <span class=nt>&#34;input&#34;</span><span class=p>:</span> <span class=s2>&#34;&#34;</span><span class=p>,</span> <span class=nt>&#34;output&#34;</span><span class=p>:</span> <span class=s2>&#34;Transformer模型的核心思想是自注意力机制（Self-Attention）。它完全抛弃了传统的循环（RNN）和卷积（CNN）结构，允许模型在处理序列中的一个词时，直接计算并关注到序列中所有其他词的重要性，从而高效地捕捉长距离依赖关系。&#34;</span><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=err>...</span> <span class=err>(准备</span><span class=mi>100-500</span><span class=err>条类似的数据)</span>
</span></span></code></pre></div><p>第二步：编写微调脚本 (<code>finetune.py</code>)</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>datasets</span> <span class=kn>import</span> <span class=n>load_dataset</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>AutoModelForCausalLM</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>AutoTokenizer</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>BitsAndBytesConfig</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>TrainingArguments</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>Trainer</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>peft</span> <span class=kn>import</span> <span class=n>LoraConfig</span><span class=p>,</span> <span class=n>get_peft_model</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># --- 1. 加载模型和分词器 (QLoRA配置) ---</span>
</span></span><span class=line><span class=cl><span class=n>model_name</span> <span class=o>=</span> <span class=s2>&#34;meta-llama/Llama-3-8B&#34;</span> <span class=c1># 或其他7B/8B模型</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>bnb_config</span> <span class=o>=</span> <span class=n>BitsAndBytesConfig</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>load_in_4bit</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>bnb_4bit_quant_type</span><span class=o>=</span><span class=s2>&#34;nf4&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>bnb_4bit_compute_dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>bfloat16</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>bnb_4bit_use_double_quant</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModelForCausalLM</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model_name</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>quantization_config</span><span class=o>=</span><span class=n>bnb_config</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>device_map</span><span class=o>=</span><span class=s2>&#34;auto&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>use_cache</span> <span class=o>=</span> <span class=kc>False</span> <span class=c1># 在训练时关闭cache</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_name</span><span class=p>,</span> <span class=n>trust_remote_code</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span><span class=o>.</span><span class=n>pad_token</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>eos_token</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span><span class=o>.</span><span class=n>padding_side</span> <span class=o>=</span> <span class=s2>&#34;right&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># --- 2. LoRA配置 ---</span>
</span></span><span class=line><span class=cl><span class=n>peft_config</span> <span class=o>=</span> <span class=n>LoraConfig</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>r</span><span class=o>=</span><span class=mi>16</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>lora_alpha</span><span class=o>=</span><span class=mi>32</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>lora_dropout</span><span class=o>=</span><span class=mf>0.05</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>target_modules</span><span class=o>=</span><span class=p>[</span><span class=s2>&#34;q_proj&#34;</span><span class=p>,</span> <span class=s2>&#34;k_proj&#34;</span><span class=p>,</span> <span class=s2>&#34;v_proj&#34;</span><span class=p>,</span> <span class=s2>&#34;o_proj&#34;</span><span class=p>,</span> <span class=s2>&#34;gate_proj&#34;</span><span class=p>,</span> <span class=s2>&#34;up_proj&#34;</span><span class=p>,</span> <span class=s2>&#34;down_proj&#34;</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=n>task_type</span><span class=o>=</span><span class=s2>&#34;CAUSAL_LM&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>peft_model</span> <span class=o>=</span> <span class=n>get_peft_model</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>peft_config</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>peft_model</span><span class=o>.</span><span class=n>print_trainable_parameters</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># --- 3. 加载和预处理数据集 ---</span>
</span></span><span class=line><span class=cl><span class=n>dataset</span> <span class=o>=</span> <span class=n>load_dataset</span><span class=p>(</span><span class=s2>&#34;json&#34;</span><span class=p>,</span> <span class=n>data_files</span><span class=o>=</span><span class=s2>&#34;ai_knowledge_qa.jsonl&#34;</span><span class=p>,</span> <span class=n>split</span><span class=o>=</span><span class=s2>&#34;train&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>format_prompt</span><span class=p>(</span><span class=n>example</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># Alpaca prompt模板</span>
</span></span><span class=line><span class=cl>    <span class=n>prompt</span> <span class=o>=</span> <span class=sa>f</span><span class=s2>&#34;&#34;&#34;Below is an instruction that describes a task. Write a response that appropriately completes the request.
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>### Instruction:
</span></span></span><span class=line><span class=cl><span class=s2></span><span class=si>{</span><span class=n>example</span><span class=p>[</span><span class=s1>&#39;instruction&#39;</span><span class=p>]</span><span class=si>}</span><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>### Response:
</span></span></span><span class=line><span class=cl><span class=s2></span><span class=si>{</span><span class=n>example</span><span class=p>[</span><span class=s1>&#39;output&#39;</span><span class=p>]</span><span class=si>}</span><span class=s2>&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=p>{</span><span class=s2>&#34;text&#34;</span><span class=p>:</span> <span class=n>prompt</span><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>dataset</span> <span class=o>=</span> <span class=n>dataset</span><span class=o>.</span><span class=n>map</span><span class=p>(</span><span class=n>format_prompt</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># --- 4. 设置训练参数 ---</span>
</span></span><span class=line><span class=cl><span class=n>training_arguments</span> <span class=o>=</span> <span class=n>TrainingArguments</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>output_dir</span><span class=o>=</span><span class=s2>&#34;./results&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>num_train_epochs</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>per_device_train_batch_size</span><span class=o>=</span><span class=mi>4</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>gradient_accumulation_steps</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>optim</span><span class=o>=</span><span class=s2>&#34;paged_adamw_32bit&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>learning_rate</span><span class=o>=</span><span class=mf>2e-4</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>weight_decay</span><span class=o>=</span><span class=mf>0.001</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>fp16</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>bf16</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=c1># 如果你的GPU支持bf16，强烈推荐</span>
</span></span><span class=line><span class=cl>    <span class=n>max_grad_norm</span><span class=o>=</span><span class=mf>0.3</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>max_steps</span><span class=o>=-</span><span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>warmup_ratio</span><span class=o>=</span><span class=mf>0.03</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>group_by_length</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>lr_scheduler_type</span><span class=o>=</span><span class=s2>&#34;constant&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>logging_steps</span><span class=o>=</span><span class=mi>25</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># --- 5. 初始化Trainer并开始训练 ---</span>
</span></span><span class=line><span class=cl><span class=n>trainer</span> <span class=o>=</span> <span class=n>Trainer</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>=</span><span class=n>peft_model</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>train_dataset</span><span class=o>=</span><span class=n>dataset</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>args</span><span class=o>=</span><span class=n>training_arguments</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># DataCollatorForLanguageModeling 会自动处理padding和masking</span>
</span></span><span class=line><span class=cl>    <span class=n>data_collator</span><span class=o>=</span><span class=k>lambda</span> <span class=n>data</span><span class=p>:</span> <span class=p>{</span><span class=s1>&#39;input_ids&#39;</span><span class=p>:</span> <span class=n>tokenizer</span><span class=p>([</span><span class=n>x</span><span class=p>[</span><span class=s1>&#39;text&#39;</span><span class=p>]</span> <span class=k>for</span> <span class=n>x</span> <span class=ow>in</span> <span class=n>data</span><span class=p>],</span> <span class=n>return_tensors</span><span class=o>=</span><span class=s1>&#39;pt&#39;</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>truncation</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>max_length</span><span class=o>=</span><span class=mi>512</span><span class=p>)</span><span class=o>.</span><span class=n>input_ids</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                <span class=s1>&#39;labels&#39;</span><span class=p>:</span> <span class=n>tokenizer</span><span class=p>([</span><span class=n>x</span><span class=p>[</span><span class=s1>&#39;text&#39;</span><span class=p>]</span> <span class=k>for</span> <span class=n>x</span> <span class=ow>in</span> <span class=n>data</span><span class=p>],</span> <span class=n>return_tensors</span><span class=o>=</span><span class=s1>&#39;pt&#39;</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>truncation</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>max_length</span><span class=o>=</span><span class=mi>512</span><span class=p>)</span><span class=o>.</span><span class=n>input_ids</span><span class=p>},</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>trainer</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># --- 6. 保存微调后的模型 ---</span>
</span></span><span class=line><span class=cl><span class=c1># 只保存LoRA适配器的权重，非常小</span>
</span></span><span class=line><span class=cl><span class=n>trainer</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>save_pretrained</span><span class=p>(</span><span class=s2>&#34;./results/final_checkpoint&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span><span class=o>.</span><span class=n>save_pretrained</span><span class=p>(</span><span class=s2>&#34;./results/final_checkpoint&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p>第三步：运行与推理</p><ol><li><p>运行训练：<code>python finetune.py</code></p></li><li><p>推理：编写一个<code>inference.py</code>脚本，加载基础模型和微调后的LoRA权重，进行问答。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>peft</span> <span class=kn>import</span> <span class=n>PeftModel</span>
</span></span><span class=line><span class=cl><span class=c1># ... 加载基础模型和分词器 (同上) ...</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 加载LoRA权重并合并</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>PeftModel</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=s2>&#34;./results/final_checkpoint&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>merge_and_unload</span><span class=p>()</span> <span class=c1># 合并权重以便快速推理</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 进行推理</span>
</span></span><span class=line><span class=cl><span class=n>instruction</span> <span class=o>=</span> <span class=s2>&#34;请解释一下什么是LoRA？&#34;</span>
</span></span><span class=line><span class=cl><span class=n>prompt</span> <span class=o>=</span> <span class=sa>f</span><span class=s2>&#34;&#34;&#34;Below is an instruction that describes a task. Write a response that appropriately completes the request.
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>### Instruction
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2></span><span class=si>{</span><span class=n>instruction</span><span class=si>}</span><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>### Response
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>inputs</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>(</span><span class=n>prompt</span><span class=p>,</span> <span class=n>return_tensors</span><span class=o>=</span><span class=s2>&#34;pt&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=s2>&#34;cuda&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>generate</span><span class=p>(</span><span class=n>inputs</span><span class=p>,</span> <span class=n>max_new_tokens</span><span class=o>=</span><span class=mi>200</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>response</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=n>outputs</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>skip_special_tokens</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>response</span><span class=p>)</span>
</span></span></code></pre></div></li></ol><p>运行推理脚本后，你会发现，模型能够以一种非常专业和准确的方式，回答关于LoRA的问题，这正是它从我们的自定义数据集中学到的新知识和表达方式。</p><h2 id=本章小结>本章小结</h2><p>在本章中，我们深入掌握了定制化大模型的关键技术——高效参数微调。</p><p>我们首先明确了微调的价值与时机，建立了一套从Prompt Engineering到RAG，再到PEFT的清晰技术选型路径。</p><p>接着，我们系统地学习了PEFT的核心思想，并重点剖析了当前最强大、最流行的LoRA及其低资源变体QLoRA的数学原理和工程实现。我们理解了如何通过低秩分解和4-bit量化，在消费级硬件上“驯服”数十亿参数的巨型模型。</p><p>我们还强调了数据工程在微调中的决定性作用，学习了如何构建高质量的指令微调数据集，并将其转化为模型能够学习的Prompt格式。</p><p>最后，通过一个端到端的实战项目，我们将所有理论知识付诸实践，亲手使用QLoRA微调了一个7B规模的大模型，使其成为了一个AI领域的问答专家。这个过程，让你完整地体验了从数据准备、模型配置、训练执行到最终推理的全流程。</p><p>完成本章后，你已经不再仅仅是LLM的使用者，你已经成为了一名能够根据业务需求，亲手“雕琢”和“塑造”大模型的AI工程师。这项能力，是你在大模型时代构建核心竞争力的关键。在下一章，我们将探索另一个与微调相辅相成的强大范式——检索增强生成（RAG），学习如何为大模型装上一个能够连接实时、私有知识库的“外挂大脑”。</p></div><div class=post_footer><div class=meta><div class=info><span class="field tags"><i class=ri-stack-line></i>
<a href=https://zhurongshuo.com/tags/%E4%B9%A6%E7%A8%BF/>书稿</a></span></div></div></div></div><div class=doc_comments></div></div></div></div><a id=back_to_top href=# class=back_to_top><i class=ri-arrow-up-s-line></i></a><footer class=footer><div class=powered_by><a href=https://varkai.com>Designed by VarKai, </a><a href=http://www.gohugo.io/>Proudly published with Hugo,</a></div><div class=footer_slogan><span>法不净空，觉无性也。</span></div><div class=powered_by style=margin-top:10px;font-size:14px><a href=https://zhurongshuo.com/>Copyright © 2010-2025 祝融说 zhurongshuo.com All Rights Reserved.</a></div></footer><script defer src=https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><link href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.css rel=stylesheet integrity="sha256-7qiTu3a8qjjWtcX9w+f2ulVUZSUdCZFEK62eRlmLmCE=" crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin=anonymous></script><script defer src=https://zhurongshuo.com/js/zozo.js></script><script type=text/javascript async src="https://cdn.jsdelivr.net/npm/mathjax@2.7.8/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["[[","]]"]],processEscapes:!0,processEnvironments:!0,skipTags:["script","noscript","style","textarea","pre"],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}}),MathJax.Hub.Queue(function(){var e,t=MathJax.Hub.getAllJax();for(e=0;e<t.length;e+=1)t[e].SourceElement().parentNode.className+=" has-jax"})</script><style>code.has-jax{font:inherit;font-size:100%;background:inherit;border:inherit;color:#515151}</style><script async src="https://www.googletagmanager.com/gtag/js?id=G-KKJ5ZEG1NB"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KKJ5ZEG1NB")</script></body></html>